---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE)
```

# Probability and random variables {#probability}

```{r load-packages}
library(tidyverse)
theme_set(theme_bw(base_size = 12) + 
            theme(strip.background = element_blank(), 
                  panel.grid = element_blank())) 
```

### Probability vs estimation

Here are two quotes from *STFS* that encapsulate the distinction between probability and estimation:

> *In probability theory, we think about processes that generate data, and we ask "what can we say about the data generated by such a process?*

> *In statistical estimation and inference, we work in the opposite direction. We start with data, and we ask "what can we say about the process that generated these data?*

So, we can define probability as the *study of data generated by specified processes* - and that's the focus of this chapter. Later on, when we get to data - we'll be assuming probabilistic models - but we have to accept the fact that the data we observed were not likely to be generated by the assumed model! But we do the best we can. 

### What is a probability?

Frequency view  

  - the probability of a given event is simply the proportion of times it occurs over many trials
  - difficult to apply to statements like "*what is the probability that it will rain tomorrow*" and "*what is the probability that a meteor will hit the earth tomorrow*" (i.e., one-shot events)

Degrees-of-belief view  

  - what is the degree of belief in the occurrence of an event that a rational person would have?

An aside: is the universe fundamentally probabilistic or deterministic?

### Set notation

A **set** ($\Omega$, in *STFS*) is an unordered collection of objects (*elements*). 

**Intersection** ($\cap$) of two sets is the set of elements that appear in both sets. 

**Union** ($\cup$) of two sets is the set that has every element that appears in *either* of the two original sets. 

The **complement** of a set ($S^C$) includes all of the elements *not* in the original set but present in the **sample space** ($\Omega$) (which contains all possible elements). Subsets are referred to as *S*, and have the following properties:

$$
\begin{aligned}
S \cup S^C = \Omega  \\
S \cap S^C = \emptyset
\end{aligned}
$$

where $\emptyset$ is used to denote the empty set, or the set with no elements. 

A Venn diagram is useful here:

![](images/venn_1.png)

The notation for the probability of an event is typically denoted using either $P()$ or $[]$: 

$$
\begin{aligned}
P(event) = [event] \\
\end{aligned}
$$

## Kolmogorov's three axioms of probability

1. Probabilities of any event *i* ($E_i$) cannot be negative:

$$
\begin{aligned}
\ [E_i] \geq 0 \\
\end{aligned}
$$

2. The probability of the event that includes every outcome is 1:

$$
\begin{aligned}
\ [\Omega] = 1 \\
\end{aligned}
$$

3. The probability of observing either of two mutually exclusive events is the sum of their individual probabilities:

$$
\begin{aligned}
\text{If } E_1 \cap E_2 = \emptyset,  \\
\text{then } [E_1 \cup E_2] = [E_1] + [E_2] 
\end{aligned}
$$

### Exercise set 4-1

1. Given P($A$), we would like to know P($A^C$). We already learned the following properties of complements:

$$
\begin{aligned}
A \cup A^C = \Omega  \\
A \cap A^C = \emptyset
\end{aligned}
$$
The intersection between $A$ and $A^C$ is zero, meaning they don't share any elements. The union of $A$ and $A^C$ is $[\Omega]$. We also learned the 2nd axiom of probability, $[\Omega] = 1$. Therefore:

$$
\begin{aligned}
\ [A] + [A^C] = 1 \\ 
\ [A^C] = 1 - [A]
\end{aligned}
$$

2. On paper. 

3. Below I've pasted an image of my handwritten version of Edge's solution - I found it easier to think about once I visualized it using a Venn diagram. 

![](images/edge_4_1_3.png)

## Conditional probability and independence

The law of conditional probability states that: 

$$
\begin{aligned}
\ [A \mid B] = \frac{[A \cap B]}{[B]} \\ 
\end{aligned}
$$

If A and B are independent, then: 

$$
\begin{aligned}
\ [A \mid B] = [A] \\ 
\end{aligned}
$$

### Exercise set 4-2

1. If we rearrange the law of probability, we get:

$$
\begin{aligned}
\ [A \mid B] [B] = [A \cap B] \\ 
\end{aligned}
$$

By definition, if A and B are independent, then we can replace $[A \mid B]$ with $[A]$ and get:

$$
\begin{aligned}
\ [A \cap B] = [A] [B] \\ 
\end{aligned}
$$

If we divide both sides by $[A]$, we get:

$$
\begin{aligned}
\ [B] = \frac{[A \cap B]}{[A]} \\ 
\end{aligned}
$$

The right-hand side of the above equation is, by the law of conditional probability, equal to $[B \mid A]$ and thus:

$$
\begin{aligned}
\ [B] = [B \mid A]\\ 
\end{aligned}
$$

2. Suppose you know $[A \mid B]$, $[A]$, and $[B]$. Calculate $[B \mid A]$. 

$$
\begin{aligned}
\ [A \mid B] = \frac{[A \cap B]}{[B]} \\ 
\end{aligned}
$$

$$
\begin{aligned}
\ [A \mid B] [B] = [A \cap B] \\ 
\end{aligned}
$$

$$
\begin{aligned}
\ [B \mid A] = \frac{[A \cap B]}{[A]} \\ 
\end{aligned}
$$

$$
\begin{aligned}
\ [B \mid A] = \frac{[A \mid B] [B]}{[A]} \\ 
\end{aligned}
$$

## Bayes' Theorem

## Discrete random variables and distributions

**Random variable**: e.g., the process of rolling a die, $X$

  - $X$ is random, can take integer values 1-6
  - $X$ is not a number yet - it is the unrealized outcome of a random process
  - The realization of that process is an **instance** - which is a number
  - Capital $X$: random variable
  - Lower case $x$: instance

All the probability information is contained in its distribution. 

In our case, $X$ is a **discrete** random variable, meaning that the number of outcomes is countable, in principle. 

Two ways to represent the distribution:

  1. Probability mass function (pmf), $f_X(x) = P(X = x)$
  2. Cumulative distribution function (cdf), $F_X(x) = P(X \leq x)$
  
The cdf is a series of partial sums of the pmf,
  
$$
\begin{aligned}
F_X(x) = P(X \leq x) = \sum_{x_i \leq x} f_X(x_i)
\end{aligned}
$$
and increases monotonically in $x$. 

### Exercise set 4-3

1. Flipping a fair coin 3 times. X is a random variable that represents the number of heads observed, and the sample space $\Omega$ contains the elements {0, 1, 2, 3}. Here are all of the ways we can observe these elements:

> x = 0: (T, T, T)  
> x = 1: (H, T, T); (T, H, T); (T, T, H)  
> x = 2: (H, H, T); (H, T, H); (T, H, H)  
> x = 3: (H, H, H)  

There are 8 possible instances. Thus, the probability mass function is:

$$
\begin{aligned}
f_X(0) =& f_X(3) = 1/8 \\
f_X(1) =& f_X(2) = 3/8 \\
f_X(x) =& 0 \text{ for all other } x \\
\end{aligned}
$$

2. If we sum $f_X(x_i)$ for all possible $x_i$, the sum would be 1 (1/8 + 1/8 + 3/8 + 3/8). 

3. In terms of $F_X$, what is $P(a \lt X \le b)$, if $b \gt a$? Hint: notice that $a \lt X \le b$ if and only if $X \leq b$ and $X > a$. 

Note that the cdf, $F_X$, for $x = a$ and $x = b$ are given by the following:

$$
\begin{aligned}
F_X(a) =& P(X \le a) \\
F_X(b) =& P(X \le b) \\
\end{aligned}
$$

We want the probability of observing a value $x$ between $a$ and $b$, so:

$$
\begin{aligned}
P(a \lt X \le b) = F_X(b) - F_X(a) \\
\end{aligned}
$$

## Continuous random variables and distributions

What about values that are not countable - anything with a decimal? Impossible to get a specific number, or instance (e.g., the probability a person weighs exactly 70kg), and thus we cannot use the pmf. But we can still use the cdf:

$$
\begin{aligned}
F_X(x) = P(X \leq x)
\end{aligned}
$$

That is, we can ask about the probability that a person will weigh 70kg or less, or whether a person will weigh between 70 and 71kg. 

### Exercise set 4-4

1. $X$ is a random variable that takes values in the interval [0, 1], and the probability distribution is uniform. Draw the cumulative distribution function $F_X(x)$ for $x \in [-1, 2]$. 

```{r}
x <- c(0,1)
Fx <- x
plot(x, Fx, type = "l", xlim = c(-1,2))
lines(c(-1, 0), c(0, 0))
lines(c(1,2), c(1, 1))
```

2. If $X$ were more likely to land in [0.4, 0.6] than in any other region of length 0.2, the line between 0.4 and 0.6 would be steeper. 

```{r}
x1 <- c(0,0.4)
x2 <- c(0.4, 0.6)
x3 <- c(0.6, 1)
Fx1 <- c(0,0.3)
Fx2 <- c(0.3, 0.7)
Fx3 <- c(0.7, 1)
plot(x1, Fx1, type = "l", xlim = c(-1,2), ylim = c(0,1), xlab =
"x", ylab = "Fx")
lines(x2, Fx2)
lines(x3, Fx3)
lines(c(-1, 0), c(0, 0))
lines(c(1,2), c(1, 1))
```

## Probability density functions

**For a continuous random variable, the pdf is the derivative of the cdf**

Recall that the cdf for a discrete random variable is a series of partial sums, given by: 

$$
\begin{aligned}
F_X(x) = P(X \leq x) = \sum_{x_i \leq x} f_X(x_i)
\end{aligned}
$$

In an analogous fashion, we can integrate a continuous function to get the cdf of a continous random variable. We define the probability density function $f_X$ (pdf) of a continuous random variable as:

$$
\begin{aligned}
F_X(x) =& \int_{- \infty}^{x} f_X(u) du \\
\end{aligned}
$$

Below I have re-created Fig 4-4, with a pdf in the upper panel and a cdf in the lower panel - for an *exponential* random variable with rate 1. 

```{r edge-fig-options, echo = FALSE}
figwidth <- 6 #6
figheight <- 4.5 #4.5
un <- "in"
reso <- 600 #pixels/in
cex.pts <- 0.6
cex.labs <- 1.2
cex.axs <- 1.1
mgp.set <- c(2.3, .8, 0)
mar.df <- c(3.5, 3.5, 1, 1)
cex.labs.fx <- 1.4
mar.fx <- c(5,5,2,1)
fontfam <- "sans"
type.plot <- "cairo"
```

```{r, echo = FALSE, fig.width = 7, fig.height = 5}
par(mar = mar.fx, las = 1, family = fontfam, bty = "n")
#par(mfrow=c(2,1))
x <- seq(0, 5, length.out = 1000)
fx <- dexp(x)
Fx <- pexp(x)

plot(x, fx, type = "l", xlab = expression(italic(x)), 
     ylab = expression(paste(italic(f[X]), "(", italic(x), ")", sep = "")), lwd = 2,
     cex.axis = cex.labs.fx, cex.lab = cex.labs.fx)
polygon(c(0, seq(0, 1, length.out = 1000), 1), c(0, dexp(seq(0, 1, length.out = 1000)), 0),  col = "grey", border = NA)
text(expression(italic(F[X](1))), x = 1/2, y = 1/4)
mtext("Probability density function", adj = 0, cex = cex.labs.fx)

plot(x, Fx, type = "l", xlab = expression(italic(x)), 
     ylab = expression(paste(italic(F[X]), "(", italic(x), ")", sep = "")), lwd = 2,
     cex.axis = cex.labs.fx, cex.lab = cex.labs.fx)
text(expression(italic(F[X](1))), x = 1.2, y = 1/4, srt = 90, col = "grey55")
mtext("Cumulative distribution function", adj = 0, cex = cex.labs.fx)
lines(c(1,1), c(0, pexp(1)), lty = 2, col = "grey55")
```

### Additional viz

We can visualize the principle of question 3 from Exercise Set 4-3 using the cdf of a standard normal distribution, where $a = -1$ and $b = 1$:

```{r, fig.width = 7, fig.height = 7, echo = FALSE}
a <- -1
b <- 1
par(mfrow = c(2,2))

## PDF for b
x <- seq(-4, 4, length = 200)
y <- dnorm(x)
plot(y ~ x, type = "l", lwd = 2, main = expression('f' ['X'] * '(x = b)'), 
     xlab = "x", ylab = expression('f' ['X']))
x <- seq(-4, b, length = 200)
y <- dnorm(x)
polygon(x = c(-4, x, b), y = c(0, y, 0), col = "red")

## PDF for a
x <- seq(-4, 4, length = 200)
y <- dnorm(x)
plot(y ~ x, type = "l", lwd = 2, main = expression('f' ['X'] * '(x = a)'), 
     xlab = "x", ylab = expression('f' ['X']))
x <- seq(-4, a, length = 200)
y <- dnorm(x)
polygon(x = c(-4, x, a), y = c(0, y, 0), col = "red")

## PDF
x <- seq(-4, 4, length = 200)
y <- dnorm(x)
plot(y ~ x, type = "l", lwd = 2, main = expression('f' ['X'] * '(a < x < b)'), 
     xlab = "x", ylab = expression('f' ['X']))
x <- seq(a, b, length = 200)
y <- dnorm(x)
polygon(x = c(a, x, b), y = c(0, y, 0), col = "red")

## CDF
x <- seq(-4, 4, length = 200)
y <- pnorm(x)
plot(y ~ x, type = "l", lwd = 1, main = expression('F' ['X'] * '(a < x < b)'), 
     xlab = "x", ylab = expression('F' ['X']))
x <- seq(a, b, length = 200)
y <- pnorm(x)
lines(x = x, y = y, lwd = 2, col = "red")
```


### Exercise set 4-5

1. If $f_X(x)$ is a probability density funcion, then total area under $f_X(x)$ is 1. 
2. Yes, $f_x$ can be a probability density funcion, because the area under the function is 1. This situation is different from a probability mass function, because the y-axis values for a pdf can be > 1 (in this case, the maximum y is 10). 


## Families of distributions

Two requirements for mass or density functions:

1. Must be non-negative

2. Sum of all values is 1 (mass), or area under curve is 1 (density)

Distribution family

  - similarly shaped distributions
  - summaries of their behavior can be computed from the same functions
  - but **parameter** values differ

### Exercise set 4-6

1. The probability mass function of the Poisson distribution is $P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}$. Plugging in the appropriate values for $k$ and $\lambda$ gives: 

(i). $e^{-5}$  
(ii). $5e^{-5}$  
(iii). $\frac{25}{2}e^{-5}$  

2. Use the probability mass function of the geometric distribution with parameter 1/2. If our first “heads” occurs on the 6th flip, then we have five tails before it. We plug $p$ = 1/2 and $k$ = 5 into $P(X = k) = (1-p)^kp$ to get 1/64.  

3. Consider a Poisson distribution with parameter $\lambda$ = 5. If we want to know the value of the probability mass function for x = 2, $f_X(2)$, we use the `dpois()` function:

```{r}
dpois(2, lambda = 5)
```

To get the value of the cumulative distribution function $F_X(2)$, we use `ppois()`:

```{r}
ppois(2, lambda = 5)
```

If we want to know the *inverse* of the cumulative distribution function. That is, we want to know the value of $q$ that solves the equation $F_X(q) = p$. What is the number $q$ such that the probability tha the random variable is less than or equal to $q$ is $p$; $q$ is the $pth$ percentile of the distribution. We can get this using `qpois()`:

```{r}
qpois(0.124652, lambda = 5)
```

The inverse of the cdf is also called the quantile function. Using the standard normal distribution (mean = 0, sd = 1), plot the probability density function for x $\in$ [-3, 3]. 

```{r}
x <- seq(-3, 3, by = 0.1)
fx <- dnorm(x = x)
plot(x, fx, type = "l", main = "PDF", col = "red")
```

Plot the cdf for x $\in$ [-3, 3].
```{r}
x <- seq(-3, 3, by = 0.1)
fx <- pnorm(q = x)
plot(x, fx, type = "l", main = "CDF", col = "red")
```

What value of $x$ is at the 97.5th percentile of the standard normal?
```{r}
qnorm(p = 0.975)
```

4. Simulating from a normal distribution and from a uniform distribution:

```{r}
n <- 1000
x <- rnorm(n)
hist(x)
x <- runif(n, min = 0, max = 1)
hist(x)
```

Now take those values between 0 and 1, and feed them into the `qnorm` function to get the values at which we see those quantiles:

```{r}
y <- qnorm(p = x, mean = 0, sd = 1)
hist(y)
```

These values are normally distributed around 0. This plot is saying that most of the probability (in fact ~95%) lies between -2 < y < 2 (2 standard deviations). 

```{r}
r <- seq(-3, 3, length.out = 1000)
cdf <- pnorm(r)
#Draw the normal cumulative distribution function.
plot(r, cdf, type = "l", xaxs = "i", yaxs = "i", xlim = c(-3,
3), xlab = expression(italic(x)), ylab =
expression(paste(italic(F[X]), "(", italic(x), ")", sep = "")),
lwd = 2)
#Draw light grey lines representing random samples from the
#standard normal distribution.
x <- rnorm(100)
for(i in x){
  lines(c(i,i), c(min(x), pnorm(i)), col = rgb(190, 190, 190,
     alpha = 60, max = 255))
  lines(c(min(x)-1,i), c(pnorm(i), pnorm(i)), col = rgb(190,
     190, 190, alpha = 60, max = 255))
}
```

### Additional exercise 

(Courtesy Blondin & Goodman)  

Assume that destructive earthquakes occur in California every 20 years. You’d like to know how probable an earthquake is between now and some future date.

1. What distribution family describes the waiting time until California’s next earthquake? What value should we use for this distribution’s parameter? [Hint: convert 20 years to a rate]

Use the exponential distribution, where $\lambda = 1/20$:

$$
\begin{aligned}
f_X(x) = \lambda e ^{-\lambda x} \\
\end{aligned}
$$

2. What is the probability that an earthquake will happen in the next 10 years? [Hint: You need to find a cumulative distribution function for the distribution you chose, either online or by integration].

Rules for integrating "e raised to the x power":

$$
\begin{aligned}
\int e^xdx =& e^x + c \\
\int e^{ax}dx =& \frac{1}{a} e^{ax} + c \\
\int be^{ax}dx =& \frac{b}{a} e^{ax} + c \\
\end{aligned}
$$

With these rules in hand, we can get the cdf by integrating the pdf for the exponential distribution:

$$
\begin{aligned}
f_X(x) =& \lambda e ^{-\lambda x} \\
F_X(X = x) =& \int_{0}^{x} \lambda e^{-\lambda x} dx \\
=& \frac{1}{- \lambda} \lambda e^{-\lambda x} \bigg\rvert_{0}^{x} \\ 
=& -e^{-\lambda x} \rvert_{0}^{x} \\ 
=& -e^{-\lambda x} \rvert_{0}^{x} \\ 
=& -e^{-\lambda x} - (-e^{-\lambda 0}) \\
=& -e^{-\lambda x} - (-1) \\
=& 1 -e^{-\lambda x}  \\
\end{aligned}
$$

Then, plug in $x = 10$ and $\lambda = 1/20$: 

```{r}
1 - exp(-(1/20) * (10))
```

Check to make sure we've done this right using the `pexp` function in R:

```{r}
pexp(q = 10, rate = 1/20)
```


3. What is the probability that an earthquake will occur between 10 and 20 years from now?

```{r}
pexp(q = 20, rate = 1/20) - pexp(q = 10, rate = 1/20)
```

4. If you have time, write the CDF of your chosen distribution as an R function. It should take a value x and a parameter, and return a probability P(X ≤ x). Use your function to plot the cumulative distribution as a function of x. 

```{r}
exp_cdf <- function(x, lambda) 1 - exp(-lambda * x)
x <- seq(0, 80, by = 0.1)
p <- exp_cdf(x = x, lambda = 1/20)
plot(p ~ x, type = "l", xlab = "x", ylab = "FX(x)")
```
