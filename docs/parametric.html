<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Parametric estimation and inference | Notes on Statistical Thinking from Scratch</title>
  <meta name="description" content="Cliff notes for Statistical Thinking from Scratch, by Doc Edge." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Parametric estimation and inference | Notes on Statistical Thinking from Scratch" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Cliff notes for Statistical Thinking from Scratch, by Doc Edge." />
  <meta name="github-repo" content="elahi/stats_from_scratch_edge" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Parametric estimation and inference | Notes on Statistical Thinking from Scratch" />
  
  <meta name="twitter:description" content="Cliff notes for Statistical Thinking from Scratch, by Doc Edge." />
  

<meta name="author" content="Robin Elahi" />


<meta name="date" content="2020-09-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="semiparametric.html"/>
<link rel="next" href="bayesian.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-121894527-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-121894527-4');
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Notes on Statistical Thinking from Scratch</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="r-eda.html"><a href="r-eda.html"><i class="fa fa-check"></i><b>2</b> R and exploratory data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="r-eda.html"><a href="r-eda.html#inspecting-the-dataframe"><i class="fa fa-check"></i><b>2.1</b> Inspecting the dataframe</a></li>
<li class="chapter" data-level="2.2" data-path="r-eda.html"><a href="r-eda.html#histograms"><i class="fa fa-check"></i><b>2.2</b> Histograms</a></li>
<li class="chapter" data-level="2.3" data-path="r-eda.html"><a href="r-eda.html#summarising-data"><i class="fa fa-check"></i><b>2.3</b> Summarising data</a></li>
<li class="chapter" data-level="2.4" data-path="r-eda.html"><a href="r-eda.html#loops"><i class="fa fa-check"></i><b>2.4</b> Loops</a></li>
<li class="chapter" data-level="2.5" data-path="r-eda.html"><a href="r-eda.html#functions"><i class="fa fa-check"></i><b>2.5</b> Functions</a></li>
<li class="chapter" data-level="2.6" data-path="r-eda.html"><a href="r-eda.html#boxplots"><i class="fa fa-check"></i><b>2.6</b> Boxplots</a></li>
<li class="chapter" data-level="2.7" data-path="r-eda.html"><a href="r-eda.html#scatterplots"><i class="fa fa-check"></i><b>2.7</b> Scatterplots</a></li>
<li class="chapter" data-level="2.8" data-path="r-eda.html"><a href="r-eda.html#exercise-set-2-2"><i class="fa fa-check"></i><b>2.8</b> Exercise set 2-2</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="best-fit-line.html"><a href="best-fit-line.html"><i class="fa fa-check"></i><b>3</b> Line of best fit</a><ul>
<li class="chapter" data-level="3.1" data-path="best-fit-line.html"><a href="best-fit-line.html#exercise-set-3-1"><i class="fa fa-check"></i><b>3.1</b> Exercise set 3-1</a></li>
<li class="chapter" data-level="3.2" data-path="best-fit-line.html"><a href="best-fit-line.html#exercise-set-3-2"><i class="fa fa-check"></i><b>3.2</b> Exercise set 3-2</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>4</b> Probability and random variables</a><ul>
<li class="chapter" data-level="4.0.1" data-path="probability.html"><a href="probability.html#probability-vs-estimation"><i class="fa fa-check"></i><b>4.0.1</b> Probability vs estimation</a></li>
<li class="chapter" data-level="4.0.2" data-path="probability.html"><a href="probability.html#what-is-a-probability"><i class="fa fa-check"></i><b>4.0.2</b> What is a probability?</a></li>
<li class="chapter" data-level="4.0.3" data-path="probability.html"><a href="probability.html#set-notation"><i class="fa fa-check"></i><b>4.0.3</b> Set notation</a></li>
<li class="chapter" data-level="4.1" data-path="probability.html"><a href="probability.html#kolmogorovs-three-axioms-of-probability"><i class="fa fa-check"></i><b>4.1</b> Kolmogorov’s three axioms of probability</a><ul>
<li class="chapter" data-level="4.1.1" data-path="probability.html"><a href="probability.html#exercise-set-4-1"><i class="fa fa-check"></i><b>4.1.1</b> Exercise set 4-1</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="probability.html"><a href="probability.html#conditional-probability-and-independence"><i class="fa fa-check"></i><b>4.2</b> Conditional probability and independence</a><ul>
<li class="chapter" data-level="4.2.1" data-path="probability.html"><a href="probability.html#exercise-set-4-2"><i class="fa fa-check"></i><b>4.2.1</b> Exercise set 4-2</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="probability.html"><a href="probability.html#bayes-theorem"><i class="fa fa-check"></i><b>4.3</b> Bayes’ Theorem</a></li>
<li class="chapter" data-level="4.4" data-path="probability.html"><a href="probability.html#discrete-random-variables-and-distributions"><i class="fa fa-check"></i><b>4.4</b> Discrete random variables and distributions</a><ul>
<li class="chapter" data-level="4.4.1" data-path="probability.html"><a href="probability.html#exercise-set-4-3"><i class="fa fa-check"></i><b>4.4.1</b> Exercise set 4-3</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="probability.html"><a href="probability.html#continuous-random-variables-and-distributions"><i class="fa fa-check"></i><b>4.5</b> Continuous random variables and distributions</a><ul>
<li class="chapter" data-level="4.5.1" data-path="probability.html"><a href="probability.html#exercise-set-4-4"><i class="fa fa-check"></i><b>4.5.1</b> Exercise set 4-4</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="probability.html"><a href="probability.html#probability-density-functions"><i class="fa fa-check"></i><b>4.6</b> Probability density functions</a><ul>
<li class="chapter" data-level="4.6.1" data-path="probability.html"><a href="probability.html#additional-viz"><i class="fa fa-check"></i><b>4.6.1</b> Additional viz</a></li>
<li class="chapter" data-level="4.6.2" data-path="probability.html"><a href="probability.html#exercise-set-4-5"><i class="fa fa-check"></i><b>4.6.2</b> Exercise set 4-5</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="probability.html"><a href="probability.html#families-of-distributions"><i class="fa fa-check"></i><b>4.7</b> Families of distributions</a><ul>
<li class="chapter" data-level="4.7.1" data-path="probability.html"><a href="probability.html#exercise-set-4-6"><i class="fa fa-check"></i><b>4.7.1</b> Exercise set 4-6</a></li>
<li class="chapter" data-level="4.7.2" data-path="probability.html"><a href="probability.html#additional-exercise"><i class="fa fa-check"></i><b>4.7.2</b> Additional exercise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="randomvars.html"><a href="randomvars.html"><i class="fa fa-check"></i><b>5</b> Properties of random variables</a><ul>
<li class="chapter" data-level="5.1" data-path="randomvars.html"><a href="randomvars.html#expected-values-and-the-law-of-large-numbers"><i class="fa fa-check"></i><b>5.1</b> Expected values and the law of large numbers</a><ul>
<li class="chapter" data-level="5.1.1" data-path="randomvars.html"><a href="randomvars.html#weak-law-of-large-numbers"><i class="fa fa-check"></i><b>5.1.1</b> Weak law of large numbers</a></li>
<li class="chapter" data-level="5.1.2" data-path="randomvars.html"><a href="randomvars.html#handy-facts-about-expectations"><i class="fa fa-check"></i><b>5.1.2</b> Handy facts about expectations</a></li>
<li class="chapter" data-level="5.1.3" data-path="randomvars.html"><a href="randomvars.html#exercise-set-5-1"><i class="fa fa-check"></i><b>5.1.3</b> Exercise set 5-1</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="randomvars.html"><a href="randomvars.html#variance-and-standard-deviation"><i class="fa fa-check"></i><b>5.2</b> Variance and standard deviation</a><ul>
<li class="chapter" data-level="5.2.1" data-path="randomvars.html"><a href="randomvars.html#beautiful-properties-of-the-variance"><i class="fa fa-check"></i><b>5.2.1</b> Beautiful properties of the variance</a></li>
<li class="chapter" data-level="5.2.2" data-path="randomvars.html"><a href="randomvars.html#exercise-set-5-2"><i class="fa fa-check"></i><b>5.2.2</b> Exercise set 5-2</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="randomvars.html"><a href="randomvars.html#joint-distributions-covariance-and-correlation"><i class="fa fa-check"></i><b>5.3</b> Joint distributions, covariance, and correlation</a><ul>
<li class="chapter" data-level="5.3.1" data-path="randomvars.html"><a href="randomvars.html#joint-probability-distributions"><i class="fa fa-check"></i><b>5.3.1</b> Joint probability distributions</a></li>
<li class="chapter" data-level="5.3.2" data-path="randomvars.html"><a href="randomvars.html#marginal-distributions"><i class="fa fa-check"></i><b>5.3.2</b> Marginal distributions</a></li>
<li class="chapter" data-level="5.3.3" data-path="randomvars.html"><a href="randomvars.html#covariance"><i class="fa fa-check"></i><b>5.3.3</b> Covariance</a></li>
<li class="chapter" data-level="5.3.4" data-path="randomvars.html"><a href="randomvars.html#correlation"><i class="fa fa-check"></i><b>5.3.4</b> Correlation</a></li>
<li class="chapter" data-level="5.3.5" data-path="randomvars.html"><a href="randomvars.html#additional-exercise-1"><i class="fa fa-check"></i><b>5.3.5</b> Additional exercise</a></li>
<li class="chapter" data-level="5.3.6" data-path="randomvars.html"><a href="randomvars.html#exercise-set-5-3"><i class="fa fa-check"></i><b>5.3.6</b> Exercise set 5-3</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="randomvars.html"><a href="randomvars.html#conditional-distribution-expectation-variance"><i class="fa fa-check"></i><b>5.4</b> Conditional distribution, expectation, variance</a></li>
<li class="chapter" data-level="5.5" data-path="randomvars.html"><a href="randomvars.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>5.5</b> The central limit theorem</a><ul>
<li class="chapter" data-level="5.5.1" data-path="randomvars.html"><a href="randomvars.html#exercise-set-5-4"><i class="fa fa-check"></i><b>5.5.1</b> Exercise set 5-4</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="randomvars.html"><a href="randomvars.html#a-probabilistic-model-for-simple-linear-regression"><i class="fa fa-check"></i><b>5.6</b> A probabilistic model for simple linear regression</a><ul>
<li class="chapter" data-level="5.6.1" data-path="randomvars.html"><a href="randomvars.html#assumptions-of-the-linear-model"><i class="fa fa-check"></i><b>5.6.1</b> Assumptions of the linear model</a></li>
<li class="chapter" data-level="5.6.2" data-path="randomvars.html"><a href="randomvars.html#important-claims-that-follow-from-these-assumptions"><i class="fa fa-check"></i><b>5.6.2</b> Important claims that follow from these assumptions</a></li>
<li class="chapter" data-level="5.6.3" data-path="randomvars.html"><a href="randomvars.html#checking-these-assumptions"><i class="fa fa-check"></i><b>5.6.3</b> Checking these assumptions</a></li>
<li class="chapter" data-level="5.6.4" data-path="randomvars.html"><a href="randomvars.html#exercise-set-5-5"><i class="fa fa-check"></i><b>5.6.4</b> Exercise set 5-5</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="estimators.html"><a href="estimators.html"><i class="fa fa-check"></i><b>6</b> Properties of point estimators</a><ul>
<li class="chapter" data-level="6.1" data-path="estimators.html"><a href="estimators.html#bias"><i class="fa fa-check"></i><b>6.1</b> Bias</a><ul>
<li class="chapter" data-level="6.1.1" data-path="estimators.html"><a href="estimators.html#exercise-set-6-1"><i class="fa fa-check"></i><b>6.1.1</b> Exercise set 6-1</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="estimators.html"><a href="estimators.html#variance"><i class="fa fa-check"></i><b>6.2</b> Variance</a><ul>
<li class="chapter" data-level="6.2.1" data-path="estimators.html"><a href="estimators.html#exercise-set-6-2"><i class="fa fa-check"></i><b>6.2.1</b> Exercise set 6-2</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="estimators.html"><a href="estimators.html#mean-squared-error"><i class="fa fa-check"></i><b>6.3</b> Mean squared error</a><ul>
<li class="chapter" data-level="6.3.1" data-path="estimators.html"><a href="estimators.html#exercise-set-6-3"><i class="fa fa-check"></i><b>6.3.1</b> Exercise set 6-3</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="estimators.html"><a href="estimators.html#consistency"><i class="fa fa-check"></i><b>6.4</b> Consistency</a><ul>
<li class="chapter" data-level="6.4.1" data-path="estimators.html"><a href="estimators.html#exercise-set-6-4"><i class="fa fa-check"></i><b>6.4.1</b> Exercise set 6-4</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="estimators.html"><a href="estimators.html#efficiency"><i class="fa fa-check"></i><b>6.5</b> Efficiency</a><ul>
<li class="chapter" data-level="6.5.1" data-path="estimators.html"><a href="estimators.html#exercise-set-6-5"><i class="fa fa-check"></i><b>6.5.1</b> Exercise set 6-5</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="estimators.html"><a href="estimators.html#statistical-decision-theory-and-risk"><i class="fa fa-check"></i><b>6.6</b> Statistical decision theory and risk</a></li>
<li class="chapter" data-level="6.7" data-path="estimators.html"><a href="estimators.html#robustness"><i class="fa fa-check"></i><b>6.7</b> Robustness</a><ul>
<li class="chapter" data-level="6.7.1" data-path="estimators.html"><a href="estimators.html#exercise-set-6-8"><i class="fa fa-check"></i><b>6.7.1</b> Exercise set 6-8</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="estimators.html"><a href="estimators.html#estimators-for-simple-linear-regression"><i class="fa fa-check"></i><b>6.8</b> Estimators for simple linear regression</a><ul>
<li class="chapter" data-level="6.8.1" data-path="estimators.html"><a href="estimators.html#exercise-set-6-9"><i class="fa fa-check"></i><b>6.8.1</b> Exercise set 6-9</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="intervals.html"><a href="intervals.html"><i class="fa fa-check"></i><b>7</b> Interval estimation and inference</a><ul>
<li class="chapter" data-level="7.1" data-path="intervals.html"><a href="intervals.html#standard-error"><i class="fa fa-check"></i><b>7.1</b> Standard error</a><ul>
<li class="chapter" data-level="7.1.1" data-path="intervals.html"><a href="intervals.html#exercise-set-7-1"><i class="fa fa-check"></i><b>7.1.1</b> Exercise set 7-1</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="intervals.html"><a href="intervals.html#confidence-intervals"><i class="fa fa-check"></i><b>7.2</b> Confidence intervals</a><ul>
<li class="chapter" data-level="7.2.1" data-path="intervals.html"><a href="intervals.html#exercise-set-7-2"><i class="fa fa-check"></i><b>7.2.1</b> Exercise set 7-2</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="intervals.html"><a href="intervals.html#frequentist-inference-i-null-hypotheses-test-statistics-and-p-values"><i class="fa fa-check"></i><b>7.3</b> Frequentist inference I: null hypotheses, test statistics, and <em>p</em> values</a><ul>
<li class="chapter" data-level="7.3.1" data-path="intervals.html"><a href="intervals.html#exercise-set-7-3"><i class="fa fa-check"></i><b>7.3.1</b> Exercise set 7-3</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="intervals.html"><a href="intervals.html#frequentist-inference-ii-alternative-hypotheses-and-the-rejection-framework"><i class="fa fa-check"></i><b>7.4</b> Frequentist inference II: alternative hypotheses and the rejection framework</a><ul>
<li class="chapter" data-level="7.4.1" data-path="intervals.html"><a href="intervals.html#exercise-set-7-4"><i class="fa fa-check"></i><b>7.4.1</b> Exercise set 7-4</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="intervals.html"><a href="intervals.html#connecting-hypothesis-tests-and-confidence-intervals"><i class="fa fa-check"></i><b>7.5</b> Connecting hypothesis tests and confidence intervals</a></li>
<li class="chapter" data-level="7.6" data-path="intervals.html"><a href="intervals.html#nhst-and-the-abuse-of-tests"><i class="fa fa-check"></i><b>7.6</b> NHST and the abuse of tests</a><ul>
<li class="chapter" data-level="7.6.1" data-path="intervals.html"><a href="intervals.html#exercise-set-7-5"><i class="fa fa-check"></i><b>7.6.1</b> Exercise set 7-5</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="intervals.html"><a href="intervals.html#frequentist-inference-iii-power"><i class="fa fa-check"></i><b>7.7</b> Frequentist inference III: power</a><ul>
<li class="chapter" data-level="7.7.1" data-path="intervals.html"><a href="intervals.html#exercise-set-7-6"><i class="fa fa-check"></i><b>7.7.1</b> Exercise set 7-6</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="intervals.html"><a href="intervals.html#putting-it-together-what-happens-when-the-sample-size-increases"><i class="fa fa-check"></i><b>7.8</b> Putting it together: what happens when the sample size increases?</a></li>
<li class="chapter" data-level="7.9" data-path="intervals.html"><a href="intervals.html#chapter-summary"><i class="fa fa-check"></i><b>7.9</b> Chapter summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="semiparametric.html"><a href="semiparametric.html"><i class="fa fa-check"></i><b>8</b> Semiparametric estimation and inference</a><ul>
<li class="chapter" data-level="8.0.1" data-path="semiparametric.html"><a href="semiparametric.html#exercise-set-8-1"><i class="fa fa-check"></i><b>8.0.1</b> Exercise set 8-1</a></li>
<li class="chapter" data-level="8.1" data-path="semiparametric.html"><a href="semiparametric.html#semiparametric-point-estimation-using-the-method-of-moments"><i class="fa fa-check"></i><b>8.1</b> Semiparametric point estimation using the method of moments</a><ul>
<li class="chapter" data-level="8.1.1" data-path="semiparametric.html"><a href="semiparametric.html#introduction-to-moments"><i class="fa fa-check"></i><b>8.1.1</b> Introduction to moments</a></li>
<li class="chapter" data-level="8.1.2" data-path="semiparametric.html"><a href="semiparametric.html#plug-in-estimators"><i class="fa fa-check"></i><b>8.1.2</b> Plug-in estimators</a></li>
<li class="chapter" data-level="8.1.3" data-path="semiparametric.html"><a href="semiparametric.html#exercise-set-8-2"><i class="fa fa-check"></i><b>8.1.3</b> Exercise set 8-2</a></li>
<li class="chapter" data-level="8.1.4" data-path="semiparametric.html"><a href="semiparametric.html#the-method-of-moments"><i class="fa fa-check"></i><b>8.1.4</b> The method of moments</a></li>
<li class="chapter" data-level="8.1.5" data-path="semiparametric.html"><a href="semiparametric.html#exercise-set-8-3"><i class="fa fa-check"></i><b>8.1.5</b> Exercise set 8-3</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="semiparametric.html"><a href="semiparametric.html#semiparametric-interval-estimation-using-the-bootstrap"><i class="fa fa-check"></i><b>8.2</b> Semiparametric interval estimation using the bootstrap</a><ul>
<li class="chapter" data-level="8.2.1" data-path="semiparametric.html"><a href="semiparametric.html#exercise-set-8-4"><i class="fa fa-check"></i><b>8.2.1</b> Exercise set 8-4</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="semiparametric.html"><a href="semiparametric.html#semiparametric-hypothesis-testing-using-permutation-tests"><i class="fa fa-check"></i><b>8.3</b> Semiparametric hypothesis testing using permutation tests</a><ul>
<li class="chapter" data-level="8.3.1" data-path="semiparametric.html"><a href="semiparametric.html#exercise-set-8-5"><i class="fa fa-check"></i><b>8.3.1</b> Exercise set 8-5</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="semiparametric.html"><a href="semiparametric.html#chapter-summary-1"><i class="fa fa-check"></i><b>8.4</b> Chapter summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="parametric.html"><a href="parametric.html"><i class="fa fa-check"></i><b>9</b> Parametric estimation and inference</a><ul>
<li class="chapter" data-level="9.0.1" data-path="parametric.html"><a href="parametric.html#exercise-set-9-1"><i class="fa fa-check"></i><b>9.0.1</b> Exercise set 9-1</a></li>
<li class="chapter" data-level="9.1" data-path="parametric.html"><a href="parametric.html#parametric-estimation-using-maximum-likelihood"><i class="fa fa-check"></i><b>9.1</b> Parametric estimation using maximum likelihood</a><ul>
<li class="chapter" data-level="9.1.1" data-path="parametric.html"><a href="parametric.html#exercise-set-9-2"><i class="fa fa-check"></i><b>9.1.1</b> Exercise set 9-2</a></li>
<li class="chapter" data-level="9.1.2" data-path="parametric.html"><a href="parametric.html#maximum-likelihood-estimation-for-simple-linear-regression"><i class="fa fa-check"></i><b>9.1.2</b> Maximum-likelihood estimation for simple linear regression</a></li>
<li class="chapter" data-level="9.1.3" data-path="parametric.html"><a href="parametric.html#exercise-set-9-3"><i class="fa fa-check"></i><b>9.1.3</b> Exercise set 9-3</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="parametric.html"><a href="parametric.html#parametric-interval-estimation-the-direct-approach-and-fisher-information"><i class="fa fa-check"></i><b>9.2</b> Parametric interval estimation: the direct approach and Fisher information</a><ul>
<li class="chapter" data-level="9.2.1" data-path="parametric.html"><a href="parametric.html#exercise-set-9-4"><i class="fa fa-check"></i><b>9.2.1</b> Exercise set 9-4</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="parametric.html"><a href="parametric.html#parametric-hypothesis-testing-using-the-wald-test"><i class="fa fa-check"></i><b>9.3</b> Parametric hypothesis testing using the Wald test</a><ul>
<li class="chapter" data-level="9.3.1" data-path="parametric.html"><a href="parametric.html#wald-test"><i class="fa fa-check"></i><b>9.3.1</b> Wald test</a></li>
<li class="chapter" data-level="9.3.2" data-path="parametric.html"><a href="parametric.html#exercise-set-9-5"><i class="fa fa-check"></i><b>9.3.2</b> Exercise set 9-5</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="parametric.html"><a href="parametric.html#parametric-hypothesis-testing-using-the-likelihood-ratio-test"><i class="fa fa-check"></i><b>9.4</b> Parametric hypothesis testing using the likelihood-ratio test</a><ul>
<li class="chapter" data-level="9.4.1" data-path="parametric.html"><a href="parametric.html#exercise-set-9-6"><i class="fa fa-check"></i><b>9.4.1</b> Exercise set 9-6</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>10</b> Bayesian estimation and inference</a><ul>
<li class="chapter" data-level="10.1" data-path="bayesian.html"><a href="bayesian.html#how-to-choose-a-prior-distribution"><i class="fa fa-check"></i><b>10.1</b> How to choose a prior distribution?</a></li>
<li class="chapter" data-level="10.2" data-path="bayesian.html"><a href="bayesian.html#the-unscaled-posterior-conjugacy-and-sampling-from-the-posterior"><i class="fa fa-check"></i><b>10.2</b> The unscaled posterior, conjugacy, and sampling from the posterior</a><ul>
<li class="chapter" data-level="10.2.1" data-path="bayesian.html"><a href="bayesian.html#rejection-sampling-algorithm"><i class="fa fa-check"></i><b>10.2.1</b> Rejection sampling algorithm</a></li>
<li class="chapter" data-level="10.2.2" data-path="bayesian.html"><a href="bayesian.html#exercise-set-10-1"><i class="fa fa-check"></i><b>10.2.2</b> Exercise set 10-1</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="bayesian.html"><a href="bayesian.html#bayesian-point-estimation-using-bayes-estimators"><i class="fa fa-check"></i><b>10.3</b> Bayesian point estimation using Bayes estimators</a><ul>
<li class="chapter" data-level="10.3.1" data-path="bayesian.html"><a href="bayesian.html#exercise-set-10-2"><i class="fa fa-check"></i><b>10.3.1</b> Exercise set 10-2</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="bayesian.html"><a href="bayesian.html#bayesian-interval-estimation-using-credible-intervals"><i class="fa fa-check"></i><b>10.4</b> Bayesian interval estimation using credible intervals</a><ul>
<li class="chapter" data-level="10.4.1" data-path="bayesian.html"><a href="bayesian.html#exercise-set-10-3"><i class="fa fa-check"></i><b>10.4.1</b> Exercise set 10-3</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="bayesian.html"><a href="bayesian.html#bayesian-hypothesis-testing-using-bayes-factors"><i class="fa fa-check"></i><b>10.5</b> Bayesian ‘hypothesis testing’ using Bayes factors</a><ul>
<li class="chapter" data-level="10.5.1" data-path="bayesian.html"><a href="bayesian.html#exercise-set-10-4"><i class="fa fa-check"></i><b>10.5.1</b> Exercise set 10-4</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="bayesian.html"><a href="bayesian.html#conclusion-bayesian-vs.-frequentist-methods"><i class="fa fa-check"></i><b>10.6</b> Conclusion: Bayesian vs. frequentist methods</a></li>
<li class="chapter" data-level="10.7" data-path="bayesian.html"><a href="bayesian.html#chapter-summary-2"><i class="fa fa-check"></i><b>10.7</b> Chapter summary</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes on Statistical Thinking from Scratch</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="parametric" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Parametric estimation and inference</h1>
<p>If all of the random variables in our model can be described by a finite set of parameters, we are using <em>parametric</em> estimation and inference. In paremetric estimation, the most important mathematical concept is the likelihood function, or <strong>likelihood</strong>.</p>
<blockquote>
<p>The likelihood allows us to compare values of a parameter in terms of their (the values’) ability to explain the observed data. In other words, if the true value of the parameter is y, how likely is the observed dataset?</p>
</blockquote>
<p>Suppose the observations, which we call <span class="math inline">\(d\)</span>, are instances drawn from a random variable <span class="math inline">\(D\)</span>, which is governed by a probability distribution function. <em>We do not know what this probability distribution is!</em> We have to think about the data - are they continuous or discrete; are they bounded at 0 or elsewhere; is the variance uniform or not? Based on the answers to these questions, we then assume a probability distribution function, <span class="math inline">\(f_D\)</span>.</p>
<p>We can use <span class="math inline">\(f_D\)</span> to evaluate the probability of the data, <span class="math inline">\(d\)</span>, at every potential value of the parameter <span class="math inline">\(\theta\)</span> we are trying to estimate, and we call this the likelihood <span class="math inline">\(L(\theta)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
L(\theta) = f_D(d | \theta)
\end{aligned}
\]</span></p>
<p>It is tempting to think of the likelihood as a probability distribution. But it is not, because the function does not sum or integrate to 1 with respect to <span class="math inline">\(\theta\)</span> (which is a requirement for a probability distribution function). This is partly why we are defining it (the likelihood) as separate idea: the likelihood is a function of the parameters, and we use it to ask questions about the plausibility of the data (which are fixed), assuming different values of the parameters. If <span class="math inline">\(L(\theta_1|d) &gt; L(\theta_2|d)\)</span>, then the data we have observed are more likely to have occurred if <span class="math inline">\(\theta = \theta_1\)</span> than <span class="math inline">\(\theta = \theta_2\)</span>. We interpret this result as: <span class="math inline">\(\theta_1\)</span> is a more plausible value for <span class="math inline">\(\theta\)</span> than <span class="math inline">\(\theta_2\)</span>.</p>
<p>Ok, so how do we actually calculate the likelihood? Suppose the data are <span class="math inline">\(n\)</span> independent observations, <span class="math inline">\(x_1, x_2, ..., x_n\)</span> each with the density function <span class="math inline">\(f_X(x)\)</span>, which depends on parameter <span class="math inline">\(\theta\)</span>. This means that these observations are independent and identically distributed, and the joint density function (of the <em>data</em> and the <em>likelihood</em>) is given by:</p>
<p><span class="math display">\[
\begin{aligned}
f_{X_1, X_2, ..., X_n}(x_1, x_2, ..., x_n) =&amp; ~ f_X(x_1) * f_X(x_2) * ... * f_X(x_n) \\
=&amp; ~ \prod_{i = 1}^n f_X(x_i)
\end{aligned}
\]</span></p>
<p>where the symbol <span class="math inline">\(\prod\)</span> denotes multiplication in the same way that the symbol <span class="math inline">\(\sum\)</span> denotes addition. Instead of multiplying, we’d prefer to work with sums for numerical reasons. Because the log of a product is the sum of the logarithms of the terms being multiplied,</p>
<p><span class="math display">\[
\begin{aligned}
\text{ln}(yz) = \text{ln}(y) + \text{ln}(z)
\end{aligned}
\]</span></p>
<p>we can express the joint density function as a sum instead:</p>
<p><span class="math display">\[
\begin{aligned}
\text{ln}[f_X(x_1) * f_X(x_2) * ... * f_X(x_n)] =&amp; ~ \sum_{i = 1}^n \text{ln}[f_X(x_i)]
\end{aligned}
\]</span></p>
<p>and define it as follows:</p>
<p><span class="math display">\[
\begin{aligned}
l(\theta) = \text{ln}[L(\theta)]
\end{aligned}
\]</span></p>
<p>The likelihood provides a framework for estimation and inference.</p>
<div id="exercise-set-9-1" class="section level3">
<h3><span class="header-section-number">9.0.1</span> Exercise set 9-1</h3>
<ol style="list-style-type: decimal">
<li>Is this statement true / false?</li>
</ol>
<p><em>The value of <span class="math inline">\(\theta\)</span> that maximizes <span class="math inline">\(L(\theta)\)</span> is the most probable value of <span class="math inline">\(\theta\)</span> given the observed data.</em></p>
<p>False - frequentists consider <span class="math inline">\(\theta\)</span> to be fixed, and thus has no probability distribution. See Edge explanation. Change to:</p>
<p><em>The value of <span class="math inline">\(\theta\)</span> that maximizes <span class="math inline">\(L(\theta)\)</span> is the one that maximizes the probability of obtaining the observed data.</em></p>
<ol start="2" style="list-style-type: decimal">
<li>Write all of the following in the simplest form:</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>The pdf of <span class="math inline">\(X\)</span>, a Normal(<span class="math inline">\(\mu, \sigma^2\)</span>) random variable:</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
f_X(x) = \frac{1}{\sigma \sqrt{2 \pi}}e^{-\frac{(x - \mu)^2}{2 \sigma^2}}
\end{aligned}
\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>The log-likelihood of <span class="math inline">\(\mu\)</span> given an observation of <span class="math inline">\(x\)</span>, which is assumed to be an instance of <span class="math inline">\(X\)</span>:</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
l(\mu) =&amp; ~ \text{ln}(\frac{1}{\sigma \sqrt{2 \pi}}e^{-\frac{(x - \mu)^2}{2 \sigma^2}}) \\
       =&amp; ~ \text{ln}(\frac{1}{\sigma \sqrt{2 \pi}}) + \text{ln}(e^{-\frac{(x - \mu)^2}{2 \sigma^2}}) \\
       =&amp; ~ \text{ln}(\frac{1}{\sigma \sqrt{2 \pi}}) - \frac{(x - \mu)^2}{2 \sigma^2} \\
\end{aligned}
\]</span></p>
<ol start="3" style="list-style-type: lower-alpha">
<li>The joint density of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, two independent Normal(<span class="math inline">\(\mu, \sigma^2\)</span>) random variables.</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
f_{X_1, X_2}(x_1, x_2) =&amp; ~ \frac{1}{\sigma \sqrt{2 \pi}}e^{-\frac{(x_1 - \mu)^2}{2 \sigma^2}} *
                         \frac{1}{\sigma \sqrt{2 \pi}}e^{-\frac{(x_2 - \mu)^2}{2 \sigma^2}} \\
                        &amp; \text{consider each of these terms as a, b, and c:} \\
                       =&amp; ~ (ab)*(ac) \\
                       =&amp; ~ a^2(bc) \\
                        &amp; \text{therefore we can write} \\
                       =&amp; ~ \frac{1}{\sigma^2 2 \pi} e^{-\frac{(x_1 - \mu)^2}{2 \sigma^2}} * 
                            e^{-\frac{(x_2 - \mu)^2}{2 \sigma^2}} \\
                        &amp; \text{we know that } a^m * a^n = a^{m + n} \text{, so} \\
                        =&amp; ~ \frac{1}{\sigma^2 2 \pi} e^{-\frac{(x_1 - \mu)^2}
                        {2 \sigma^2} - \frac{(x_2 - \mu)^2}{2 \sigma^2}} \\  
                        &amp; \text{we can factor out a -1} \\
                        =&amp; ~ \frac{1}{\sigma^2 2 \pi} e^{- \left ( \frac{(x_1 - \mu)^2}
                        {2 \sigma^2} + \frac{(x_2 - \mu)^2}{2 \sigma^2} \right )} \\  
                        &amp; \text{and we are left with} \\
                       =&amp; ~ \frac{1}{\sigma^2 2 \pi} e^{-\frac{(x_1 - \mu)^2 + (x_2 - \mu)^2}{2 \sigma^2}} \\   
\end{aligned}
\]</span></p>
<ol start="4" style="list-style-type: lower-alpha">
<li>The log-likelihood of <span class="math inline">\(\mu\)</span> given an observation of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, which are assumed to be instances of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>:</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
l(\mu) =&amp; ~ \text{ln} \left (\frac{1}{\sigma^2 2 \pi} e^{-\frac{(x_1 - \mu)^2 + (x_2 - \mu)^2}{2 \sigma^2}} \right) \\
       =&amp; ~ \text{ln}(\frac{1}{\sigma^2 2 \pi}) + \text{ln}( e^{-\frac{(x_1 - \mu)^2 + (x_2 - \mu)^2}{2 \sigma^2}}) \\
       =&amp; ~ \text{ln}(\frac{1}{\sigma^2 2 \pi}) -\frac{(x_1 - \mu)^2}{2 \sigma^2} -\frac{(x_2 - \mu)^2}{2 \sigma^2}\\
       &amp; \text{though I don&#39;t know how to get to Edge&#39;s answer, apart from starting with our answer in 1b:} \\
       =&amp; ~ 2~\text{ln}(\frac{1}{\sigma \sqrt {2 \pi}}) -\frac{(x_1 - \mu)^2}{2 \sigma^2} -\frac{(x_2 - \mu)^2}{2 \sigma^2}\\
\end{aligned}
\]</span></p>
<ol start="5" style="list-style-type: lower-alpha">
<li>The log-likelihood of <span class="math inline">\(\mu\)</span> given observations of <span class="math inline">\(x_1, x_2, ..., x_n\)</span>, which are assumed to be instances of <span class="math inline">\(n\)</span> independent random variables with a Normal(<span class="math inline">\(\mu, \sigma^2\)</span>) distribution:</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
l(\mu) =&amp; ~ n~\text{ln}(\frac{1}{\sigma \sqrt {2 \pi}}) - \sum_{i = 1}^n \frac{(x_i - \mu)^2}{2 \sigma^2} \\
\end{aligned}
\]</span></p>
</div>
<div id="parametric-estimation-using-maximum-likelihood" class="section level2">
<h2><span class="header-section-number">9.1</span> Parametric estimation using maximum likelihood</h2>
<blockquote>
<p>The <strong>maximum-likelihood estimate</strong> of a parameter is the value of the parameter that maximizes the probability of observing the data</p>
</blockquote>
<p>The maximum-likelihood estimate of the parameter <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\hat \theta\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\hat \theta =&amp; ~ \text{argmax} ~ L(\theta) \\
             &amp; \text{or} \\
\hat \theta =&amp; ~ \text{argmax} ~ l(\theta)
\end{aligned}
\]</span></p>
<ul>
<li>‘argmax’ means ‘argument of the maximum’</li>
<li>in this case, it is the value that maximizes the likelihood <span class="math inline">\(L(\theta)\)</span> or log-likelihood <span class="math inline">\(l(\theta)\)</span></li>
<li>usually, we use the log-likelihood to find <span class="math inline">\(\hat \theta\)</span></li>
</ul>
<p>To identify <span class="math inline">\(\hat \theta\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Write down the likelihood function <span class="math inline">\(L(\theta)\)</span></li>
<li>Take the log of likelihood function to get <span class="math inline">\(l(\theta)\)</span>, and simplify</li>
<li>Maximize <span class="math inline">\(l(\theta)\)</span> in terms of <span class="math inline">\(\theta\)</span>. The value of <span class="math inline">\(\theta\)</span> that maximizes <span class="math inline">\(l(\theta)\)</span> is the maximum-likelihood estimator <span class="math inline">\(\hat \theta\)</span></li>
</ol>
<p>Edge goes through each of these steps to find the MLE of the parameter <span class="math inline">\(\lambda\)</span> of the exponential distribution, but I won’t repeat it here - mostly because I will probably never complete these steps in practice (if I tried to do this at all, I’d probably try to solve for the MLE numerically).</p>
<p>The single most important assumption is that the data are actually being generated by the likelihood function we specified. If we are right, then the MLE has some desirable properties:</p>
<ul>
<li>consistency: <span class="math inline">\(\hat \theta\)</span> converges to the true value of <span class="math inline">\(\theta\)</span> as the sample size increases</li>
<li>asymptotically normally distributed: as the <span class="math inline">\(n\)</span> approaches infinity, the distribution of the MLE approaches a normal distribution</li>
<li>asymptotic efficiency: in the large samples, there are no consistent estimators with lower mean squared error than MLE</li>
<li>functional invariance: this means you can estimate a value (using a defined function) based on <span class="math inline">\(\hat \theta\)</span>, and it too will be a maximum-likelihood estimator for that derived value</li>
</ul>
<p>Three (repeated) caveats, for emphasis:</p>
<ol style="list-style-type: decimal">
<li>The MLE is <em>NOT</em> the value of the parameter that is most probable given the data. It is the parameter value that makes <em>the data</em> most probable</li>
<li>The appeal of the MLE is efficiency, but this efficiency is defined with respect to the mean squared error, which may not be the appropriate loss function to the problem at hand</li>
<li>The MLE is only meaningful if the model plausibly generated the data</li>
</ol>
<div id="exercise-set-9-2" class="section level3">
<h3><span class="header-section-number">9.1.1</span> Exercise set 9-2</h3>
<ol style="list-style-type: decimal">
<li></li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>The likelihood function for a Bernoulli distributed dataset is:</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
L(p) =&amp; ~ \prod_{i = 1}^n p^{x_i} (1-p)^{1 - x_i}
\end{aligned}
\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>The log-likelihood is:</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
l(p) =&amp; ~ \text{ln}[L(p)] \\
     =&amp; ~ \text{ln} \left [\prod_{i = 1}^n p^{x_i} (1-p)^{1 - x_i} \right] \\
     =&amp; ~ \sum_{i = 1}^n \text{ln}[p^{x_i} (1-p)^{1 - x_i}] \\
     =&amp; ~ \sum_{i = 1}^n \text{ln}[p^{x_i}] + \text{ln}[(1-p)^{1 - x_i}] \\
     =&amp; ~ \sum_{i = 1}^n x_i \text{ln}(p) + (1 - x_i)\text{ln}(1-p) \\
\end{aligned}
\]</span><br />
c. First, we will draw ten observations from a Bernoulli(0.6) distribution. We’ll use a binomial distribution with <span class="math inline">\(n = k\)</span> and <span class="math inline">\(p = 0.6\)</span>, recognizing that the binomial is basically the sum of <em>n</em> independent Bernoulli trials with <span class="math inline">\(p = 0.6\)</span>.</p>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb359-1"><a href="parametric.html#cb359-1"></a>n &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb359-2"><a href="parametric.html#cb359-2"></a>trials &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb359-3"><a href="parametric.html#cb359-3"></a>p &lt;-<span class="st"> </span><span class="fl">0.6</span></span>
<span id="cb359-4"><a href="parametric.html#cb359-4"></a><span class="kw">set.seed</span>(<span class="dv">121</span>)</span>
<span id="cb359-5"><a href="parametric.html#cb359-5"></a>x &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="dt">n =</span> n, <span class="dt">size =</span> trials, <span class="dt">prob =</span> p)</span>
<span id="cb359-6"><a href="parametric.html#cb359-6"></a>x</span></code></pre></div>
<pre><code>##  [1] 1 0 1 0 1 1 1 0 1 1</code></pre>
<p>Because we prefer to use log-likelihoods for mathematical reasons, let’s translate this statement into R code:</p>
<p><span class="math display">\[
\begin{aligned}
l(p) =&amp; ~ \sum_{i = 1}^n x_i \text{ln}(p) + (1 - x_i)\text{ln}(1-p) \\
\end{aligned}
\]</span></p>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb361-1"><a href="parametric.html#cb361-1"></a>ln_vector &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="kw">length</span>(x))</span>
<span id="cb361-2"><a href="parametric.html#cb361-2"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x)){</span>
<span id="cb361-3"><a href="parametric.html#cb361-3"></a>  ln_vector[i] &lt;-<span class="st"> </span>x[i]<span class="op">*</span><span class="kw">log</span>(p) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x[i]) <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p)}</span>
<span id="cb361-4"><a href="parametric.html#cb361-4"></a>ln_vector</span></code></pre></div>
<pre><code>##  [1] -0.5108256 -0.9162907 -0.5108256 -0.9162907 -0.5108256 -0.5108256
##  [7] -0.5108256 -0.9162907 -0.5108256 -0.5108256</code></pre>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb363-1"><a href="parametric.html#cb363-1"></a><span class="kw">exp</span>(ln_vector)</span></code></pre></div>
<pre><code>##  [1] 0.6 0.4 0.6 0.4 0.6 0.6 0.6 0.4 0.6 0.6</code></pre>
<p>To recap, we’ve calculated the log-likelihood of each observation (0 or 1), given a probability of 0.6. Above, I also took the exponent of our vector, to return the likelihood of each observation. Assuming that these observations are IID, we can add up the individual log-likelihoods and get the total log-likelihood of the dataset (n = 10), given <span class="math inline">\(p = 0.6\)</span>. We can also take the exponent of the total log-likelihood to get the likelihood:</p>
<div class="sourceCode" id="cb365"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb365-1"><a href="parametric.html#cb365-1"></a><span class="kw">sum</span>(ln_vector)</span></code></pre></div>
<pre><code>## [1] -6.324652</code></pre>
<div class="sourceCode" id="cb367"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb367-1"><a href="parametric.html#cb367-1"></a><span class="kw">exp</span>(<span class="kw">sum</span>(ln_vector))</span></code></pre></div>
<pre><code>## [1] 0.00179159</code></pre>
<p>That is just for a single value of p. Now we’ll repeat this process for a vector of p from 0 to 1. First we create function that will calculate the log-likelihood of a dataset given one value of <span class="math inline">\(p\)</span>, based on the for loop above:</p>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb369-1"><a href="parametric.html#cb369-1"></a><span class="co"># Function to calculate the log-likelihood for a vector of observations</span></span>
<span id="cb369-2"><a href="parametric.html#cb369-2"></a>ln_bern &lt;-<span class="st"> </span><span class="cf">function</span>(p, x){</span>
<span id="cb369-3"><a href="parametric.html#cb369-3"></a>  ln_vector &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="kw">length</span>(x))</span>
<span id="cb369-4"><a href="parametric.html#cb369-4"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x)){</span>
<span id="cb369-5"><a href="parametric.html#cb369-5"></a>    ln_vector[i] &lt;-<span class="st"> </span>x[i]<span class="op">*</span><span class="kw">log</span>(p) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x[i]) <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p)}</span>
<span id="cb369-6"><a href="parametric.html#cb369-6"></a>  ln &lt;-<span class="st"> </span><span class="kw">sum</span>(ln_vector)</span>
<span id="cb369-7"><a href="parametric.html#cb369-7"></a>  <span class="kw">return</span>(ln)</span>
<span id="cb369-8"><a href="parametric.html#cb369-8"></a>}</span></code></pre></div>
<p>Now we create a vector for <span class="math inline">\(p\)</span>, loop through all the values, then plot:</p>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb370-1"><a href="parametric.html#cb370-1"></a><span class="co"># Sequence of p</span></span>
<span id="cb370-2"><a href="parametric.html#cb370-2"></a>p_vec &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.001</span>, <span class="fl">0.999</span>, <span class="dt">by =</span> <span class="fl">0.001</span>)</span>
<span id="cb370-3"><a href="parametric.html#cb370-3"></a>ln_vec &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="kw">length</span>(p_vec))</span>
<span id="cb370-4"><a href="parametric.html#cb370-4"></a></span>
<span id="cb370-5"><a href="parametric.html#cb370-5"></a><span class="co"># Log-likelihood of the data</span></span>
<span id="cb370-6"><a href="parametric.html#cb370-6"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(p_vec)){</span>
<span id="cb370-7"><a href="parametric.html#cb370-7"></a>  ln_vec[i] &lt;-<span class="st"> </span><span class="kw">ln_bern</span>(<span class="dt">p =</span> p_vec[i], <span class="dt">x =</span> x)</span>
<span id="cb370-8"><a href="parametric.html#cb370-8"></a>}</span>
<span id="cb370-9"><a href="parametric.html#cb370-9"></a></span>
<span id="cb370-10"><a href="parametric.html#cb370-10"></a><span class="co"># Get the value of p that maximizes the log-likelihood</span></span>
<span id="cb370-11"><a href="parametric.html#cb370-11"></a>max_p &lt;-<span class="st"> </span>p_vec[<span class="kw">which.max</span>(ln_vec)]</span>
<span id="cb370-12"><a href="parametric.html#cb370-12"></a>max_ln &lt;-<span class="st"> </span>ln_vec[<span class="kw">which.max</span>(ln_vec)]</span>
<span id="cb370-13"><a href="parametric.html#cb370-13"></a></span>
<span id="cb370-14"><a href="parametric.html#cb370-14"></a><span class="co"># Likelihood of the data</span></span>
<span id="cb370-15"><a href="parametric.html#cb370-15"></a>Ln_vec &lt;-<span class="st"> </span><span class="kw">exp</span>(ln_vec)</span>
<span id="cb370-16"><a href="parametric.html#cb370-16"></a>max_Ln &lt;-<span class="st"> </span>Ln_vec[<span class="kw">which.max</span>(Ln_vec)]</span>
<span id="cb370-17"><a href="parametric.html#cb370-17"></a></span>
<span id="cb370-18"><a href="parametric.html#cb370-18"></a><span class="co"># Plot</span></span>
<span id="cb370-19"><a href="parametric.html#cb370-19"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb370-20"><a href="parametric.html#cb370-20"></a><span class="kw">plot</span>(<span class="kw">table</span>(x), <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Frequency&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;10 observations, p = 0.6&quot;</span>)</span>
<span id="cb370-21"><a href="parametric.html#cb370-21"></a><span class="kw">plot</span>(p_vec, Ln_vec, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;p&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Likelihood&quot;</span>)</span>
<span id="cb370-22"><a href="parametric.html#cb370-22"></a><span class="kw">abline</span>(<span class="dt">v =</span> <span class="fl">0.6</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>)</span>
<span id="cb370-23"><a href="parametric.html#cb370-23"></a><span class="kw">points</span>(<span class="dt">x =</span> max_p, <span class="dt">y =</span> max_Ln, <span class="dt">pch =</span> <span class="dv">19</span>)</span>
<span id="cb370-24"><a href="parametric.html#cb370-24"></a><span class="kw">plot</span>(p_vec, ln_vec, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;p&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Log likelihood&quot;</span>)</span>
<span id="cb370-25"><a href="parametric.html#cb370-25"></a><span class="kw">abline</span>(<span class="dt">v =</span> <span class="fl">0.6</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>)</span>
<span id="cb370-26"><a href="parametric.html#cb370-26"></a><span class="kw">points</span>(<span class="dt">x =</span> max_p, <span class="dt">y =</span> max_ln, <span class="dt">pch =</span> <span class="dv">19</span>)</span></code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-5"></span>
<img src="09_parametric-estimation_files/figure-html/unnamed-chunk-5-1.png" alt="This is awesome. We just found the maximum likelihood estimate of p (the black points), going from the mathematical expression we derived, and translating it to R code. The red dashed line is the true value of p. Savor this moment." width="672" />
<p class="caption">
Figure 9.1: This is awesome. We just found the maximum likelihood estimate of p (the black points), going from the mathematical expression we derived, and translating it to R code. The red dashed line is the true value of p. Savor this moment.
</p>
</div>
<p>Below is Edge’s answer, but there is a disconnect between the answer in part b and translating the mathematical expression into R code. Specifically, his function takes a shortcut by using the binomial distribution instead of the Bernoulli. But it is a good check on my approach above:</p>
<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb371-1"><a href="parametric.html#cb371-1"></a>n &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb371-2"><a href="parametric.html#cb371-2"></a>trials &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb371-3"><a href="parametric.html#cb371-3"></a>p &lt;-<span class="st"> </span><span class="fl">0.6</span></span>
<span id="cb371-4"><a href="parametric.html#cb371-4"></a><span class="kw">set.seed</span>(<span class="dv">121</span>)</span>
<span id="cb371-5"><a href="parametric.html#cb371-5"></a>x &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="dt">n =</span> n, <span class="dt">size =</span> trials, <span class="dt">prob =</span> p)</span>
<span id="cb371-6"><a href="parametric.html#cb371-6"></a></span>
<span id="cb371-7"><a href="parametric.html#cb371-7"></a><span class="co"># Function to calculate likelihood for a vector of observations</span></span>
<span id="cb371-8"><a href="parametric.html#cb371-8"></a>Ln_bern &lt;-<span class="st"> </span><span class="cf">function</span>(p, x){</span>
<span id="cb371-9"><a href="parametric.html#cb371-9"></a>  k &lt;-<span class="st"> </span><span class="kw">sum</span>(x)</span>
<span id="cb371-10"><a href="parametric.html#cb371-10"></a>  n &lt;-<span class="st"> </span><span class="kw">length</span>(x)</span>
<span id="cb371-11"><a href="parametric.html#cb371-11"></a>  Ln &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="kw">length</span>(p))</span>
<span id="cb371-12"><a href="parametric.html#cb371-12"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(p)){</span>
<span id="cb371-13"><a href="parametric.html#cb371-13"></a>    Ln[i] &lt;-<span class="st"> </span><span class="kw">prod</span>(p[i]<span class="op">^</span>k <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p[i])<span class="op">^</span>(n <span class="op">-</span><span class="st"> </span>k))</span>
<span id="cb371-14"><a href="parametric.html#cb371-14"></a>  }</span>
<span id="cb371-15"><a href="parametric.html#cb371-15"></a>  <span class="kw">return</span>(Ln)</span>
<span id="cb371-16"><a href="parametric.html#cb371-16"></a>}</span>
<span id="cb371-17"><a href="parametric.html#cb371-17"></a></span>
<span id="cb371-18"><a href="parametric.html#cb371-18"></a><span class="co"># Sequence of p</span></span>
<span id="cb371-19"><a href="parametric.html#cb371-19"></a>p_vec &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.001</span>, <span class="fl">0.999</span>, <span class="dt">by =</span> <span class="fl">0.001</span>)</span>
<span id="cb371-20"><a href="parametric.html#cb371-20"></a><span class="co"># Likelihood of the data</span></span>
<span id="cb371-21"><a href="parametric.html#cb371-21"></a>Ln &lt;-<span class="st"> </span><span class="kw">Ln_bern</span>(<span class="dt">p =</span> p_vec, <span class="dt">x =</span> x)</span>
<span id="cb371-22"><a href="parametric.html#cb371-22"></a>max_p &lt;-<span class="st"> </span>p_vec[<span class="kw">which.max</span>(Ln)]</span>
<span id="cb371-23"><a href="parametric.html#cb371-23"></a>max_Ln &lt;-<span class="st"> </span>Ln[<span class="kw">which.max</span>(Ln)]</span>
<span id="cb371-24"><a href="parametric.html#cb371-24"></a><span class="co"># Log-likelihood of the data</span></span>
<span id="cb371-25"><a href="parametric.html#cb371-25"></a>ln &lt;-<span class="st"> </span><span class="kw">log</span>(Ln)</span>
<span id="cb371-26"><a href="parametric.html#cb371-26"></a>max_ln &lt;-<span class="st"> </span>ln[<span class="kw">which.max</span>(ln)]</span>
<span id="cb371-27"><a href="parametric.html#cb371-27"></a></span>
<span id="cb371-28"><a href="parametric.html#cb371-28"></a><span class="co"># Plot</span></span>
<span id="cb371-29"><a href="parametric.html#cb371-29"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb371-30"><a href="parametric.html#cb371-30"></a><span class="kw">plot</span>(<span class="kw">table</span>(x), <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Frequency&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;10 observations, p = 0.6&quot;</span>)</span>
<span id="cb371-31"><a href="parametric.html#cb371-31"></a><span class="kw">plot</span>(p_vec, Ln, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;p&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Likelihood&quot;</span>)</span>
<span id="cb371-32"><a href="parametric.html#cb371-32"></a><span class="kw">abline</span>(<span class="dt">v =</span> <span class="fl">0.6</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>)</span>
<span id="cb371-33"><a href="parametric.html#cb371-33"></a><span class="kw">points</span>(<span class="dt">x =</span> max_p, <span class="dt">y =</span> max_Ln, <span class="dt">pch =</span> <span class="dv">19</span>)</span>
<span id="cb371-34"><a href="parametric.html#cb371-34"></a><span class="kw">plot</span>(p_vec, ln, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;p&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Log likelihood&quot;</span>)</span>
<span id="cb371-35"><a href="parametric.html#cb371-35"></a><span class="kw">abline</span>(<span class="dt">v =</span> <span class="fl">0.6</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>)</span>
<span id="cb371-36"><a href="parametric.html#cb371-36"></a><span class="kw">points</span>(<span class="dt">x =</span> max_p, <span class="dt">y =</span> max_ln, <span class="dt">pch =</span> <span class="dv">19</span>)</span></code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-6"></span>
<img src="09_parametric-estimation_files/figure-html/unnamed-chunk-6-1.png" alt="Same results as above. Ponder the differences in the code for calculating the likelihood." width="672" />
<p class="caption">
Figure 9.2: Same results as above. Ponder the differences in the code for calculating the likelihood.
</p>
</div>
<ol start="4" style="list-style-type: lower-alpha">
<li></li>
</ol>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb372-1"><a href="parametric.html#cb372-1"></a>max_p</span></code></pre></div>
<pre><code>## [1] 0.7</code></pre>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb374-1"><a href="parametric.html#cb374-1"></a><span class="kw">summary</span>(x)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    0.00    0.25    1.00    0.70    1.00    1.00</code></pre>
<p>The MLE is 0.7, which is the same as the proportion of observations equal to 1.</p>
<ol start="2" style="list-style-type: decimal">
<li>Suppose that <span class="math inline">\(X_1, X_2, ..., X_n\)</span> are distributed as Normal(<span class="math inline">\(\theta, \sigma^2\)</span>).</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>What is the MLE of <span class="math inline">\(\theta\)</span>?</li>
</ol>
<p>First, we get the expression for the log-likelihood of <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
l(\theta) =&amp; ~ n~\text{ln}(\frac{1}{\sigma \sqrt {2 \pi}}) - \sum_{i = 1}^n \frac{(x_i - \theta)^2}{2 \sigma^2} \\
          =&amp; ~ n~\text{ln}(\frac{1}{\sigma \sqrt {2 \pi}}) - 
          \frac{1}{2 \sigma^2} 
          \sum_{i = 1}^n (x_i - \theta)^2 \\
          =&amp; ~ n~\text{ln}(\frac{1}{\sigma \sqrt {2 \pi}}) - 
          \frac{1}{2 \sigma^2} 
          \sum_{i = 1}^n (x_i^2 - 2x_i \theta + \theta^2) \\
\end{aligned}
\]</span></p>
<p>Then we take the derivative and show that the MLE is the sample mean, <span class="math inline">\(\bar x\)</span>; see Edge solution for the remaining details.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>See Edge solution.</li>
</ol>
</div>
<div id="maximum-likelihood-estimation-for-simple-linear-regression" class="section level3">
<h3><span class="header-section-number">9.1.2</span> Maximum-likelihood estimation for simple linear regression</h3>
<p><span class="math display">\[
\begin{aligned}
Y_i =&amp; \alpha + \beta x_i + \epsilon
\end{aligned}
\]</span></p>
<ul>
<li><span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are fixed constants</li>
<li>the observations <span class="math inline">\(x_i\)</span> are no longer considered random, but are fixed constants (this is different from the method of moments estimator)</li>
<li>the only random variable in the model is the disturbance term, <span class="math inline">\(\epsilon\)</span>, and it is assumed to have a parametric distribution (~ Normal(<span class="math inline">\(0, \sigma^2\)</span>))</li>
<li>the dependent variable is considered to be random (despite the fact that it is observed?)</li>
</ul>
<div id="assumptions-2" class="section level4">
<h4><span class="header-section-number">9.1.2.1</span> Assumptions</h4>
<ul>
<li><p><em>Linearity</em>: <span class="math inline">\(\text{E}(\epsilon | X = x) = ~ 0\)</span></p></li>
<li><p><em>Homoscedasticity</em>: <span class="math inline">\(\text{Var}(\epsilon | X = x) = 0\)</span> for all <span class="math inline">\(x\)</span></p></li>
<li><p><em>Independence of units</em>: for all <em>i</em> and <span class="math inline">\(j \neq i\)</span>, <span class="math inline">\(\epsilon_i\)</span> is independent of <span class="math inline">\(\epsilon_j\)</span></p></li>
<li><p><em>Distribution</em>: for all <em>i</em>, <span class="math inline">\(\epsilon_i\)</span> is drawn from a normal distribution</p></li>
</ul>
<p>Edge goes through the steps necessary to derive the maximum likelihood estimator for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, but I won’t repeat it here. It turns out that the MLE for these terms is equivalent to the expressions we derived using least-squares in chapter 3.</p>
<blockquote>
<p>Thus we can interpret the least-squares line as the maximum likelihood estimator of the ‘true’ line under the assumption of normally distributed, independent, homoscedastic disturbances</p>
</blockquote>
<p>It also turns out the the ML estimates are the same as the MOM estimates, but the assumptions used to justify each are different (MOM estimates do not invoke normality and constant variance of the disturbances), and thus provide different guarantees (e.g., ML estimates are asymptotically efficient and functionally invariant)</p>
</div>
</div>
<div id="exercise-set-9-3" class="section level3">
<h3><span class="header-section-number">9.1.3</span> Exercise set 9-3</h3>
<ol style="list-style-type: decimal">
<li><p>See attempted solution on paper. I was able to get close to Edge’s answer, but there is a pesky <span class="math inline">\(n\)</span> term that I cannot recreate..</p></li>
<li><p>Skipped</p></li>
<li><p>Skipped</p></li>
<li><p>Skipped</p></li>
</ol>
</div>
</div>
<div id="parametric-interval-estimation-the-direct-approach-and-fisher-information" class="section level2">
<h2><span class="header-section-number">9.2</span> Parametric interval estimation: the direct approach and Fisher information</h2>
<p>Three approaches for estimating the variance of a maximum-likelihood estimator:</p>
<ol style="list-style-type: decimal">
<li><p>Use a <em>resampling method</em>, like the bootstrap</p></li>
<li><p>Use the <em>direct approach</em>: analyze mathematically the specific model of interest; e.g., problem 2 of exercise set 9-3 carried out this direct approach for the variance of <span class="math inline">\(\hat \beta\)</span></p></li>
<li><p>Use the <em>Fisher information</em>, a general result for maximum-likelihood estimation. This entails taking the second derivative of the log-likelihood function to understand the shape (steepness) of the function around the maximum-likelihood estimate; a steeper function indicates less uncertainty and vice versa. See Edge’s Figure 9-3.</p></li>
</ol>
<div id="exercise-set-9-4" class="section level3">
<h3><span class="header-section-number">9.2.1</span> Exercise set 9-4</h3>
<ol style="list-style-type: decimal">
<li>Skipped (using the direct approach and Fisher information to get the variance of the Poisson distribution)</li>
</ol>
</div>
</div>
<div id="parametric-hypothesis-testing-using-the-wald-test" class="section level2">
<h2><span class="header-section-number">9.3</span> Parametric hypothesis testing using the Wald test</h2>
<p>As with interval estimation above, we have three options for hypothesis testing in a parametric framework:</p>
<ol style="list-style-type: decimal">
<li><p><em>Resampling</em>; e.g., a permutation test</p></li>
<li><p><em>Direct approach</em>, where we design a test specific to the model in question</p></li>
<li><p><em>Wald test</em> or the <em>likelihood ratio test</em>, both of which apply to a wide range of problems in maximum-likelihood estimation</p></li>
</ol>
<div id="wald-test" class="section level3">
<h3><span class="header-section-number">9.3.1</span> Wald test</h3>
<p>Suppose we want to test the hypothesis that a parameter <span class="math inline">\(\theta\)</span> is equal to a hypothesized value <span class="math inline">\(\theta_0\)</span>. Consider <span class="math inline">\(\theta_0\)</span> the null hypothesis.</p>
<p>For many models, the asymptotic distribution of the maximum-likelihood estimator <span class="math inline">\(\hat \theta\)</span> is normal, with an expectation equal to <span class="math inline">\(\theta\)</span>.</p>
<p>We define the test statistic, <span class="math inline">\(W\)</span>, as:</p>
<p><span class="math display">\[
\begin{aligned}
W =&amp; ~ \frac{\hat \theta - \theta_0}{\sqrt{\text{Var}(\hat \theta)}}
\end{aligned}
\]</span></p>
<p>If the null hypothesis is true, the quantity in the numerator is 0. We can further say (though I don’t fully understand why) that for large sample sizes, this quantity is normally distributed with expectation 0 and variance 1. (why the variance is 1, is unclear). But let’s move on.</p>
<p>We do not know <span class="math inline">\(\text{Var}(\hat \theta)\)</span>, but we can estimate it as <span class="math inline">\(\widehat{\text{Var}}(\hat \theta)\)</span> using direct methods or the observed Fisher information, giving a test statistic <span class="math inline">\(W^*\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
W \approx W^* =&amp; ~ \frac{\hat \theta - \theta_0}
                       {\sqrt{\widehat{\text{Var}}(\hat \theta)}}
\end{aligned}
\]</span></p>
<p>For large samples, if the null hypothesis is true, <span class="math inline">\(W^*\)</span> will have an approximate Normal(0,1) distribution. That means we can compare the calculated <span class="math inline">\(W^*\)</span> with a Normal(0,1) distribution. The approximate two-sided <span class="math inline">\(p\)</span> arising from an observed value of <span class="math inline">\(W^*\)</span> is:</p>
<p><span class="math display">\[
\begin{aligned}
p_W = 2 \Phi (-|W^*|)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\Phi\)</span> is the cumulative distribution function of the Normal(0,1) distribution. The ‘2’ is there because the test is two-sided.</p>
</div>
<div id="exercise-set-9-5" class="section level3">
<h3><span class="header-section-number">9.3.2</span> Exercise set 9-5</h3>
<ol style="list-style-type: decimal">
<li>Compute the <span class="math inline">\(p\)</span> value for the Wald test of the hypothesis that <span class="math inline">\(\beta = 0\)</span> using the Anscombe data.</li>
</ol>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb376-1"><a href="parametric.html#cb376-1"></a><span class="co"># Original fertilizer use and cereal yield data</span></span>
<span id="cb376-2"><a href="parametric.html#cb376-2"></a>d &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> anscombe<span class="op">$</span>x1, <span class="dt">y =</span> anscombe<span class="op">$</span>y1)</span></code></pre></div>
<p>Use the least-squares slope to find <span class="math inline">\(\hat\beta\)</span>:
<span class="math display">\[
\begin{aligned}
            &amp; \text{(eq. 3.9)} \\
\tilde\beta &amp; = \frac{\sum_{i=1}^n (x_i - \bar x) (y_i - \bar y)}
                   {\sum_{i=1}^n [(x_i - \bar x)^2]}
\end{aligned}
\]</span></p>
<p>Evaluate in <code>R</code>:</p>
<div class="sourceCode" id="cb377"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb377-1"><a href="parametric.html#cb377-1"></a>x &lt;-<span class="st"> </span>anscombe<span class="op">$</span>x1</span>
<span id="cb377-2"><a href="parametric.html#cb377-2"></a>y &lt;-<span class="st"> </span>anscombe<span class="op">$</span>y1</span>
<span id="cb377-3"><a href="parametric.html#cb377-3"></a>n &lt;-<span class="st"> </span><span class="kw">length</span>(x)</span>
<span id="cb377-4"><a href="parametric.html#cb377-4"></a>xbar &lt;-<span class="st"> </span><span class="kw">mean</span>(x)</span>
<span id="cb377-5"><a href="parametric.html#cb377-5"></a>ybar &lt;-<span class="st"> </span><span class="kw">mean</span>(y)</span>
<span id="cb377-6"><a href="parametric.html#cb377-6"></a>b_hat &lt;-<span class="st"> </span><span class="kw">sum</span>((x <span class="op">-</span><span class="st"> </span>xbar)<span class="op">*</span>(y <span class="op">-</span><span class="st"> </span>ybar)) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>((x <span class="op">-</span><span class="st"> </span>xbar)<span class="op">^</span><span class="dv">2</span>)</span></code></pre></div>
<p>Use the least-squares intercept to find <span class="math inline">\(\hat\alpha\)</span>:
<span class="math display">\[
\begin{aligned}
             &amp; \text{(eq. 3.8)} \\
\tilde\alpha &amp; = \bar y - \tilde\beta \bar x
\end{aligned}
\]</span></p>
<p>Evaluate in <code>R</code>:</p>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb378-1"><a href="parametric.html#cb378-1"></a>a_hat &lt;-<span class="st"> </span>ybar <span class="op">-</span><span class="st"> </span>b_hat<span class="op">*</span>xbar</span></code></pre></div>
<p>Use equation 9.14b to estimate the variance of the disturbance term (I don’t know how to create an upside down hat symbol..):</p>
<p><span class="math display">\[
\begin{aligned}
\frac{n}{n-2} \widehat{\sigma^2} =&amp; ~ \frac{1}{n-2} \sum_{i = 1}^n 
                                      (Y_i - \hat\alpha - \hat\beta x_i)^2
\end{aligned}
\]</span></p>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb379-1"><a href="parametric.html#cb379-1"></a>v_hat_dist &lt;-<span class="st"> </span><span class="kw">sum</span>((y <span class="op">-</span><span class="st"> </span>a_hat <span class="op">-</span><span class="st"> </span>b_hat <span class="op">*</span><span class="st"> </span>x)<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span>(n <span class="op">-</span><span class="st"> </span><span class="dv">2</span>)</span></code></pre></div>
<p>Now estimate the variance of the slope using equation 9.12:</p>
<p><span class="math display">\[
\begin{aligned}
            &amp; \text{(eq. 9.12)} \\
\text{Var}(\hat\beta) &amp; = \frac{\sigma^2}
                          {\sum_{i=1}^n [(x_i - \bar x)^2]}
\end{aligned}
\]</span></p>
<div class="sourceCode" id="cb380"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb380-1"><a href="parametric.html#cb380-1"></a>v_hat_beta &lt;-<span class="st"> </span>v_hat_dist<span class="op">/</span><span class="kw">sum</span>((x <span class="op">-</span><span class="st"> </span>xbar)<span class="op">^</span><span class="dv">2</span>)</span></code></pre></div>
<p>Now we plug these estimates into our equation for <span class="math inline">\(W^*\)</span>:</p>
<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb381-1"><a href="parametric.html#cb381-1"></a>B0 &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb381-2"><a href="parametric.html#cb381-2"></a>wald &lt;-<span class="st"> </span>(b_hat <span class="op">-</span><span class="st"> </span>B0) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(v_hat_beta)</span></code></pre></div>
<p>Calculate <span class="math inline">\(p\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
p_W = 2 \Phi (-|W^*|)
\end{aligned}
\]</span></p>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb382-1"><a href="parametric.html#cb382-1"></a><span class="co"># Right tail</span></span>
<span id="cb382-2"><a href="parametric.html#cb382-2"></a><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> wald, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>) </span></code></pre></div>
<pre><code>## [1] 1.110376e-05</code></pre>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb384-1"><a href="parametric.html#cb384-1"></a><span class="co"># Left tail</span></span>
<span id="cb384-2"><a href="parametric.html#cb384-2"></a><span class="kw">pnorm</span>(<span class="dt">q =</span> <span class="op">-</span>wald, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>) </span></code></pre></div>
<pre><code>## [1] 1.110376e-05</code></pre>
<div class="sourceCode" id="cb386"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb386-1"><a href="parametric.html#cb386-1"></a><span class="co"># Two-sided test, using the left tail</span></span>
<span id="cb386-2"><a href="parametric.html#cb386-2"></a><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">pnorm</span>(<span class="dt">q =</span> <span class="op">-</span>wald, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 2.220751e-05</code></pre>
<p>Calculate <span class="math inline">\(T\)</span>:</p>
<div class="sourceCode" id="cb388"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb388-1"><a href="parametric.html#cb388-1"></a><span class="co"># Left tail of t-distribution</span></span>
<span id="cb388-2"><a href="parametric.html#cb388-2"></a><span class="co"># df = 11 observations minus two parameters that we estimated (alpha, beta)</span></span>
<span id="cb388-3"><a href="parametric.html#cb388-3"></a><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">pt</span>(<span class="dt">q =</span> <span class="op">-</span>wald, <span class="dt">df =</span> <span class="dv">11-2</span>)</span></code></pre></div>
<pre><code>## [1] 0.002169629</code></pre>
<p>Compare with <code>lm</code>:</p>
<div class="sourceCode" id="cb390"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb390-1"><a href="parametric.html#cb390-1"></a>fit &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> d)</span>
<span id="cb390-2"><a href="parametric.html#cb390-2"></a><span class="kw">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = d)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.92127 -0.45577 -0.04136  0.70941  1.83882 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)   3.0001     1.1247   2.667  0.02573 * 
## x             0.5001     0.1179   4.241  0.00217 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.237 on 9 degrees of freedom
## Multiple R-squared:  0.6665, Adjusted R-squared:  0.6295 
## F-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217</code></pre>
<p>Notice that the reported t-value is equal to our calculated <span class="math inline">\(W^*\)</span>, and that the reported <span class="math inline">\(p\)</span> uses the <span class="math inline">\(T\)</span> distribution.</p>
<ol start="2" style="list-style-type: decimal">
<li><p>A random variable with a <span class="math inline">\(\chi^2\)</span> distribution has the same distribution as the sum of the squares of <span class="math inline">\(k\)</span> independent, Normal(0,1) random variables. We know that the asymptotic distribution of a maximum-likelihood estimator <span class="math inline">\(\hat\theta\)</span> is normal, with the expectation equal to the true parameter value <span class="math inline">\(\theta\)</span>. If <span class="math inline">\(H_0\)</span> is true, then <span class="math inline">\(W\)</span> is Normal(0,1). If we evaluate <span class="math inline">\(W^2\)</span>, then its asymptotic distribution under <span class="math inline">\(H_0\)</span> is <span class="math inline">\(\chi^2(1)\)</span>. (need to chew on this some more)</p></li>
<li><p>Examining Type 1 error and power of the Wald test when the significance level is <span class="math inline">\(\alpha = 0.05\)</span>.</p></li>
</ol>
<div class="sourceCode" id="cb392"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb392-1"><a href="parametric.html#cb392-1"></a><span class="kw">sim.Wald.B</span>(<span class="dt">n =</span> <span class="dv">10</span>, <span class="dt">nsim =</span> <span class="dv">100</span>, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> <span class="fl">0.1</span>)</span></code></pre></div>
<div class="sourceCode" id="cb393"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb393-1"><a href="parametric.html#cb393-1"></a><span class="co"># 10 pairs</span></span>
<span id="cb393-2"><a href="parametric.html#cb393-2"></a>beta_vec &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>)</span>
<span id="cb393-3"><a href="parametric.html#cb393-3"></a>n_datasets &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb393-4"><a href="parametric.html#cb393-4"></a>p_mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">nrow =</span> n_datasets, <span class="dt">ncol =</span> <span class="kw">length</span>(beta_vec))</span>
<span id="cb393-5"><a href="parametric.html#cb393-5"></a></span>
<span id="cb393-6"><a href="parametric.html#cb393-6"></a>n_pairs &lt;-<span class="st"> </span><span class="dv">10</span> </span>
<span id="cb393-7"><a href="parametric.html#cb393-7"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(beta_vec)){</span>
<span id="cb393-8"><a href="parametric.html#cb393-8"></a>  p_mat[,i] &lt;-<span class="st"> </span><span class="kw">sim.Wald.B</span>(<span class="dt">n =</span> n_pairs, <span class="dt">nsim =</span> n_datasets, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> beta_vec[i])</span>
<span id="cb393-9"><a href="parametric.html#cb393-9"></a>}</span>
<span id="cb393-10"><a href="parametric.html#cb393-10"></a></span>
<span id="cb393-11"><a href="parametric.html#cb393-11"></a><span class="co"># Create summary table</span></span>
<span id="cb393-12"><a href="parametric.html#cb393-12"></a>df_wald &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">beta =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>), </span>
<span id="cb393-13"><a href="parametric.html#cb393-13"></a>                  <span class="dt">n_10 =</span> <span class="kw">colMeans</span>(p_mat <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span>))</span>
<span id="cb393-14"><a href="parametric.html#cb393-14"></a></span>
<span id="cb393-15"><a href="parametric.html#cb393-15"></a></span>
<span id="cb393-16"><a href="parametric.html#cb393-16"></a><span class="co"># 50 pairs</span></span>
<span id="cb393-17"><a href="parametric.html#cb393-17"></a>n_pairs &lt;-<span class="st"> </span><span class="dv">50</span> </span>
<span id="cb393-18"><a href="parametric.html#cb393-18"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(beta_vec)){</span>
<span id="cb393-19"><a href="parametric.html#cb393-19"></a>  p_mat[,i] &lt;-<span class="st"> </span><span class="kw">sim.Wald.B</span>(<span class="dt">n =</span> n_pairs, <span class="dt">nsim =</span> n_datasets, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> beta_vec[i])</span>
<span id="cb393-20"><a href="parametric.html#cb393-20"></a>}</span>
<span id="cb393-21"><a href="parametric.html#cb393-21"></a></span>
<span id="cb393-22"><a href="parametric.html#cb393-22"></a><span class="co"># Add to summary table</span></span>
<span id="cb393-23"><a href="parametric.html#cb393-23"></a>df_wald &lt;-<span class="st"> </span>df_wald <span class="op">%&gt;%</span></span>
<span id="cb393-24"><a href="parametric.html#cb393-24"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">n_50 =</span> <span class="kw">colMeans</span>(p_mat <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span>))</span>
<span id="cb393-25"><a href="parametric.html#cb393-25"></a></span>
<span id="cb393-26"><a href="parametric.html#cb393-26"></a><span class="co"># 100 pairs</span></span>
<span id="cb393-27"><a href="parametric.html#cb393-27"></a>n_pairs &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb393-28"><a href="parametric.html#cb393-28"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(beta_vec)){</span>
<span id="cb393-29"><a href="parametric.html#cb393-29"></a>  p_mat[,i] &lt;-<span class="st"> </span><span class="kw">sim.Wald.B</span>(<span class="dt">n =</span> n_pairs, <span class="dt">nsim =</span> n_datasets, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> beta_vec[i])</span>
<span id="cb393-30"><a href="parametric.html#cb393-30"></a>}</span>
<span id="cb393-31"><a href="parametric.html#cb393-31"></a></span>
<span id="cb393-32"><a href="parametric.html#cb393-32"></a><span class="co"># Add to summary table</span></span>
<span id="cb393-33"><a href="parametric.html#cb393-33"></a>df_wald &lt;-<span class="st"> </span>df_wald <span class="op">%&gt;%</span></span>
<span id="cb393-34"><a href="parametric.html#cb393-34"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">n_100 =</span> <span class="kw">colMeans</span>(p_mat <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span>))</span>
<span id="cb393-35"><a href="parametric.html#cb393-35"></a></span>
<span id="cb393-36"><a href="parametric.html#cb393-36"></a><span class="kw">library</span>(knitr)</span>
<span id="cb393-37"><a href="parametric.html#cb393-37"></a><span class="kw">kable</span>(df_wald, <span class="dt">caption =</span> <span class="st">&quot;Proportion of significance tests for the estimated slope that were significant, where the true beta = 0, 0.1, 0.2 and the number of observations was 10, 50, 100.&quot;</span>)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-18">Table 9.1: </span>Proportion of significance tests for the estimated slope that were significant, where the true beta = 0, 0.1, 0.2 and the number of observations was 10, 50, 100.</caption>
<thead>
<tr class="header">
<th align="right">beta</th>
<th align="right">n_10</th>
<th align="right">n_50</th>
<th align="right">n_100</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.0</td>
<td align="right">0.085</td>
<td align="right">0.062</td>
<td align="right">0.061</td>
</tr>
<tr class="even">
<td align="right">0.1</td>
<td align="right">0.130</td>
<td align="right">0.312</td>
<td align="right">0.513</td>
</tr>
<tr class="odd">
<td align="right">0.2</td>
<td align="right">0.270</td>
<td align="right">0.781</td>
<td align="right">0.971</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="parametric-hypothesis-testing-using-the-likelihood-ratio-test" class="section level2">
<h2><span class="header-section-number">9.4</span> Parametric hypothesis testing using the likelihood-ratio test</h2>
<p>Likelihood-ratio tests can be used to make joint inferences about several parameters at once. They accomplish this by comparing <em>nested</em> models. For example, consider the full model:</p>
<p><span class="math display">\[
\begin{aligned}
Y_i &amp; = \alpha +  \beta_1 x_i +  \beta_2 w_i +  \beta_3 z_i
\end{aligned}
\]</span></p>
<p>In this full model, the <em>free</em> parameters are: <span class="math inline">\(\alpha, \beta_1, \beta_2, \beta_3\)</span>. If we give any of these parameters a <em>fixed</em> value - then the resulting model is considered to be nested within the full model. Often, we compare models where some of the coefficients are set to 0, indicating no effect. So, all of the following models can be considered to be nested within the full model:</p>
<p><span class="math display">\[
\begin{aligned}
Y_i &amp; = \alpha + \beta_3 z_i \\
Y_i &amp; = \alpha + \beta_1 x_i +  \beta_2 w_i \\
Y_i &amp; = \alpha 
\end{aligned}
\]</span></p>
<p>The likelihood-ratio test compares the maximum likelihood of two nested models, and keeps track of the difference in the number of free parameters (<span class="math inline">\(k\)</span>) in the models. The test statistic <span class="math inline">\(\Lambda\)</span> is calculated as:</p>
<p><span class="math display">\[
\begin{aligned}
\Lambda &amp; = 2 \text{ln} \frac{L(\hat\theta)}{L(\hat\theta_0)} \\
        &amp; = 2(l(\hat\theta) - l(\hat\theta_0))
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\hat\theta\)</span> and <span class="math inline">\(\hat\theta_0\)</span> represent the full (or more complex) model and the nested model with a hypothesized value, respectively. In other words, <span class="math inline">\(\hat\theta_0)\)</span> represents a null hypothesis.</p>
<p>According to Wilks’ theorem, if the null hypothesis is true, the statistic <span class="math inline">\(\Lambda\)</span> is asymptotically distributed as <span class="math inline">\(\chi^2 (k)\)</span>, where <span class="math inline">\(k\)</span> is the number of parameters that are constrained in the nested model (representing the null hypothesis) but free in the full (comparison) model. The <span class="math inline">\(p\)</span> value is the probability of obtaining a value of <span class="math inline">\(\Lambda\)</span> as large or larger than the one observed given that the null hypothesis is true:</p>
<p><span class="math display">\[
\begin{aligned}
p_{LRT} = 1 - F_{\chi^2(k)}(\Lambda) 
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(F_{\chi^2(k)}\)</span> is the cumulative distribution of the <span class="math inline">\(\chi^2\)</span> distribution.</p>
<div id="exercise-set-9-6" class="section level3">
<h3><span class="header-section-number">9.4.1</span> Exercise set 9-6</h3>
<ol style="list-style-type: decimal">
<li></li>
</ol>
<ol style="list-style-type: lower-alpha">
<li><p>Answer on paper.</p></li>
<li></li>
</ol>
<p>We wish to calculate the LRT, where the null hypothesis is that the slope is zero.</p>
<p><span class="math display">\[
\begin{aligned}
\Lambda &amp; = 2(l(\hat\theta) - l(\hat\theta_0))
\end{aligned}
\]</span></p>
<p>Here is the formula for the log-likelihood using the least-square estimates.</p>
<p><span class="math display">\[
\begin{aligned}
l(\mu) =&amp; ~ n~\text{ln}(\frac{1}{\sigma \sqrt {2 \pi}}) - 
         \frac{1}{2\sigma^2} \sum_{i = 1}^n (y_i - \alpha - \beta x_i)^2 \\
\end{aligned}
\]</span></p>
<p>We can simplify this equation where <span class="math inline">\(\beta = 0\)</span> and consequently <span class="math inline">\(\hat\alpha = \bar y\)</span> (from the answer to 1a). Here is a function written by Edge to calculate the test statistic <span class="math inline">\(\Lambda\)</span>:</p>
<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb394-1"><a href="parametric.html#cb394-1"></a>lr.stat.slr &lt;-<span class="st"> </span><span class="cf">function</span>(x, y){</span>
<span id="cb394-2"><a href="parametric.html#cb394-2"></a>  n &lt;-<span class="st"> </span><span class="kw">length</span>(x)</span>
<span id="cb394-3"><a href="parametric.html#cb394-3"></a>  <span class="co">#compute MLEs of beta and alpha</span></span>
<span id="cb394-4"><a href="parametric.html#cb394-4"></a>  B.hat &lt;-<span class="st"> </span>(<span class="kw">sum</span>(x<span class="op">*</span>y)<span class="op">-</span><span class="kw">sum</span>(x)<span class="op">*</span><span class="kw">sum</span>(y)<span class="op">/</span>n)<span class="op">/</span>( <span class="kw">sum</span>(x<span class="op">^</span><span class="dv">2</span>) <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(x)<span class="op">^</span><span class="dv">2</span><span class="op">/</span>n)</span>
<span id="cb394-5"><a href="parametric.html#cb394-5"></a>  A.hat &lt;-<span class="st"> </span>(<span class="kw">sum</span>(y) <span class="op">-</span><span class="st"> </span>B.hat<span class="op">*</span><span class="kw">sum</span>(x))<span class="op">/</span>n</span>
<span id="cb394-6"><a href="parametric.html#cb394-6"></a>  <span class="co">#Compute estimated variance of MLE of beta</span></span>
<span id="cb394-7"><a href="parametric.html#cb394-7"></a>  vhat &lt;-<span class="st"> </span><span class="kw">sum</span>((y <span class="op">-</span><span class="st"> </span>A.hat <span class="op">-</span><span class="st"> </span>B.hat<span class="op">*</span>x)<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span>(n<span class="dv">-2</span>)</span>
<span id="cb394-8"><a href="parametric.html#cb394-8"></a>  <span class="co">#likelihood-ratio statistic</span></span>
<span id="cb394-9"><a href="parametric.html#cb394-9"></a>  lr &lt;-<span class="st"> </span>(<span class="kw">sum</span>((y <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(y))<span class="op">^</span><span class="dv">2</span>) <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>((y <span class="op">-</span><span class="st"> </span>A.hat <span class="op">-</span><span class="st"> </span>B.hat<span class="op">*</span>x)<span class="op">^</span><span class="dv">2</span>))<span class="op">/</span>vhat</span>
<span id="cb394-10"><a href="parametric.html#cb394-10"></a>  <span class="kw">return</span>(lr)</span>
<span id="cb394-11"><a href="parametric.html#cb394-11"></a>}</span>
<span id="cb394-12"><a href="parametric.html#cb394-12"></a></span>
<span id="cb394-13"><a href="parametric.html#cb394-13"></a>Lambda &lt;-<span class="st"> </span><span class="kw">lr.stat.slr</span>(x, y)</span></code></pre></div>
<p>And we’ll calculate <span class="math inline">\(p\)</span> using <span class="math inline">\(k = 1\)</span> because we have fixed one parameter:</p>
<div class="sourceCode" id="cb395"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb395-1"><a href="parametric.html#cb395-1"></a><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pchisq</span>(<span class="dt">q =</span> Lambda, <span class="dt">df =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 2.220751e-05</code></pre>
<p>Compare with the Wald test:</p>
<div class="sourceCode" id="cb397"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb397-1"><a href="parametric.html#cb397-1"></a><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">pnorm</span>(<span class="dt">q =</span> <span class="op">-</span>wald, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 2.220751e-05</code></pre>
<p><strong>This give the same answer</strong></p>
<p>In addition, here I use the <code>anova</code> function in R to do the likelihood ratio test on two linear models, one with and the other without slope term.</p>
<div class="sourceCode" id="cb399"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb399-1"><a href="parametric.html#cb399-1"></a>lm1 &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x)</span>
<span id="cb399-2"><a href="parametric.html#cb399-2"></a><span class="kw">summary</span>(lm1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.92127 -0.45577 -0.04136  0.70941  1.83882 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)   3.0001     1.1247   2.667  0.02573 * 
## x             0.5001     0.1179   4.241  0.00217 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.237 on 9 degrees of freedom
## Multiple R-squared:  0.6665, Adjusted R-squared:  0.6295 
## F-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217</code></pre>
<div class="sourceCode" id="cb401"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb401-1"><a href="parametric.html#cb401-1"></a>lm2 &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="dv">1</span>)</span>
<span id="cb401-2"><a href="parametric.html#cb401-2"></a><span class="kw">summary</span>(lm2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ 1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.2409 -1.1859  0.0791  1.0691  3.3391 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   7.5009     0.6125   12.25 2.41e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.032 on 10 degrees of freedom</code></pre>
<div class="sourceCode" id="cb403"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb403-1"><a href="parametric.html#cb403-1"></a><span class="kw">anova</span>(lm1, lm2)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: y ~ x
## Model 2: y ~ 1
##   Res.Df    RSS Df Sum of Sq     F  Pr(&gt;F)   
## 1      9 13.763                              
## 2     10 41.273 -1    -27.51 17.99 0.00217 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Notice that the test statistic is the same, but the <span class="math inline">\(p\)</span> value differs, because it appears to use the F distribution instead of the <span class="math inline">\(\chi^2\)</span> distribution for the hypothesis test.</p>
<!-- ## crap -->
<!-- maximum likelihood estimation is one common method for estimating paremater in a parametric model, just like method of moments. We assume$X_1, X_2..., X_n$ be IID with *PDF* $f(x;\theta)$, define the likelihood function as  -->
<!-- $$ -->
<!-- \mathcal{L}(\theta)=\Pi_{i=1}^nf(X_i;\theta) -->
<!-- $$ -->
<!-- And the log likelihood function is defined by $\mathcal{l}_n(\theta)=log \mathcal{L}_n(\theta)=\sum_{i=1}^n log f(X_i; \theta)$.  -->
<!-- The *maximum likelihood estimator* MLE, denoted by $\hat{\theta}_n$, is the value of $\theta$ that maximizes $\mathcal{L}_n(\theta)$ -->

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="semiparametric.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bayesian.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
