<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Properties of random variables | Notes on Statistical Thinking from Scratch</title>
  <meta name="description" content="Cliff notes for Statistical Thinking from Scratch, by Doc Edge." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Properties of random variables | Notes on Statistical Thinking from Scratch" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Cliff notes for Statistical Thinking from Scratch, by Doc Edge." />
  <meta name="github-repo" content="elahi/stats_from_scratch_edge" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Properties of random variables | Notes on Statistical Thinking from Scratch" />
  
  <meta name="twitter:description" content="Cliff notes for Statistical Thinking from Scratch, by Doc Edge." />
  

<meta name="author" content="Robin Elahi" />


<meta name="date" content="2020-05-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="probability.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-121894527-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-121894527-4');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Notes on Statistical Thinking from Scratch</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="r-eda.html"><a href="r-eda.html"><i class="fa fa-check"></i><b>2</b> R and exploratory data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="r-eda.html"><a href="r-eda.html#inspecting-the-dataframe"><i class="fa fa-check"></i><b>2.1</b> Inspecting the dataframe</a></li>
<li class="chapter" data-level="2.2" data-path="r-eda.html"><a href="r-eda.html#histograms"><i class="fa fa-check"></i><b>2.2</b> Histograms</a></li>
<li class="chapter" data-level="2.3" data-path="r-eda.html"><a href="r-eda.html#summarising-data"><i class="fa fa-check"></i><b>2.3</b> Summarising data</a></li>
<li class="chapter" data-level="2.4" data-path="r-eda.html"><a href="r-eda.html#loops"><i class="fa fa-check"></i><b>2.4</b> Loops</a></li>
<li class="chapter" data-level="2.5" data-path="r-eda.html"><a href="r-eda.html#functions"><i class="fa fa-check"></i><b>2.5</b> Functions</a></li>
<li class="chapter" data-level="2.6" data-path="r-eda.html"><a href="r-eda.html#boxplots"><i class="fa fa-check"></i><b>2.6</b> Boxplots</a></li>
<li class="chapter" data-level="2.7" data-path="r-eda.html"><a href="r-eda.html#scatterplots"><i class="fa fa-check"></i><b>2.7</b> Scatterplots</a></li>
<li class="chapter" data-level="2.8" data-path="r-eda.html"><a href="r-eda.html#exercise-set-2-2"><i class="fa fa-check"></i><b>2.8</b> Exercise set 2-2</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="best-fit-line.html"><a href="best-fit-line.html"><i class="fa fa-check"></i><b>3</b> Line of best fit</a><ul>
<li class="chapter" data-level="3.1" data-path="best-fit-line.html"><a href="best-fit-line.html#exercise-set-3-1"><i class="fa fa-check"></i><b>3.1</b> Exercise set 3-1</a></li>
<li class="chapter" data-level="3.2" data-path="best-fit-line.html"><a href="best-fit-line.html#exercise-set-3-2"><i class="fa fa-check"></i><b>3.2</b> Exercise set 3-2</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>4</b> Probability and random variables</a><ul>
<li class="chapter" data-level="4.0.1" data-path="probability.html"><a href="probability.html#probability-vs-estimation"><i class="fa fa-check"></i><b>4.0.1</b> Probability vs estimation</a></li>
<li class="chapter" data-level="4.0.2" data-path="probability.html"><a href="probability.html#what-is-a-probability"><i class="fa fa-check"></i><b>4.0.2</b> What is a probability?</a></li>
<li class="chapter" data-level="4.0.3" data-path="probability.html"><a href="probability.html#set-notation"><i class="fa fa-check"></i><b>4.0.3</b> Set notation</a></li>
<li class="chapter" data-level="4.1" data-path="probability.html"><a href="probability.html#kolmogorovs-three-axioms-of-probability"><i class="fa fa-check"></i><b>4.1</b> Kolmogorov’s three axioms of probability</a><ul>
<li class="chapter" data-level="4.1.1" data-path="probability.html"><a href="probability.html#exercise-set-4-1"><i class="fa fa-check"></i><b>4.1.1</b> Exercise set 4-1</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="probability.html"><a href="probability.html#conditional-probability-and-independence"><i class="fa fa-check"></i><b>4.2</b> Conditional probability and independence</a><ul>
<li class="chapter" data-level="4.2.1" data-path="probability.html"><a href="probability.html#exercise-set-4-2"><i class="fa fa-check"></i><b>4.2.1</b> Exercise set 4-2</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="probability.html"><a href="probability.html#bayes-theorem"><i class="fa fa-check"></i><b>4.3</b> Bayes’ Theorem</a></li>
<li class="chapter" data-level="4.4" data-path="probability.html"><a href="probability.html#discrete-random-variables-and-distributions"><i class="fa fa-check"></i><b>4.4</b> Discrete random variables and distributions</a><ul>
<li class="chapter" data-level="4.4.1" data-path="probability.html"><a href="probability.html#exercise-set-4-3"><i class="fa fa-check"></i><b>4.4.1</b> Exercise set 4-3</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="probability.html"><a href="probability.html#continuous-random-variables-and-distributions"><i class="fa fa-check"></i><b>4.5</b> Continuous random variables and distributions</a><ul>
<li class="chapter" data-level="4.5.1" data-path="probability.html"><a href="probability.html#exercise-set-4-4"><i class="fa fa-check"></i><b>4.5.1</b> Exercise set 4-4</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="probability.html"><a href="probability.html#probability-density-functions"><i class="fa fa-check"></i><b>4.6</b> Probability density functions</a><ul>
<li class="chapter" data-level="4.6.1" data-path="probability.html"><a href="probability.html#additional-viz"><i class="fa fa-check"></i><b>4.6.1</b> Additional viz</a></li>
<li class="chapter" data-level="4.6.2" data-path="probability.html"><a href="probability.html#exercise-set-4-5"><i class="fa fa-check"></i><b>4.6.2</b> Exercise set 4-5</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="probability.html"><a href="probability.html#families-of-distributions"><i class="fa fa-check"></i><b>4.7</b> Families of distributions</a><ul>
<li class="chapter" data-level="4.7.1" data-path="probability.html"><a href="probability.html#exercise-set-4-6"><i class="fa fa-check"></i><b>4.7.1</b> Exercise set 4-6</a></li>
<li class="chapter" data-level="4.7.2" data-path="probability.html"><a href="probability.html#additional-exercise"><i class="fa fa-check"></i><b>4.7.2</b> Additional exercise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="randomvars.html"><a href="randomvars.html"><i class="fa fa-check"></i><b>5</b> Properties of random variables</a><ul>
<li class="chapter" data-level="5.1" data-path="randomvars.html"><a href="randomvars.html#expected-values-and-the-law-of-large-numbers"><i class="fa fa-check"></i><b>5.1</b> Expected values and the law of large numbers</a><ul>
<li class="chapter" data-level="5.1.1" data-path="randomvars.html"><a href="randomvars.html#weak-law-of-large-numbers"><i class="fa fa-check"></i><b>5.1.1</b> Weak law of large numbers</a></li>
<li class="chapter" data-level="5.1.2" data-path="randomvars.html"><a href="randomvars.html#handy-facts-about-expectations"><i class="fa fa-check"></i><b>5.1.2</b> Handy facts about expectations</a></li>
<li class="chapter" data-level="5.1.3" data-path="randomvars.html"><a href="randomvars.html#exercise-set-5-1"><i class="fa fa-check"></i><b>5.1.3</b> Exercise set 5-1</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="randomvars.html"><a href="randomvars.html#variance-and-standard-deviation"><i class="fa fa-check"></i><b>5.2</b> Variance and standard deviation</a><ul>
<li class="chapter" data-level="5.2.1" data-path="randomvars.html"><a href="randomvars.html#beautiful-properties-of-the-variance"><i class="fa fa-check"></i><b>5.2.1</b> Beautiful properties of the variance</a></li>
<li class="chapter" data-level="5.2.2" data-path="randomvars.html"><a href="randomvars.html#exercise-set-5-2"><i class="fa fa-check"></i><b>5.2.2</b> Exercise set 5-2</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="randomvars.html"><a href="randomvars.html#joint-distributions-covariance-and-correlation"><i class="fa fa-check"></i><b>5.3</b> Joint distributions, covariance, and correlation</a><ul>
<li class="chapter" data-level="5.3.1" data-path="randomvars.html"><a href="randomvars.html#joint-probability-distributions"><i class="fa fa-check"></i><b>5.3.1</b> Joint probability distributions</a></li>
<li class="chapter" data-level="5.3.2" data-path="randomvars.html"><a href="randomvars.html#marginal-distributions"><i class="fa fa-check"></i><b>5.3.2</b> Marginal distributions</a></li>
<li class="chapter" data-level="5.3.3" data-path="randomvars.html"><a href="randomvars.html#covariance"><i class="fa fa-check"></i><b>5.3.3</b> Covariance</a></li>
<li class="chapter" data-level="5.3.4" data-path="randomvars.html"><a href="randomvars.html#correlation"><i class="fa fa-check"></i><b>5.3.4</b> Correlation</a></li>
<li class="chapter" data-level="5.3.5" data-path="randomvars.html"><a href="randomvars.html#additional-exercise-1"><i class="fa fa-check"></i><b>5.3.5</b> Additional exercise</a></li>
<li class="chapter" data-level="5.3.6" data-path="randomvars.html"><a href="randomvars.html#exercise-set-5-3"><i class="fa fa-check"></i><b>5.3.6</b> Exercise set 5-3</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="randomvars.html"><a href="randomvars.html#conditional-distribution-expectation-variance"><i class="fa fa-check"></i><b>5.4</b> Conditional distribution, expectation, variance</a></li>
<li class="chapter" data-level="5.5" data-path="randomvars.html"><a href="randomvars.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>5.5</b> The central limit theorem</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes on Statistical Thinking from Scratch</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="randomvars" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Properties of random variables</h1>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb81-1" title="1"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb81-2" title="2"><span class="kw">theme_set</span>(<span class="kw">theme_bw</span>(<span class="dt">base_size =</span> <span class="dv">12</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb81-3" title="3"><span class="st">            </span><span class="kw">theme</span>(<span class="dt">strip.background =</span> <span class="kw">element_blank</span>(), </a>
<a class="sourceLine" id="cb81-4" title="4">                  <span class="dt">panel.grid =</span> <span class="kw">element_blank</span>())) </a></code></pre></div>
<div id="expected-values-and-the-law-of-large-numbers" class="section level2">
<h2><span class="header-section-number">5.1</span> Expected values and the law of large numbers</h2>
<p>When summarizing a probability distribution, it is useful to have a measure of:</p>
<ul>
<li>Location (<em>Expectation</em>; E(<span class="math inline">\(X\)</span>))</li>
<li>Dispersal (<em>Variance</em>; Var(<span class="math inline">\(X\)</span>))</li>
</ul>
<p>In this section, we’re focusing on the expectation.</p>
<p>The expectation of a discrete random variable is the average:</p>
<p><span class="math display">\[
\begin{aligned}
\text{E}(X) =&amp; \sum_{i = 1}^{k}x_i P(X = x_i) \\
=&amp; \sum_{i = 1}^{k}x_i f_X(x_i) \\
\end{aligned}
\]</span></p>
<p>If <span class="math inline">\(Y\)</span> represents a six-sided die, then:</p>
<p><span class="math display">\[
\begin{aligned}
\text{E}(Y) =&amp; \sum_{i = 1}^{k}y_i f_Y(y_i) \\
=&amp; 1(1/6) + 2(1/6) + 3(1/6) + 4(1/6) + 5(1/6) + 6(1/6) \\
=&amp; 21/6 \\ 
=&amp; 7/2 \\
\end{aligned}
\]</span></p>
<p>If <span class="math inline">\(X\)</span> is continuous:</p>
<p><span class="math display">\[
\begin{aligned}
\text{E}(X) =&amp; \int_{- \infty}^{\infty} x f_X(x) dx \\
\end{aligned}
\]</span></p>
<p>Here we are integrating over the probability density function, rather than summing over the mass density function.</p>
<div id="weak-law-of-large-numbers" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Weak law of large numbers</h3>
<p>The expectation is more like a long-term average, rather than an actual instance (7/2 is not a possible instance of a dice roll).</p>
<p><span class="math inline">\(X_i\)</span> are i.i.d.</p>
<p>Assume E(<span class="math inline">\(X_1\)</span>) = E(<span class="math inline">\(X_2\)</span>) = … = E(<span class="math inline">\(X_n\)</span>) = <span class="math inline">\(\mu\)</span></p>
<p>Define <span class="math inline">\(\overline{X}_n\)</span> as the mean of the observations:</p>
<p><span class="math display">\[
\begin{aligned}
\overline{X}_n = \frac{1}{n} (X_1 + X_2 + X_3 + ... + X_n)
\end{aligned}
\]</span></p>
<p>As <span class="math inline">\(n \rightarrow \infty\)</span>, <span class="math inline">\(\overline{X}_n\)</span> “converges in probability” to <span class="math inline">\(\mu\)</span>. This means for any positive constant <span class="math inline">\(\delta\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
lim_{n \rightarrow \infty} \text{P}(|\overline{X}_n - \mu| &gt; \delta) = 0
\end{aligned}
\]</span></p>
</div>
<div id="handy-facts-about-expectations" class="section level3">
<h3><span class="header-section-number">5.1.2</span> Handy facts about expectations</h3>
<p>The expectation of a constant times a random variable is the constant times the expectation of the random variable:</p>
<p><span class="math display">\[ 
\begin{aligned}
\text{E}(aX) = a \text{E}(X)
\end{aligned}
\]</span>
The expectation of a constant is the constant:</p>
<p><span class="math display">\[ 
\begin{aligned}
\text{E}(c) = c
\end{aligned}
\]</span></p>
<p>The expectation of a sum of random variables is the sum of the expectations of those random variables:</p>
<p><span class="math display">\[ 
\begin{aligned}
\text{E}(X + Y) = \text{E}(X) + \text{E}(Y)
\end{aligned}
\]</span></p>
<p>Putting all these facts together, we can calculate the expectation of two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> as:</p>
<p><span class="math display">\[ 
\begin{aligned}
\text{E}(aX + bY + c) = a \text{E}(X) + b \text{E}(Y) + c
\end{aligned}
\]</span></p>
<p>This is called the <strong>linearity of expectation</strong>, which we will use frequently in the exercises. Linearity does not hold for other measures of location (e.g., median, mode). This fact accounts, in part, for the privileged status of the mean in statistics.</p>
<p>To calculate the expectation of a function:</p>
<p><span class="math display">\[ 
\begin{aligned}
\text{E}[g(X)] = \sum_{i = 1}^{k} g(x_i)f_X(x_i)
\end{aligned}
\]</span></p>
<p><span class="math display">\[ 
\begin{aligned}
\text{E}[g(X)] = \int_{-\infty}^{\infty} g(x)f_X(x)dx
\end{aligned}
\]</span></p>
</div>
<div id="exercise-set-5-1" class="section level3">
<h3><span class="header-section-number">5.1.3</span> Exercise set 5-1</h3>
<p>1a. Expected value of a Bernoulli random variable with parameter <em>p</em>?</p>
<p><span class="math display">\[ 
\begin{aligned}
f_X(x) = \text{P}(X = x) = p^x(1 - p)^{1-x} \text{ for } x \in \text{{0, 1}} \\
\end{aligned}
\]</span></p>
<p>Because there are only two outcomes (0 or 1), we can compute the expectation directly:</p>
<p><span class="math display">\[ 
\begin{aligned}
\text{E}(X) =&amp; \sum_0^1 x f_X(x) \\
=&amp; \sum_0^1 x p^x(1 - p)^{1-x} \\ 
=&amp; 0 p^0(1 - p)^{1-0} + 1 p^1(1 - p)^{1-1} \\
=&amp; 0 p^0(1 - p)^{1} + 1 p^1(1 - p)^{0} \\
=&amp; 0 (1) (1-p) + p(1) \\ 
=&amp; 0 + p \\
=&amp; p
\end{aligned}
\]</span></p>
<p>1b. What is the expected value of a binomial random variable with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>?</p>
<p>Here’s the pmf for the binomial distribution:</p>
<p><span class="math display">\[ 
\begin{aligned}
f_X(x) = \text{P}(X = x) = \binom{n}{x} p^x(1 - p)^{n - x} \text{ for } x \in \text{{0, 1, 2, ..., n}} \\
\end{aligned}
\]</span></p>
<p>If we plug that into the equation for E(<span class="math inline">\(X\)</span>), we get:</p>
<p><span class="math display">\[ 
\begin{aligned}
\text{E}(X) =&amp; \sum_0^n x f_X(x) \\
=&amp; \sum_0^n x \binom{n}{x} p^x(1 - p)^{n - x} \\ 
\end{aligned}
\]</span>
Well, I don’t know how to evaluate this sum directly, considering the upper limit of <span class="math inline">\(n\)</span> is infinite. So we’ll use the fact that the binomial is the sum of <span class="math inline">\(n\)</span> independent Bernoulli trials (<span class="math inline">\(X_i\)</span>).</p>
<p><span class="math display">\[ 
\begin{aligned}
\text{E}(X) =&amp; \text{E}(\sum_{i=1}^nX_i)
\end{aligned}
\]</span></p>
<p>Because the expectation is linear, the expectation of the sum is the sum of the expectations; we can rearrange:</p>
<p><span class="math display">\[ 
\begin{aligned}
\text{E}(X) =&amp; \sum_{i=1}^n \text{E}(X_i)
\end{aligned}
\]</span></p>
<p>From 1a, we can substitute <span class="math inline">\(p\)</span> for <span class="math inline">\(\text{E}(X_i)\)</span>:</p>
<p><span class="math display">\[ 
\begin{aligned}
\text{E}(X) =&amp; \sum_{i=1}^n p \\
=&amp; np
\end{aligned}
\]</span></p>
<p>1c. What is the expected value of a discrete uniform random variable with parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>?</p>
<p>The probability mass function is:</p>
<p><span class="math display">\[ 
\begin{aligned}
\text{P}(X = k) =&amp; \frac{1}{b - a + 1} \\
\end{aligned}
\]</span>
The expectation is:</p>
<p><span class="math display">\[ 
\begin{aligned}
\text{E}(X) =&amp; \sum_{x = a}^b x f_X(x) \\
=&amp; \sum_{x = a}^b x \frac{1}{b - a + 1}  \\
=&amp; \frac{1}{b - a + 1} \sum_{x = a}^b x \\
\end{aligned}
\]</span></p>
<p>We were given a hint that is useful now: for integers <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> with <span class="math inline">\(b &gt; a\)</span>, the sum of all the integers including <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, is:</p>
<p><span class="math display">\[ 
\begin{aligned}
\sum_{k = a}^b k  =&amp; \frac{(a + b)(b - a + 1)}{2} \\
\end{aligned}
\]</span></p>
<p>So, plugging that hint in we get:</p>
<p><span class="math display">\[ 
\begin{aligned}
=&amp; \frac{1}{b - a + 1} \times \frac{(a + b)(b - a + 1)}{2} \\
=&amp; \frac{a + b}{2} \\
\end{aligned}
\]</span></p>
<p>1d. What is the expected value of a continuous uniform random variable with parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>?</p>
<p>The probability density function is:</p>
<p><span class="math display">\[ 
\begin{aligned}
\text{P}(X) =&amp; \frac{1}{b - a} \\
\end{aligned}
\]</span></p>
<p>The expectation is:</p>
<p><span class="math display">\[ 
\begin{aligned}
\text{E}(X) =&amp; \int_{a}^b x f_X(x) dx \\
=&amp; \int_{a}^b x \frac{1}{b - a} dx  \\
=&amp; \frac{1}{b - a} \int_{a}^b x  dx  \\
\end{aligned}
\]</span></p>
<p>Now we have to integrate the 2nd term:</p>
<p><span class="math display">\[ 
\begin{aligned}
=&amp; \frac{1}{b - a} \times \frac{1}{2} x^2 \bigg\rvert_{a}^{b} \\
=&amp; \frac{1}{b - a} \times (\frac{b^2}{2} - \frac{a^2}{2}) \\
=&amp; \frac{1}{b - a} \times (\frac{b^2 - a^2}{2}) \\
\end{aligned}
\]</span></p>
<p>We use the hint from earlier, that <span class="math inline">\(b^2 - a^2 = (b-a)(b+a)\)</span>:</p>
<p><span class="math display">\[ 
\begin{aligned}
=&amp; \frac{1}{b - a} \times (\frac{(b-a)(b+a)}{2}) \\
=&amp; \frac{a + b}{2} \\
\end{aligned}
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Exploring the law of large numbers by simulation. In Edge’s code block below, <code>samp.size</code> represents <span class="math inline">\(n\)</span> in the weak law of large numbers (above); <code>n.samps</code> represents independent random variables <span class="math inline">\(X_n\)</span>. The expectation for all <span class="math inline">\(X_i\)</span> is <span class="math inline">\(\mu\)</span>.</li>
</ol>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb82-1" title="1">samp.size &lt;-<span class="st"> </span><span class="dv">20</span></a>
<a class="sourceLine" id="cb82-2" title="2">n.samps &lt;-<span class="st"> </span><span class="dv">1000</span></a>
<a class="sourceLine" id="cb82-3" title="3">samps &lt;-<span class="st"> </span><span class="kw">rnorm</span>(samp.size <span class="op">*</span><span class="st"> </span>n.samps, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb82-4" title="4"><span class="co"># Each column represents a random variable, X_i</span></a>
<a class="sourceLine" id="cb82-5" title="5"><span class="co"># Each row represents a sample (instance) drawn from X_i</span></a>
<a class="sourceLine" id="cb82-6" title="6">samp.mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(samps, <span class="dt">ncol =</span> n.samps) </a>
<a class="sourceLine" id="cb82-7" title="7"><span class="kw">str</span>(samp.mat)</a></code></pre></div>
<pre><code>##  num [1:20, 1:1000] -0.838 0.625 0.846 -0.187 0.24 ...</code></pre>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb84-1" title="1"><span class="co"># Here we calculate the sample mean for each X_i (column)</span></a>
<a class="sourceLine" id="cb84-2" title="2">samp.means &lt;-<span class="st"> </span><span class="kw">colMeans</span>(samp.mat)</a>
<a class="sourceLine" id="cb84-3" title="3"><span class="kw">str</span>(samp.means)</a></code></pre></div>
<pre><code>##  num [1:1000] -0.4343 -0.1757 -0.2925 0.2837 0.0551 ...</code></pre>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb86-1" title="1"><span class="kw">hist</span>(samp.means)</a></code></pre></div>
<p><img src="05_random_variables_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>2a. What happens if we change <code>samp.size</code> (i.e., <span class="math inline">\(n\)</span>)?</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb87-1" title="1">n_vector &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">20</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb87-2" title="2">samp_means_mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="ot">NA</span>, <span class="dt">nrow =</span> n.samps, <span class="dt">ncol =</span> <span class="kw">length</span>(n_vector))</a>
<a class="sourceLine" id="cb87-3" title="3"></a>
<a class="sourceLine" id="cb87-4" title="4">calculate_sample_means &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">samp.size =</span> <span class="dv">20</span>, <span class="dt">n.samps =</span> <span class="dv">1000</span>){</a>
<a class="sourceLine" id="cb87-5" title="5">  samps &lt;-<span class="st"> </span><span class="kw">rnorm</span>(samp.size <span class="op">*</span><span class="st"> </span>n.samps, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb87-6" title="6">  samp.mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(samps, <span class="dt">ncol =</span> n.samps) </a>
<a class="sourceLine" id="cb87-7" title="7">  samp.means &lt;-<span class="st"> </span><span class="kw">colMeans</span>(samp.mat)</a>
<a class="sourceLine" id="cb87-8" title="8">  <span class="kw">return</span>(samp.means)</a>
<a class="sourceLine" id="cb87-9" title="9">}</a>
<a class="sourceLine" id="cb87-10" title="10"></a>
<a class="sourceLine" id="cb87-11" title="11"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>))</a>
<a class="sourceLine" id="cb87-12" title="12"><span class="kw">set.seed</span>(<span class="dv">21</span>)</a>
<a class="sourceLine" id="cb87-13" title="13"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(n_vector)){</a>
<a class="sourceLine" id="cb87-14" title="14">  samp_size_i &lt;-<span class="st"> </span>n_vector[i]</a>
<a class="sourceLine" id="cb87-15" title="15">  samp_means_i &lt;-<span class="st"> </span><span class="kw">calculate_sample_means</span>(<span class="dt">samp.size =</span> samp_size_i)</a>
<a class="sourceLine" id="cb87-16" title="16">  <span class="kw">hist</span>(samp_means_i, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">250</span>), </a>
<a class="sourceLine" id="cb87-17" title="17">       <span class="dt">xlab =</span> <span class="st">&quot;Sample mean&quot;</span>, </a>
<a class="sourceLine" id="cb87-18" title="18">       <span class="dt">main =</span> <span class="kw">paste</span>(<span class="st">&quot;n = &quot;</span>, samp_size_i, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</a>
<a class="sourceLine" id="cb87-19" title="19">}</a></code></pre></div>
<p><img src="05_random_variables_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>2b. Using the exponential distribution.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" title="1">n_vector &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">20</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb88-2" title="2">samp_means_mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="ot">NA</span>, <span class="dt">nrow =</span> n.samps, <span class="dt">ncol =</span> <span class="kw">length</span>(n_vector))</a>
<a class="sourceLine" id="cb88-3" title="3"></a>
<a class="sourceLine" id="cb88-4" title="4">calculate_sample_means_exp &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">samp.size =</span> <span class="dv">20</span>, <span class="dt">n.samps =</span> <span class="dv">1000</span>){</a>
<a class="sourceLine" id="cb88-5" title="5">  samps &lt;-<span class="st"> </span><span class="kw">rexp</span>(samp.size <span class="op">*</span><span class="st"> </span>n.samps, <span class="dt">rate =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb88-6" title="6">  samp.mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(samps, <span class="dt">ncol =</span> n.samps) </a>
<a class="sourceLine" id="cb88-7" title="7">  samp.means &lt;-<span class="st"> </span><span class="kw">colMeans</span>(samp.mat)</a>
<a class="sourceLine" id="cb88-8" title="8">  <span class="kw">return</span>(samp.means)</a>
<a class="sourceLine" id="cb88-9" title="9">}</a>
<a class="sourceLine" id="cb88-10" title="10"></a>
<a class="sourceLine" id="cb88-11" title="11"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>))</a>
<a class="sourceLine" id="cb88-12" title="12"><span class="kw">set.seed</span>(<span class="dv">21</span>)</a>
<a class="sourceLine" id="cb88-13" title="13"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(n_vector)){</a>
<a class="sourceLine" id="cb88-14" title="14">  samp_size_i &lt;-<span class="st"> </span>n_vector[i]</a>
<a class="sourceLine" id="cb88-15" title="15">  samp_means_i &lt;-<span class="st"> </span><span class="kw">calculate_sample_means_exp</span>(<span class="dt">samp.size =</span> samp_size_i)</a>
<a class="sourceLine" id="cb88-16" title="16">  <span class="kw">hist</span>(samp_means_i, </a>
<a class="sourceLine" id="cb88-17" title="17">       <span class="dt">xlab =</span> <span class="st">&quot;Sample mean&quot;</span>, </a>
<a class="sourceLine" id="cb88-18" title="18">       <span class="dt">main =</span> <span class="kw">paste</span>(<span class="st">&quot;n = &quot;</span>, samp_size_i, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</a>
<a class="sourceLine" id="cb88-19" title="19">}</a></code></pre></div>
<p><img src="05_random_variables_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
</div>
<div id="variance-and-standard-deviation" class="section level2">
<h2><span class="header-section-number">5.2</span> Variance and standard deviation</h2>
<p>The variance is a measurement of dispersal - i.e., how spread out is the distribution? And spread out from what, exactly? It is useful to think about the distance <span class="math inline">\(X_i\)</span> takes from the expectation, E<span class="math inline">\((X)\)</span>: <span class="math inline">\(X - \text{E}(X)\)</span>. What if we took the expectation of this - i.e., the average value of the distance from the mean?</p>
<p><span class="math display">\[ 
\begin{aligned}
\text{E}(X - \text{E}(X)) \\
\text{by linearity of expectation, we get:} \\
\text{E}(X) - \text{E}(\text{E}(X)) \\
\text{E}(X) - \text{E}(X) \\
0
\end{aligned}
\]</span></p>
<p>This won’t work - we need to find a way to constrain the expression inside the parentheses to be non-negative. One way to do this is to use the mean absolute deviation, <span class="math inline">\(|X - \text{E}(X)|\)</span>. Another way is to use the mean squared deviation, <span class="math inline">\([X - \text{E}(X)]^2\)</span>. The squared term constrains the variance to be <span class="math inline">\(\ge 0\)</span>:</p>
<p><span class="math display">\[ 
\begin{aligned}
\text{Var}(X) =&amp; \text{E}([X - \text{E}(X)]^2) \\
\end{aligned}
\]</span></p>
<p>The mean squared deviation has two mathematical advantages:</p>
<ol style="list-style-type: decimal">
<li><p>It is easier to compute mathematically than an analogous quanitity using absolute deviations (but why?)</p></li>
<li><p>The variances of linear functions of random variables are ‘beautifully behaved’, whereas the analogous quantities for absolute deviations can be a hassle.</p></li>
</ol>
<p>I will just take Edge’s word on these two points for now.</p>
<div id="beautiful-properties-of-the-variance" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Beautiful properties of the variance</h3>
<p>The variance can be rewritten as:</p>
<p><span class="math display">\[ 
\begin{aligned}
\text{Var}(X) = \text{E}(X^2) - [\text{E}(X)]^2 \\
\end{aligned}
\]</span></p>
<p>which is generally easier to compute.</p>
<p>Adding a constant to a random variable does <em>not</em> affect the variance:</p>
<p><span class="math display">\[ 
\begin{aligned}
\text{Var}(a + cX) = c^2\text{Var}(X) \\
\end{aligned}
\]</span>
where <span class="math inline">\(a\)</span> and <span class="math inline">\(c\)</span> are constants.</p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables, then:</p>
<p><span class="math display">\[ 
\begin{aligned}
\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) \\
\end{aligned}
\]</span></p>
<p>One big problem with the variance is that is in the wrong (<span class="math inline">\(X^2\)</span>) units. To fix this, we calculate the <em>standard deviation</em>:</p>
<p><span class="math display">\[ 
\begin{aligned}
\text{SD}(X) = \sqrt{\text{Var}(X)} \\
\end{aligned}
\]</span></p>
<p>SD is usually larger (never smaller) than MAD, and is more sensitive to large deviations.</p>
</div>
<div id="exercise-set-5-2" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Exercise set 5-2</h3>
<p>I had to walk through Edge’s solutions bit by bit; my handwritten version is <a href="images/edge_4_1_3.pdf">here</a>.</p>
</div>
</div>
<div id="joint-distributions-covariance-and-correlation" class="section level2">
<h2><span class="header-section-number">5.3</span> Joint distributions, covariance, and correlation</h2>
<p>This section covers four key concepts:</p>
<ol style="list-style-type: decimal">
<li><p>Joint probability distribution: the probability distribution of the joint occurrence of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span></p></li>
<li><p>Marginal distribution of X: the probability distribution of <span class="math inline">\(X\)</span>, summing (integrating) over all values of <span class="math inline">\(Y\)</span></p></li>
<li><p>Covariance: a measurement of the extent to which <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> depart from independence</p></li>
<li><p>Correlation: covariance rescaled to go from -1 to 1</p></li>
</ol>
<div id="joint-probability-distributions" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Joint probability distributions</h3>
<p>Joint probability distribution: the probability distribution of the joint occurrence of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span></p>
<p>The joint cumulative probability distribution of two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is given by:</p>
<p><span class="math display">\[
\begin{aligned}
F_{X,Y}(x, y) = \text{P} (X \leq x ~\cap ~ Y \leq y)
\end{aligned}
\]</span></p>
<p>Here is the corresponding joint probability mass function:</p>
<p><span class="math display">\[
\begin{aligned}
f_{X,Y}(x, y) = \text{P} (X = x ~\cap ~ Y = y)
\end{aligned}
\]</span></p>
<p>And for two continuous variables, we can recover the cumulative distribution function by integrating the probability density function with respect to <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
F_{X,Y}(x, y) =&amp; ~ \text{P}(X \leq x ~\cap ~ Y \leq y) \\
=&amp; \int_{- \infty}^{x} \int_{- \infty}^{y} f_{X,Y}(x, y) dx dy
\end{aligned}
\]</span></p>
</div>
<div id="marginal-distributions" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Marginal distributions</h3>
<p>Marginal distribution of X: the probability distribution of <span class="math inline">\(X\)</span>, summing (integrating) over all values of <span class="math inline">\(Y\)</span></p>
<p>For discrete random variables, the marginal distribution of <span class="math inline">\(X\)</span> is:</p>
<p><span class="math display">\[
\begin{aligned}
f_{X}(x) =&amp; ~ \text{P} (X = x) \\
         =&amp; ~ \sum_{y} \text{P} (X = x ~\cap ~ Y = y) \\
         =&amp; \sum_{y} f_{X,Y}(x,y)
\end{aligned}
\]</span></p>
<p>For continuous random variables, the marginal distribution of <span class="math inline">\(X\)</span> is:</p>
<p><span class="math display">\[
\begin{aligned}
f_{X}(x) =&amp; \int_{- \infty}^{\infty} f_{X,Y}(x, y) dy
\end{aligned}
\]</span></p>
</div>
<div id="covariance" class="section level3">
<h3><span class="header-section-number">5.3.3</span> Covariance</h3>
<p>Covariance is a measurement of the extent to which <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> depart from independence</p>
<p>Such a measure should have two basic properties:</p>
<ul>
<li>The number should be positive when <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> increase or decrease together</li>
<li>The number should be negative when <span class="math inline">\(X\)</span> increases and <span class="math inline">\(Y\)</span> decreases (and vice versa).</li>
</ul>
<p>Consider the random variable <span class="math inline">\([X - \text{E}(X)][Y - \text{E}(Y)]\)</span>. If we sample a joint probability distribution, resulting in a set (<span class="math inline">\(\Omega\)</span>) of pairs of <span class="math inline">\((x, y)\)</span>, ask yourself:</p>
<ul>
<li>Is the sign positive or negative when most of the pairs <span class="math inline">\((x, y)\)</span> are such that <span class="math inline">\(x &gt; \text{E}(X)\)</span> and <span class="math inline">\(y &gt; \text{E}(Y)\)</span>?</li>
<li>Is the sign positive or negative when most of the pairs <span class="math inline">\((x, y)\)</span> are such that <span class="math inline">\(x &lt; \text{E}(X)\)</span> and <span class="math inline">\(y &lt; \text{E}(Y)\)</span>?</li>
<li>Is the sign positive or negative when most of the pairs <span class="math inline">\((x, y)\)</span> are such that <span class="math inline">\(x &gt; \text{E}(X)\)</span> and <span class="math inline">\(y &lt; \text{E}(Y)\)</span>?</li>
<li>Is the sign positive or negative when most of the pairs <span class="math inline">\((x, y)\)</span> are such that <span class="math inline">\(x &lt; \text{E}(X)\)</span> and <span class="math inline">\(y &gt; \text{E}(Y)\)</span>?</li>
</ul>
<p>Hopefully, you have convinced yourself that the random variable <span class="math inline">\([X - \text{E}(X)][Y - \text{E}(Y)]\)</span> satisfies the two aforementioned properties. Now we take the expectation of this random variable to arrive at the covariance.</p>
<p>Conveniently (or purposefully?), the covariance is an extension of the variance:</p>
<p><span class="math display">\[
\begin{aligned}
\text{Cov}(X,Y) =&amp; ~ \text{E}([X - \text{E}(X)][Y - \text{E}(Y)]) \\
                =&amp; ~ \text{E}(XY) - \text{E}(X)\text{E}(Y)
\end{aligned}
\]</span></p>
<p>If you replace <span class="math inline">\(\text{E}(Y)\)</span> with <span class="math inline">\(\text{E}(X)\)</span> in the above equation, you should recover the definition of Var<span class="math inline">\((X)\)</span>, <span class="math inline">\(\text{E}(X^2) - [\text{E}(X)]^2\)</span>.</p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\text{Cov}(X,Y) = 0\)</span> (we showed this in an earlier problem set).</p>
<p>However, if <span class="math inline">\(\text{Cov}(X,Y) = 0\)</span>, that does not necessarily imply that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</p>
</div>
<div id="correlation" class="section level3">
<h3><span class="header-section-number">5.3.4</span> Correlation</h3>
<p>Correlation: covariance rescaled to go from -1 to 1</p>
<p>The covariance is not a pure measure of the linear dependence between two variables, because it is sensitive to the scaling of the variables. Therefore, we cannot use the covariance to compare the strengths of different bivariate relationships. In other words, we cannot use the covariance to answer the question: <em>Is the relationship between cereal yield and fertilizer consumption stronger than the relationship between career earnings and college GPA?</em>. Instead, we calculate the correlation:</p>
<p><span class="math display">\[
\begin{aligned}
\text{Cor}(X,Y) =&amp; ~ \rho_{X,Y} \\
                =&amp; ~ \frac{\text{Cov}(X,Y)}{\sqrt{\text{Var}(X)\text{Var}(Y)}} \\
                =&amp; ~ \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}\\
\end{aligned}
\]</span></p>
<p>You can prove to yourself that the correlation is bounded from -1 to 1 using a simple heuristic. Which variable should be the most correlated with <span class="math inline">\(X\)</span>? Well, that would be <span class="math inline">\(X\)</span>. Plugging in <span class="math inline">\(X\)</span> for <span class="math inline">\(Y\)</span>, we get:</p>
<p><span class="math display">\[
\begin{aligned}
\text{Cor}(X,X) =&amp; ~ \frac{\text{Cov}(X,X)}{\sigma_X \sigma_X} \\
                 =&amp; ~ \frac{\text{Var}(X)}{\text{Var}(X)} \\
                 =&amp; ~ 1
\end{aligned}
\]</span></p>
<p>Using the same logic, <span class="math inline">\(-X\)</span>, is the least correlated with <span class="math inline">\(X\)</span>. Try working through the algebra, you should get -1:</p>
<p><span class="math display">\[
\begin{aligned}
\text{Cor}(X,-X) =&amp; ~ \frac{\text{Cov}(X,-X)}{\sqrt{\text{Var}(X)\text{Var}(-X)}} \\
                =&amp; ~ \frac{\text{E}(X \times -X) - \text{E}(X)\text{E}(-X)}{\sqrt{\text{Var}(X)\text{Var}(-X)}} \\
\end{aligned}
\]</span></p>
</div>
<div id="additional-exercise-1" class="section level3">
<h3><span class="header-section-number">5.3.5</span> Additional exercise</h3>
<p>I will add one problem, to reinforce the concepts of joint and marginal distributions, with two discrete random variables. This problem covers similar ideas to Edge’s first exercise in set 5-3.</p>
<p>You watched 100 female birds last spring, and recorded the number of offspring per bird (X; 1, 2, or 3 chicks). You also recorded the age of each mom (Y; 1, 2, or 3 years).</p>
<p>You observed:
10 1-yr olds, all with one chick.
27 2-yr olds; 13 had one chick, 12 had two chicks, and 2 had three chicks.
63 3-yr olds; 23 had one chick, 36 had two chicks, and 4 had three chicks.</p>
<p>Calculate:</p>
<ol style="list-style-type: decimal">
<li><p>The probability of observing each possible outcome (e.g., a 1-yr old bird has 1 chick; a 1-yr old bird has 2 chicks; etc.).</p></li>
<li><p>The probability of observing a 1-yr old bird; a 2-yr old bird; and a 3-yr old bird.</p></li>
<li><p>The probability of observing 1 chick per mom; 2 chicks per mom; 3 chicks per mom.</p></li>
</ol>
<p>STOP! NO PEEKING ! ANSWER IS BELOW:</p>
<p>Wait for it…</p>
<p>…wait for it …</p>
<p>…here it is: an excel (gasp!) plot!</p>
<p><img src="images/birbs_for_rmd.png" style="width:200.0%" /></p>
<p>The key here is to recognize that yellow represents the joint probabilities of X and Y; the green and blue represents the marginal probabilities of X and Y, respectively. Stare at this until it clicks. A similar principle applies to continuous distributions, but rather than summing across Y, we integrate across Y to get the marginal distribution of X.</p>
</div>
<div id="exercise-set-5-3" class="section level3">
<h3><span class="header-section-number">5.3.6</span> Exercise set 5-3</h3>
<p>I had to walk through Edge’s solutions bit by bit; my handwritten version is <a href="images/edge_5_3.pdf">here</a>.</p>
</div>
</div>
<div id="conditional-distribution-expectation-variance" class="section level2">
<h2><span class="header-section-number">5.4</span> Conditional distribution, expectation, variance</h2>
<p>For two discrete random variables, the conditional probability mass function is:</p>
<p><span class="math display">\[
\begin{aligned}
f_{X|Y}(x |Y = y) =&amp; \text{P}(X = x | Y = y) \\
                  =&amp; \frac{\text{P} (X = x ~\cap ~ Y = y)}{\text{P}(Y = y)} \\
                  =&amp; \frac{f_{X,Y}(x,y)}{f_{Y}(y)}
\end{aligned}
\]</span></p>
<p>For two continuous random variables, the conditional probability density function is defined similarly:</p>
<p><span class="math display">\[
\begin{aligned}
f_{X|Y}(x |Y = y) =&amp; \frac{f_{X,Y}(x,y)}{f_{Y}(y)}
\end{aligned}
\]</span></p>
<p>Edge has a nice visualization and explanation of conditional distribution in his Fig 5-5.</p>
</div>
<div id="the-central-limit-theorem" class="section level2">
<h2><span class="header-section-number">5.5</span> The central limit theorem</h2>
<p>Natural populations are large, so we usually gather just a sample and use that as a surrogate for the whole population. If we take <span class="math inline">\(n\)</span> samples, then another <span class="math inline">\(n\)</span> samples, and then another <span class="math inline">\(n\)</span> samples, and calculate <span class="math inline">\(\overline{X}_1\)</span>, <span class="math inline">\(\overline{X}_2\)</span>, and <span class="math inline">\(\overline{X}_3\)</span>, differences in our estimate of <span class="math inline">\(\overline{X}\)</span> are due to sampling variation. The weak law of large numbers (above) tells us that as <span class="math inline">\(n\)</span> approaches <span class="math inline">\(\infty\)</span>, our estimate of <span class="math inline">\(\overline{X}_n\)</span> approaches the true population mean, <span class="math inline">\(\mu\)</span>, and that the Var(<span class="math inline">\(\overline{X}_n\)</span>) = <span class="math inline">\(\sigma^2 / n\)</span>, approaches 0.</p>
<p>But what is the <em>shape</em> of this distribution? That is where the central limit theorem (CLT) comes in. As <span class="math inline">\(n\)</span> approaches <span class="math inline">\(\infty\)</span>, the distribution of <span class="math inline">\(\overline{X}_n\)</span> converges to a normal distribution with expectation <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2 / n\)</span>.</p>
<p>An importance consequences of the CLT is the surpising result that the distribution of sample means <span class="math inline">\(\overline{X}_n\)</span> is <em>approximately</em> normal even when the distribution of the individual observations are not normally distributed! The implications of the CLT are huge: it allows us to use the normal distribution (and the powerful set of analytical tools that depend on it) in real-world situations where the underlying data are not normally distributed, as long as we have enough samples. What is enough? A general rule of thumb is 30, but will vary with the underlying probability distribution of the population. You will explore this using simulations in the problem set below.</p>
<!-- ### Exercise set 5-4 -->
<!-- 1. Bean machine in action! -->
<!-- ```{r, eval = FALSE} -->
<!-- library(animation) -->
<!-- nball <- 500 #change the number of balls -->
<!-- nlayer <- 10 #change the number of rows of pegs on the board -->
<!-- rate <- 10 #change the speed at which the balls fall  -->
<!-- ani.options(nmax = nball + nlayer - 2, interval = 1/rate)  -->
<!-- quincunx(balls = nball, layers = nlayer) -->
<!-- ``` -->
<!-- 2. Exploring the beta distribution -->
<!-- To see what the beta distribution looks like for a given set of shape parameters, set the sample size to 1. For example:  -->
<!-- ```{r} -->
<!-- library(stfspack) -->
<!-- # dosm.beta.hist(n = 1, nsim = 10000, shape1 = 1, shape2 = 1) -->
<!-- ``` -->
<!-- will give you a histogram of 10,000 observations from a beta distribution with parameters 1 and 1. If you increase the sample size, then the distribution of the sample mean gets closer to normality. Try this — starting with samples of size 1 and increasing the sample size — with the following sets of parameter values: (1, 1), (0.2, 0.2), (2, 0.5), (0.5, 2), (3, 3). Feel free to try other parameter sets — it’s fun. What do you notice? -->
<!-- ```{r, fig.height = 7, fig.width = 7} -->
<!-- sims <- 1000 -->
<!-- s1 <- 0.2 # change this -->
<!-- s2 <- 0.2 # change this -->
<!-- par(mfrow = c(2,3)) -->
<!-- dosm.beta.hist(n = 1, nsim = sims, shape1 = s1, shape2 = s2) -->
<!-- dosm.beta.hist(n = 4, nsim = sims, shape1 = s1, shape2 = s2) -->
<!-- dosm.beta.hist(n = 8, nsim = sims, shape1 = s1, shape2 = s2) -->
<!-- dosm.beta.hist(n = 16, nsim = sims, shape1 = s1, shape2 = s2) -->
<!-- dosm.beta.hist(n = 32, nsim = sims, shape1 = s1, shape2 = s2) -->
<!-- dosm.beta.hist(n = 64, nsim = sims, shape1 = s1, shape2 = s2) -->
<!-- ``` -->
<!-- Let's deconstruct what is going on with this function, where n = 1 (we simulate 10000 observations from a single set of parameter values).  -->
<!-- ```{r} -->
<!-- dosm.beta.hist -->
<!-- nsim <- 10000 -->
<!-- n <- 1 -->
<!-- s1 <- 0.2 # change this -->
<!-- s2 <- 0.2 # change this -->
<!-- samps <- rbeta(n * nsim, shape1 = s1, shape2 = s2) -->
<!-- str(samps) # here are 10,000 -->
<!-- # We are converting the vector into a matrix -->
<!-- # So that we can easily calculate the mean of each row -->
<!-- sim.mat <- matrix(samps, nrow = nsim) -->
<!-- dim(sim.mat) -->
<!-- head(sim.mat)  -->
<!-- # Calculate rowmeans - with n=1, this doesn't change anything -->
<!-- # But change n to anything bigger and inspect the dimensions of the objects -->
<!-- dosm <- rowMeans(sim.mat) -->
<!-- str(dosm) -->
<!-- head(dosm) # compare these values to sim.mat -->
<!-- par(mfrow = c(1,1)) -->
<!-- hist(dosm, freq = FALSE) # plotting the simulated values -->
<!-- # Set up a vector that goes from 0 to 1 to overlay a normal distribution on the histogram -->
<!-- x <- seq(0, 1, length.out = 1000)  -->
<!-- # Now plot a normal distribution, using the mean and sd of the simulated values -->
<!-- lines(x, dnorm(x, mean = mean(dosm), sd = sd(dosm)), col = "red") -->
<!-- ``` -->
<!-- 3. The Pareto distribution is a skewed, heavy-tailed, power-law distribution used in description of social, scientific, geophysical, actuarial, and many other types of observable phenomena. It was applied originally to the distribution of wealth in a society, fitting the observation that a large portion of wealth is held by a small fraction of the population. Named after the Italian civil engineer, economist, and sociologist Vilfredo Pareto.  -->
<!-- Parameters of the `rpareto` function: -->
<!--   - a: shape (on the web as $\alpha$) -->
<!--   - b: scale (on the web as $x_m$) -->
<!-- If the shape parameter is $\leq$ 1, $E(X)$ is $\infty$.  -->
<!-- If the shape parameter is $\leq$ 2, $Var(X)$ is $\infty$.  -->
<!-- First we simulate many sampes of size 1000 from a Pareto distribution with shape = 4.  -->
<!-- ```{r} -->
<!-- # experiment with n and the parameters a and b -->
<!-- n <- 100      -->
<!-- n_sims <- 10000 -->
<!-- a <- 1 -->
<!-- b <- 4 -->
<!-- x <- rpareto(n = n, a = a, b = b) -->
<!-- summary(x) -->
<!-- # Calculate mean and sd -->
<!-- mu <- mean(x) -->
<!-- stdev <- sd(x) -->
<!-- hist(x, freq = FALSE) -->
<!-- # Set up a vector that goes from 0 to 1 to overlay a normal distribution on the histogram -->
<!-- x_vals <- seq(min(x), max(x), length.out = 1000)  -->
<!-- # Now plot a normal distribution, using the mean and sd of the simulated values -->
<!-- lines(x_vals, dnorm(x_vals, mean = mu, sd = stdev), col = "red") -->
<!-- ``` -->
<!-- ```{r} -->
<!-- # Compare tail to normal -->
<!-- compare.tail.to.normal -->
<!-- k <- 2 # sds -->
<!-- compare.tail.to.normal(x = x, k = k, mu = mu, sigma = stdev) -->
<!-- summary(x) -->
<!-- mu -->
<!-- stdev -->
<!-- # This gives the value of the mean, minus the value k*stdev  -->
<!-- # (i.e., an extreme negative value) -->
<!-- # Below I will use my object stdev in place of sigma (the parameter from Edge's function) -->
<!-- (mu - k * stdev) -->
<!-- # Extreme positive value -->
<!-- (mu + k * stdev) -->
<!-- # This statement asks whether the value in x is an extreme value -->
<!-- # The operator '|' is 'OR' -->
<!-- # Is x extreme negative OR extreme positive? -->
<!-- x < (mu - k * stdev) | x > (mu + k * stdev) -->
<!-- # We can get the frequencies of this logical vector using table -->
<!-- table(x < (mu - k * stdev) | x > (mu + k * stdev)) -->
<!-- # Or, as Edge, does, calculate the average of TRUEs - which is simply the proportion of TRUEs -->
<!-- mean(x < (mu - k * stdev) | x > (mu + k * stdev)) -->
<!-- # What proportion/probability of TRUEs would we expect under a normal probability distribution? -->
<!-- pnorm(k) # probability of observing a value less than k standard deviations above the mean -->
<!-- pnorm(-k) # probability of observing a value less than k standard deviations below the mean -->
<!-- (1 - (pnorm(k) - pnorm(-k))) # probability of observing an extreme value -->
<!-- # So putting it all together, we have the ratio of: -->
<!-- # the probability of observing an extreme value in the data, over the -->
<!-- # the probability of observing an extreme value in a normal distribution: -->
<!-- mean(x < (mu - k * stdev) | x > (mu + k * stdev))/(1 - (pnorm(k) - pnorm(-k))) -->
<!-- compare.tail.to.normal(x = x, k = k, mu = mu, sigma = stdev) -->
<!-- # If this ratio is < 1, then the data have fewer extreme values than suggested by a normal -->
<!-- # If this ratio is > 1, then the data have more extreme values than suggested by a normal -->
<!-- ``` -->
<!-- Above, I haven't computed the means of many simulations - which is the crux of the question! So here I just paste Edge's solution. In it, he calculates $E(X)$ and $Var(X)$ using the Pareto probability distribution. I have changed `n` and `n.sim` to match my values above.  -->
<!-- ```{r} -->
<!-- #Sample size per simulation (n) and number of simulations. -->
<!-- n <- 100 -->
<!-- n.sim <- 10000 -->
<!-- #Pareto parameters. Variance is finite, and so -->
<!-- #CLT applies, if a > 2. For large a, convergence to -->
<!-- #normal is better. With small a, convergence is slow, -->
<!-- #especially in the tails. -->
<!-- a <- 4 -->
<!-- b <- 1 -->
<!-- #Compute the expectation and variance of the distribution -->
<!-- #of the sample mean. a must be above 2 for these expressions -->
<!-- #to hold. -->
<!-- expec.par <- a*b/(a-1) -->
<!-- var.par <- a*b^2 / ((a-1)^2 * (a-2)) -->
<!-- sd.mean <- sqrt(var.par / n) -->
<!-- #Simulate data -->
<!-- sim <- matrix(rpareto(n*n.sim, a, b), nrow = n.sim) -->
<!-- # Each column represents ith sample taken per simulation -->
<!-- # Each row represents a different simulation -->
<!-- sim[1:3, 1:10] -->
<!-- # Compute sample means. -->
<!-- means.sim <- rowMeans(sim) -->
<!-- str(means.sim) -->
<!-- #Draw a histogram of the sample means along with the approximate -->
<!-- #normal pdf that follows from the CLT. -->
<!-- hist(means.sim, prob = TRUE) -->
<!-- curve(dnorm(x, expec.par, sd.mean), add = TRUE, col = 'red') -->
<!-- compare.tail.to.normal(means.sim, 1/2, expec.par, sd.mean) -->
<!-- compare.tail.to.normal(means.sim, 1, expec.par, sd.mean) -->
<!-- compare.tail.to.normal(means.sim, 2, expec.par, sd.mean) -->
<!-- compare.tail.to.normal(means.sim, 3, expec.par, sd.mean) -->
<!-- compare.tail.to.normal(means.sim, 4, expec.par, sd.mean) -->
<!-- compare.tail.to.normal(means.sim, 5, expec.par, sd.mean) -->
<!-- compare.tail.to.normal(means.sim, 6, expec.par, sd.mean) -->
<!-- ``` -->
<!-- ## A probabilistic model for simple linear regression -->
<!-- ### Exercise set 5-5 -->
<!-- 1. Write the square of the correlation coefficient (eq. 5.30) in terms of the variance of Y (eq. 5.32) and the conditional variance of Y given X (eq. 5.31).  -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \text{eq. 5.30: } \rho_{X,Y} = \beta \frac{\sigma_X}{\sigma_Y} \\ -->
<!-- \text{eq. 5.31: } Var(Y) = \beta^2 \sigma_X^2 + \sigma_{\epsilon}^2 \\ -->
<!-- \text{eq. 5.32: } Var(Y \mid X = x) = \sigma_{\epsilon}^2  \\ -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- Squaring $\rho_{X,Y}$, and expressing $Var(Y)$ using the definition from above: -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \rho_{X,Y}^2 = \beta^2 \frac{\sigma_X^2}{\sigma_Y^2} = \beta^2 \frac{\sigma_X^2}{Var(Y)} \\ -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \rho_{X,Y}^2 =  \beta^2 \frac{\sigma_X^2}{\beta^2 \sigma_X^2 + \sigma_{\epsilon}^2} \\ -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- Some algebra... -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \rho_{X,Y}^2 = 1 - \frac{\sigma_{\epsilon}^2}{\beta^2 \sigma_X^2 + \sigma_{\epsilon}^2} \\ -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- And we use the formulas from above again to restate as: -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \rho_{X,Y}^2 = 1 - \frac{Var(Y \mid X = x)}{Var(Y)} \\ -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- which gives us the 'proportion of variance explained'. So if there isn't much variance left in $Y$ after conditioning on $X$ (i.e., the numerator is small relative to the denominator), if we subtract it from 1, we get a high $r^2$. And vice versa.  -->
<!-- 2. Simulating a regression.  -->
<!-- ```{r} -->
<!-- library(stfspack) -->
<!-- sim.lm -->
<!-- sim_0_1 <- sim.lm(n = 50, a = 0, b = 1) -->
<!-- head(sim_0_1) -->
<!-- plot(sim_0_1[,1], sim_0_1[,2]) -->
<!-- ``` -->
<!-- Still using all the default values for parameters: -->
<!-- ```{r} -->
<!-- sim_0_1 <- sim.lm(n = 50, a = 0, b = 1,  -->
<!--                   sigma.disturb = 1, mu.x = 8, sigma.x = 2,  -->
<!--                   rdisturb = rnorm, rx = rnorm, het.coef = 0) -->
<!-- plot(sim_0_1[,1], sim_0_1[,2]) -->
<!-- ``` -->
<!-- Now I'll change one at a time: -->
<!-- ```{r} -->
<!-- sim_0_1 <- sim.lm(n = 50, a = 0, b = 1,  -->
<!--                   sigma.disturb = 2, mu.x = 8, sigma.x = 2,  -->
<!--                   rdisturb = rnorm, rx = rnorm, het.coef = 0) -->
<!-- plot(sim_0_1[,1], sim_0_1[,2]) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- sim_0_1 <- sim.lm(n = 50, a = 0, b = 1,  -->
<!--                   sigma.disturb = 1, mu.x = 16, sigma.x = 2,  -->
<!--                   rdisturb = rnorm, rx = rnorm, het.coef = 0) -->
<!-- plot(sim_0_1[,1], sim_0_1[,2]) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- sim_0_1 <- sim.lm(n = 50, a = 0, b = 1,  -->
<!--                   sigma.disturb = 1, mu.x = 8, sigma.x = 4,  -->
<!--                   rdisturb = rnorm, rx = rnorm, het.coef = 0) -->
<!-- plot(sim_0_1[,1], sim_0_1[,2]) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- sim_0_1 <- sim.lm(n = 50, a = 0, b = 1,  -->
<!--                   sigma.disturb = 1, mu.x = 8, sigma.x = 2,  -->
<!--                   rdisturb = rlaplace, rx = rnorm, het.coef = 0) -->
<!-- plot(sim_0_1[,1], sim_0_1[,2]) -->
<!-- ``` -->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="probability.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
