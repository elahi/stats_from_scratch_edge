<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Properties of point estimators | Notes on Statistical Thinking from Scratch</title>
  <meta name="description" content="Cliff notes for Statistical Thinking from Scratch, by Doc Edge." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Properties of point estimators | Notes on Statistical Thinking from Scratch" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Cliff notes for Statistical Thinking from Scratch, by Doc Edge." />
  <meta name="github-repo" content="elahi/stats_from_scratch_edge" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Properties of point estimators | Notes on Statistical Thinking from Scratch" />
  
  <meta name="twitter:description" content="Cliff notes for Statistical Thinking from Scratch, by Doc Edge." />
  

<meta name="author" content="Robin Elahi" />


<meta name="date" content="2020-09-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="randomvars.html"/>
<link rel="next" href="intervals.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-121894527-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-121894527-4');
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Notes on Statistical Thinking from Scratch</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="r-eda.html"><a href="r-eda.html"><i class="fa fa-check"></i><b>2</b> R and exploratory data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="r-eda.html"><a href="r-eda.html#inspecting-the-dataframe"><i class="fa fa-check"></i><b>2.1</b> Inspecting the dataframe</a></li>
<li class="chapter" data-level="2.2" data-path="r-eda.html"><a href="r-eda.html#histograms"><i class="fa fa-check"></i><b>2.2</b> Histograms</a></li>
<li class="chapter" data-level="2.3" data-path="r-eda.html"><a href="r-eda.html#summarising-data"><i class="fa fa-check"></i><b>2.3</b> Summarising data</a></li>
<li class="chapter" data-level="2.4" data-path="r-eda.html"><a href="r-eda.html#loops"><i class="fa fa-check"></i><b>2.4</b> Loops</a></li>
<li class="chapter" data-level="2.5" data-path="r-eda.html"><a href="r-eda.html#functions"><i class="fa fa-check"></i><b>2.5</b> Functions</a></li>
<li class="chapter" data-level="2.6" data-path="r-eda.html"><a href="r-eda.html#boxplots"><i class="fa fa-check"></i><b>2.6</b> Boxplots</a></li>
<li class="chapter" data-level="2.7" data-path="r-eda.html"><a href="r-eda.html#scatterplots"><i class="fa fa-check"></i><b>2.7</b> Scatterplots</a></li>
<li class="chapter" data-level="2.8" data-path="r-eda.html"><a href="r-eda.html#exercise-set-2-2"><i class="fa fa-check"></i><b>2.8</b> Exercise set 2-2</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="best-fit-line.html"><a href="best-fit-line.html"><i class="fa fa-check"></i><b>3</b> Line of best fit</a><ul>
<li class="chapter" data-level="3.1" data-path="best-fit-line.html"><a href="best-fit-line.html#exercise-set-3-1"><i class="fa fa-check"></i><b>3.1</b> Exercise set 3-1</a></li>
<li class="chapter" data-level="3.2" data-path="best-fit-line.html"><a href="best-fit-line.html#exercise-set-3-2"><i class="fa fa-check"></i><b>3.2</b> Exercise set 3-2</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>4</b> Probability and random variables</a><ul>
<li class="chapter" data-level="4.0.1" data-path="probability.html"><a href="probability.html#probability-vs-estimation"><i class="fa fa-check"></i><b>4.0.1</b> Probability vs estimation</a></li>
<li class="chapter" data-level="4.0.2" data-path="probability.html"><a href="probability.html#what-is-a-probability"><i class="fa fa-check"></i><b>4.0.2</b> What is a probability?</a></li>
<li class="chapter" data-level="4.0.3" data-path="probability.html"><a href="probability.html#set-notation"><i class="fa fa-check"></i><b>4.0.3</b> Set notation</a></li>
<li class="chapter" data-level="4.1" data-path="probability.html"><a href="probability.html#kolmogorovs-three-axioms-of-probability"><i class="fa fa-check"></i><b>4.1</b> Kolmogorov’s three axioms of probability</a><ul>
<li class="chapter" data-level="4.1.1" data-path="probability.html"><a href="probability.html#exercise-set-4-1"><i class="fa fa-check"></i><b>4.1.1</b> Exercise set 4-1</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="probability.html"><a href="probability.html#conditional-probability-and-independence"><i class="fa fa-check"></i><b>4.2</b> Conditional probability and independence</a><ul>
<li class="chapter" data-level="4.2.1" data-path="probability.html"><a href="probability.html#exercise-set-4-2"><i class="fa fa-check"></i><b>4.2.1</b> Exercise set 4-2</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="probability.html"><a href="probability.html#bayes-theorem"><i class="fa fa-check"></i><b>4.3</b> Bayes’ Theorem</a></li>
<li class="chapter" data-level="4.4" data-path="probability.html"><a href="probability.html#discrete-random-variables-and-distributions"><i class="fa fa-check"></i><b>4.4</b> Discrete random variables and distributions</a><ul>
<li class="chapter" data-level="4.4.1" data-path="probability.html"><a href="probability.html#exercise-set-4-3"><i class="fa fa-check"></i><b>4.4.1</b> Exercise set 4-3</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="probability.html"><a href="probability.html#continuous-random-variables-and-distributions"><i class="fa fa-check"></i><b>4.5</b> Continuous random variables and distributions</a><ul>
<li class="chapter" data-level="4.5.1" data-path="probability.html"><a href="probability.html#exercise-set-4-4"><i class="fa fa-check"></i><b>4.5.1</b> Exercise set 4-4</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="probability.html"><a href="probability.html#probability-density-functions"><i class="fa fa-check"></i><b>4.6</b> Probability density functions</a><ul>
<li class="chapter" data-level="4.6.1" data-path="probability.html"><a href="probability.html#additional-viz"><i class="fa fa-check"></i><b>4.6.1</b> Additional viz</a></li>
<li class="chapter" data-level="4.6.2" data-path="probability.html"><a href="probability.html#exercise-set-4-5"><i class="fa fa-check"></i><b>4.6.2</b> Exercise set 4-5</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="probability.html"><a href="probability.html#families-of-distributions"><i class="fa fa-check"></i><b>4.7</b> Families of distributions</a><ul>
<li class="chapter" data-level="4.7.1" data-path="probability.html"><a href="probability.html#exercise-set-4-6"><i class="fa fa-check"></i><b>4.7.1</b> Exercise set 4-6</a></li>
<li class="chapter" data-level="4.7.2" data-path="probability.html"><a href="probability.html#additional-exercise"><i class="fa fa-check"></i><b>4.7.2</b> Additional exercise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="randomvars.html"><a href="randomvars.html"><i class="fa fa-check"></i><b>5</b> Properties of random variables</a><ul>
<li class="chapter" data-level="5.1" data-path="randomvars.html"><a href="randomvars.html#expected-values-and-the-law-of-large-numbers"><i class="fa fa-check"></i><b>5.1</b> Expected values and the law of large numbers</a><ul>
<li class="chapter" data-level="5.1.1" data-path="randomvars.html"><a href="randomvars.html#weak-law-of-large-numbers"><i class="fa fa-check"></i><b>5.1.1</b> Weak law of large numbers</a></li>
<li class="chapter" data-level="5.1.2" data-path="randomvars.html"><a href="randomvars.html#handy-facts-about-expectations"><i class="fa fa-check"></i><b>5.1.2</b> Handy facts about expectations</a></li>
<li class="chapter" data-level="5.1.3" data-path="randomvars.html"><a href="randomvars.html#exercise-set-5-1"><i class="fa fa-check"></i><b>5.1.3</b> Exercise set 5-1</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="randomvars.html"><a href="randomvars.html#variance-and-standard-deviation"><i class="fa fa-check"></i><b>5.2</b> Variance and standard deviation</a><ul>
<li class="chapter" data-level="5.2.1" data-path="randomvars.html"><a href="randomvars.html#beautiful-properties-of-the-variance"><i class="fa fa-check"></i><b>5.2.1</b> Beautiful properties of the variance</a></li>
<li class="chapter" data-level="5.2.2" data-path="randomvars.html"><a href="randomvars.html#exercise-set-5-2"><i class="fa fa-check"></i><b>5.2.2</b> Exercise set 5-2</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="randomvars.html"><a href="randomvars.html#joint-distributions-covariance-and-correlation"><i class="fa fa-check"></i><b>5.3</b> Joint distributions, covariance, and correlation</a><ul>
<li class="chapter" data-level="5.3.1" data-path="randomvars.html"><a href="randomvars.html#joint-probability-distributions"><i class="fa fa-check"></i><b>5.3.1</b> Joint probability distributions</a></li>
<li class="chapter" data-level="5.3.2" data-path="randomvars.html"><a href="randomvars.html#marginal-distributions"><i class="fa fa-check"></i><b>5.3.2</b> Marginal distributions</a></li>
<li class="chapter" data-level="5.3.3" data-path="randomvars.html"><a href="randomvars.html#covariance"><i class="fa fa-check"></i><b>5.3.3</b> Covariance</a></li>
<li class="chapter" data-level="5.3.4" data-path="randomvars.html"><a href="randomvars.html#correlation"><i class="fa fa-check"></i><b>5.3.4</b> Correlation</a></li>
<li class="chapter" data-level="5.3.5" data-path="randomvars.html"><a href="randomvars.html#additional-exercise-1"><i class="fa fa-check"></i><b>5.3.5</b> Additional exercise</a></li>
<li class="chapter" data-level="5.3.6" data-path="randomvars.html"><a href="randomvars.html#exercise-set-5-3"><i class="fa fa-check"></i><b>5.3.6</b> Exercise set 5-3</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="randomvars.html"><a href="randomvars.html#conditional-distribution-expectation-variance"><i class="fa fa-check"></i><b>5.4</b> Conditional distribution, expectation, variance</a></li>
<li class="chapter" data-level="5.5" data-path="randomvars.html"><a href="randomvars.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>5.5</b> The central limit theorem</a><ul>
<li class="chapter" data-level="5.5.1" data-path="randomvars.html"><a href="randomvars.html#exercise-set-5-4"><i class="fa fa-check"></i><b>5.5.1</b> Exercise set 5-4</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="randomvars.html"><a href="randomvars.html#a-probabilistic-model-for-simple-linear-regression"><i class="fa fa-check"></i><b>5.6</b> A probabilistic model for simple linear regression</a><ul>
<li class="chapter" data-level="5.6.1" data-path="randomvars.html"><a href="randomvars.html#assumptions-of-the-linear-model"><i class="fa fa-check"></i><b>5.6.1</b> Assumptions of the linear model</a></li>
<li class="chapter" data-level="5.6.2" data-path="randomvars.html"><a href="randomvars.html#important-claims-that-follow-from-these-assumptions"><i class="fa fa-check"></i><b>5.6.2</b> Important claims that follow from these assumptions</a></li>
<li class="chapter" data-level="5.6.3" data-path="randomvars.html"><a href="randomvars.html#checking-these-assumptions"><i class="fa fa-check"></i><b>5.6.3</b> Checking these assumptions</a></li>
<li class="chapter" data-level="5.6.4" data-path="randomvars.html"><a href="randomvars.html#exercise-set-5-5"><i class="fa fa-check"></i><b>5.6.4</b> Exercise set 5-5</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="estimators.html"><a href="estimators.html"><i class="fa fa-check"></i><b>6</b> Properties of point estimators</a><ul>
<li class="chapter" data-level="6.1" data-path="estimators.html"><a href="estimators.html#bias"><i class="fa fa-check"></i><b>6.1</b> Bias</a><ul>
<li class="chapter" data-level="6.1.1" data-path="estimators.html"><a href="estimators.html#exercise-set-6-1"><i class="fa fa-check"></i><b>6.1.1</b> Exercise set 6-1</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="estimators.html"><a href="estimators.html#variance"><i class="fa fa-check"></i><b>6.2</b> Variance</a><ul>
<li class="chapter" data-level="6.2.1" data-path="estimators.html"><a href="estimators.html#exercise-set-6-2"><i class="fa fa-check"></i><b>6.2.1</b> Exercise set 6-2</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="estimators.html"><a href="estimators.html#mean-squared-error"><i class="fa fa-check"></i><b>6.3</b> Mean squared error</a><ul>
<li class="chapter" data-level="6.3.1" data-path="estimators.html"><a href="estimators.html#exercise-set-6-3"><i class="fa fa-check"></i><b>6.3.1</b> Exercise set 6-3</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="estimators.html"><a href="estimators.html#consistency"><i class="fa fa-check"></i><b>6.4</b> Consistency</a><ul>
<li class="chapter" data-level="6.4.1" data-path="estimators.html"><a href="estimators.html#exercise-set-6-4"><i class="fa fa-check"></i><b>6.4.1</b> Exercise set 6-4</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="estimators.html"><a href="estimators.html#efficiency"><i class="fa fa-check"></i><b>6.5</b> Efficiency</a><ul>
<li class="chapter" data-level="6.5.1" data-path="estimators.html"><a href="estimators.html#exercise-set-6-5"><i class="fa fa-check"></i><b>6.5.1</b> Exercise set 6-5</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="estimators.html"><a href="estimators.html#statistical-decision-theory-and-risk"><i class="fa fa-check"></i><b>6.6</b> Statistical decision theory and risk</a></li>
<li class="chapter" data-level="6.7" data-path="estimators.html"><a href="estimators.html#robustness"><i class="fa fa-check"></i><b>6.7</b> Robustness</a><ul>
<li class="chapter" data-level="6.7.1" data-path="estimators.html"><a href="estimators.html#exercise-set-6-8"><i class="fa fa-check"></i><b>6.7.1</b> Exercise set 6-8</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="estimators.html"><a href="estimators.html#estimators-for-simple-linear-regression"><i class="fa fa-check"></i><b>6.8</b> Estimators for simple linear regression</a><ul>
<li class="chapter" data-level="6.8.1" data-path="estimators.html"><a href="estimators.html#exercise-set-6-9"><i class="fa fa-check"></i><b>6.8.1</b> Exercise set 6-9</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="intervals.html"><a href="intervals.html"><i class="fa fa-check"></i><b>7</b> Interval estimation and inference</a><ul>
<li class="chapter" data-level="7.1" data-path="intervals.html"><a href="intervals.html#standard-error"><i class="fa fa-check"></i><b>7.1</b> Standard error</a><ul>
<li class="chapter" data-level="7.1.1" data-path="intervals.html"><a href="intervals.html#exercise-set-7-1"><i class="fa fa-check"></i><b>7.1.1</b> Exercise set 7-1</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="intervals.html"><a href="intervals.html#confidence-intervals"><i class="fa fa-check"></i><b>7.2</b> Confidence intervals</a><ul>
<li class="chapter" data-level="7.2.1" data-path="intervals.html"><a href="intervals.html#exercise-set-7-2"><i class="fa fa-check"></i><b>7.2.1</b> Exercise set 7-2</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="intervals.html"><a href="intervals.html#frequentist-inference-i-null-hypotheses-test-statistics-and-p-values"><i class="fa fa-check"></i><b>7.3</b> Frequentist inference I: null hypotheses, test statistics, and <em>p</em> values</a><ul>
<li class="chapter" data-level="7.3.1" data-path="intervals.html"><a href="intervals.html#exercise-set-7-3"><i class="fa fa-check"></i><b>7.3.1</b> Exercise set 7-3</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="intervals.html"><a href="intervals.html#frequentist-inference-ii-alternative-hypotheses-and-the-rejection-framework"><i class="fa fa-check"></i><b>7.4</b> Frequentist inference II: alternative hypotheses and the rejection framework</a><ul>
<li class="chapter" data-level="7.4.1" data-path="intervals.html"><a href="intervals.html#exercise-set-7-4"><i class="fa fa-check"></i><b>7.4.1</b> Exercise set 7-4</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="intervals.html"><a href="intervals.html#connecting-hypothesis-tests-and-confidence-intervals"><i class="fa fa-check"></i><b>7.5</b> Connecting hypothesis tests and confidence intervals</a></li>
<li class="chapter" data-level="7.6" data-path="intervals.html"><a href="intervals.html#nhst-and-the-abuse-of-tests"><i class="fa fa-check"></i><b>7.6</b> NHST and the abuse of tests</a><ul>
<li class="chapter" data-level="7.6.1" data-path="intervals.html"><a href="intervals.html#exercise-set-7-5"><i class="fa fa-check"></i><b>7.6.1</b> Exercise set 7-5</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="intervals.html"><a href="intervals.html#frequentist-inference-iii-power"><i class="fa fa-check"></i><b>7.7</b> Frequentist inference III: power</a><ul>
<li class="chapter" data-level="7.7.1" data-path="intervals.html"><a href="intervals.html#exercise-set-7-6"><i class="fa fa-check"></i><b>7.7.1</b> Exercise set 7-6</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="intervals.html"><a href="intervals.html#putting-it-together-what-happens-when-the-sample-size-increases"><i class="fa fa-check"></i><b>7.8</b> Putting it together: what happens when the sample size increases?</a></li>
<li class="chapter" data-level="7.9" data-path="intervals.html"><a href="intervals.html#chapter-summary"><i class="fa fa-check"></i><b>7.9</b> Chapter summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="semiparametric.html"><a href="semiparametric.html"><i class="fa fa-check"></i><b>8</b> Semiparametric estimation and inference</a><ul>
<li class="chapter" data-level="8.0.1" data-path="semiparametric.html"><a href="semiparametric.html#exercise-set-8-1"><i class="fa fa-check"></i><b>8.0.1</b> Exercise set 8-1</a></li>
<li class="chapter" data-level="8.1" data-path="semiparametric.html"><a href="semiparametric.html#semiparametric-point-estimation-using-the-method-of-moments"><i class="fa fa-check"></i><b>8.1</b> Semiparametric point estimation using the method of moments</a><ul>
<li class="chapter" data-level="8.1.1" data-path="semiparametric.html"><a href="semiparametric.html#introduction-to-moments"><i class="fa fa-check"></i><b>8.1.1</b> Introduction to moments</a></li>
<li class="chapter" data-level="8.1.2" data-path="semiparametric.html"><a href="semiparametric.html#plug-in-estimators"><i class="fa fa-check"></i><b>8.1.2</b> Plug-in estimators</a></li>
<li class="chapter" data-level="8.1.3" data-path="semiparametric.html"><a href="semiparametric.html#exercise-set-8-2"><i class="fa fa-check"></i><b>8.1.3</b> Exercise set 8-2</a></li>
<li class="chapter" data-level="8.1.4" data-path="semiparametric.html"><a href="semiparametric.html#the-method-of-moments"><i class="fa fa-check"></i><b>8.1.4</b> The method of moments</a></li>
<li class="chapter" data-level="8.1.5" data-path="semiparametric.html"><a href="semiparametric.html#exercise-set-8-3"><i class="fa fa-check"></i><b>8.1.5</b> Exercise set 8-3</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="semiparametric.html"><a href="semiparametric.html#semiparametric-interval-estimation-using-the-bootstrap"><i class="fa fa-check"></i><b>8.2</b> Semiparametric interval estimation using the bootstrap</a><ul>
<li class="chapter" data-level="8.2.1" data-path="semiparametric.html"><a href="semiparametric.html#exercise-set-8-4"><i class="fa fa-check"></i><b>8.2.1</b> Exercise set 8-4</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="semiparametric.html"><a href="semiparametric.html#semiparametric-hypothesis-testing-using-permutation-tests"><i class="fa fa-check"></i><b>8.3</b> Semiparametric hypothesis testing using permutation tests</a><ul>
<li class="chapter" data-level="8.3.1" data-path="semiparametric.html"><a href="semiparametric.html#exercise-set-8-5"><i class="fa fa-check"></i><b>8.3.1</b> Exercise set 8-5</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="semiparametric.html"><a href="semiparametric.html#chapter-summary-1"><i class="fa fa-check"></i><b>8.4</b> Chapter summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="parametric.html"><a href="parametric.html"><i class="fa fa-check"></i><b>9</b> Parametric estimation and inference</a><ul>
<li class="chapter" data-level="9.0.1" data-path="parametric.html"><a href="parametric.html#exercise-set-9-1"><i class="fa fa-check"></i><b>9.0.1</b> Exercise set 9-1</a></li>
<li class="chapter" data-level="9.1" data-path="parametric.html"><a href="parametric.html#parametric-estimation-using-maximum-likelihood"><i class="fa fa-check"></i><b>9.1</b> Parametric estimation using maximum likelihood</a><ul>
<li class="chapter" data-level="9.1.1" data-path="parametric.html"><a href="parametric.html#exercise-set-9-2"><i class="fa fa-check"></i><b>9.1.1</b> Exercise set 9-2</a></li>
<li class="chapter" data-level="9.1.2" data-path="parametric.html"><a href="parametric.html#maximum-likelihood-estimation-for-simple-linear-regression"><i class="fa fa-check"></i><b>9.1.2</b> Maximum-likelihood estimation for simple linear regression</a></li>
<li class="chapter" data-level="9.1.3" data-path="parametric.html"><a href="parametric.html#exercise-set-9-3"><i class="fa fa-check"></i><b>9.1.3</b> Exercise set 9-3</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="parametric.html"><a href="parametric.html#parametric-interval-estimation-the-direct-approach-and-fisher-information"><i class="fa fa-check"></i><b>9.2</b> Parametric interval estimation: the direct approach and Fisher information</a><ul>
<li class="chapter" data-level="9.2.1" data-path="parametric.html"><a href="parametric.html#exercise-set-9-4"><i class="fa fa-check"></i><b>9.2.1</b> Exercise set 9-4</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="parametric.html"><a href="parametric.html#parametric-hypothesis-testing-using-the-wald-test"><i class="fa fa-check"></i><b>9.3</b> Parametric hypothesis testing using the Wald test</a><ul>
<li class="chapter" data-level="9.3.1" data-path="parametric.html"><a href="parametric.html#wald-test"><i class="fa fa-check"></i><b>9.3.1</b> Wald test</a></li>
<li class="chapter" data-level="9.3.2" data-path="parametric.html"><a href="parametric.html#exercise-set-9-5"><i class="fa fa-check"></i><b>9.3.2</b> Exercise set 9-5</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="parametric.html"><a href="parametric.html#parametric-hypothesis-testing-using-the-likelihood-ratio-test"><i class="fa fa-check"></i><b>9.4</b> Parametric hypothesis testing using the likelihood-ratio test</a><ul>
<li class="chapter" data-level="9.4.1" data-path="parametric.html"><a href="parametric.html#exercise-set-9-6"><i class="fa fa-check"></i><b>9.4.1</b> Exercise set 9-6</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>10</b> Bayesian estimation and inference</a><ul>
<li class="chapter" data-level="10.1" data-path="bayesian.html"><a href="bayesian.html#how-to-choose-a-prior-distribution"><i class="fa fa-check"></i><b>10.1</b> How to choose a prior distribution?</a></li>
<li class="chapter" data-level="10.2" data-path="bayesian.html"><a href="bayesian.html#the-unscaled-posterior-conjugacy-and-sampling-from-the-posterior"><i class="fa fa-check"></i><b>10.2</b> The unscaled posterior, conjugacy, and sampling from the posterior</a><ul>
<li class="chapter" data-level="10.2.1" data-path="bayesian.html"><a href="bayesian.html#rejection-sampling-algorithm"><i class="fa fa-check"></i><b>10.2.1</b> Rejection sampling algorithm</a></li>
<li class="chapter" data-level="10.2.2" data-path="bayesian.html"><a href="bayesian.html#exercise-set-10-1"><i class="fa fa-check"></i><b>10.2.2</b> Exercise set 10-1</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="bayesian.html"><a href="bayesian.html#bayesian-point-estimation-using-bayes-estimators"><i class="fa fa-check"></i><b>10.3</b> Bayesian point estimation using Bayes estimators</a><ul>
<li class="chapter" data-level="10.3.1" data-path="bayesian.html"><a href="bayesian.html#exercise-set-10-2"><i class="fa fa-check"></i><b>10.3.1</b> Exercise set 10-2</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="bayesian.html"><a href="bayesian.html#bayesian-interval-estimation-using-credible-intervals"><i class="fa fa-check"></i><b>10.4</b> Bayesian interval estimation using credible intervals</a><ul>
<li class="chapter" data-level="10.4.1" data-path="bayesian.html"><a href="bayesian.html#exercise-set-10-3"><i class="fa fa-check"></i><b>10.4.1</b> Exercise set 10-3</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="bayesian.html"><a href="bayesian.html#bayesian-hypothesis-testing-using-bayes-factors"><i class="fa fa-check"></i><b>10.5</b> Bayesian ‘hypothesis testing’ using Bayes factors</a><ul>
<li class="chapter" data-level="10.5.1" data-path="bayesian.html"><a href="bayesian.html#exercise-set-10-4"><i class="fa fa-check"></i><b>10.5.1</b> Exercise set 10-4</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="bayesian.html"><a href="bayesian.html#conclusion-bayesian-vs.-frequentist-methods"><i class="fa fa-check"></i><b>10.6</b> Conclusion: Bayesian vs. frequentist methods</a></li>
<li class="chapter" data-level="10.7" data-path="bayesian.html"><a href="bayesian.html#chapter-summary-2"><i class="fa fa-check"></i><b>10.7</b> Chapter summary</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes on Statistical Thinking from Scratch</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="estimators" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Properties of point estimators</h1>
<p>Estimator (<span class="math inline">\(\hat{\theta}\)</span>): a function that can be applied to data to produce estimates of a true quantity</p>
<p>Estimate (<span class="math inline">\(\theta\)</span>): the value of a quantity returned by an estimator applied to data (e.g., a sample mean, a parameter of a probability distribution, a regression coefficient)</p>
<p>Estimand (<span class="math inline">\(\theta\)</span>): the quantity we are trying to estimate</p>
<p>Before we see the data, they can be represented as a random variable, <span class="math inline">\(D\)</span>. After we see the data, they are no longer random, and represented by <span class="math inline">\(d\)</span>.</p>
<p>If we apply an estimator to <span class="math inline">\(D\)</span>, the result is <span class="math inline">\(\hat{\theta}(D)\)</span>, which can vary from one dataset to the next. The distribution of an estimator is sometimes called the <em>sampling distribution</em>. The sample size of <span class="math inline">\(D\)</span> will affect the estimate, and we use <span class="math inline">\(n\)</span> to denote the sample size in the expression, <span class="math inline">\(\hat{\theta}_n(D)\)</span>.</p>
<p>In the first example, we apply the following estimator to <span class="math inline">\(n\)</span> i.i.d. random variables <span class="math inline">\(X_i\)</span>, which are normally distributed with a mean <span class="math inline">\(\theta\)</span> and variance of 1 (<span class="math inline">\(X_i \sim N(\theta, 1)\)</span>):</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\theta}_n(D) = \frac{1}{n} \sum_{i = 1}^n X_i \\
\end{aligned}
\]</span></p>
<p>That is, we would like to estimate the mean of our random variables, <span class="math inline">\(X_i\)</span>. We can do this in <code>R</code>, for 25 random variables that have a mean of 0 and a variance of 1:</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="estimators.html#cb174-1"></a><span class="kw">set.seed</span>(<span class="dv">12</span>)</span>
<span id="cb174-2"><a href="estimators.html#cb174-2"></a>d &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">25</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)</span>
<span id="cb174-3"><a href="estimators.html#cb174-3"></a><span class="kw">mean</span>(d)</span></code></pre></div>
<pre><code>## [1] -0.1883626</code></pre>
<div id="bias" class="section level2">
<h2><span class="header-section-number">6.1</span> Bias</h2>
<p>The <em>bias</em> of an estimator is the difference between the true value of the quantity we’re interested in
(e.g., <span class="math inline">\(\theta\)</span>), and the expectation of our estimator (<span class="math inline">\(E[\hat{\theta}_n(D)]\)</span>). That is, our estimator is unbiased when it does not consistently over- or under- estimate the expectation; that is to say, it is <em>accurate</em>. Formally, the bias is:</p>
<p><span class="math display">\[
\begin{aligned}
\text{B}(\hat{\theta}_n) = E[\hat{\theta}_n(D)] - \theta \\
\end{aligned}
\]</span></p>
<p>We prefer estimators whose bias is 0 (i.e., unbiased).</p>
<div id="exercise-set-6-1" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Exercise set 6-1</h3>
<ol style="list-style-type: decimal">
<li><p>The bias of the sample mean for <span class="math inline">\(X_i \sim N(\theta, 1)\)</span> is 0. See handwritten notes.</p></li>
<li><p>Examining estimators for a normal distribution:</p></li>
</ol>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="estimators.html#cb176-1"></a><span class="kw">library</span>(stfspack)</span>
<span id="cb176-2"><a href="estimators.html#cb176-2"></a>s.mat &lt;-<span class="st"> </span><span class="kw">mat.samps</span>(<span class="dt">n =</span> <span class="dv">25</span>, <span class="dt">nsim =</span> <span class="dv">10000</span>)</span>
<span id="cb176-3"><a href="estimators.html#cb176-3"></a><span class="kw">str</span>(s.mat)</span></code></pre></div>
<pre><code>##  num [1:10000, 1:25] 0.435 -1.023 -1.746 1.36 -0.642 ...</code></pre>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="estimators.html#cb178-1"></a><span class="co">## Sampling distribution of means</span></span>
<span id="cb178-2"><a href="estimators.html#cb178-2"></a>ests.mean &lt;-<span class="st"> </span><span class="kw">apply</span>(s.mat, <span class="dv">1</span>, mean)</span>
<span id="cb178-3"><a href="estimators.html#cb178-3"></a>mean_of_means &lt;-<span class="st"> </span><span class="kw">mean</span>(ests.mean)</span>
<span id="cb178-4"><a href="estimators.html#cb178-4"></a></span>
<span id="cb178-5"><a href="estimators.html#cb178-5"></a><span class="co">## Sampling distribution of medians</span></span>
<span id="cb178-6"><a href="estimators.html#cb178-6"></a>ests.median &lt;-<span class="st"> </span><span class="kw">apply</span>(s.mat, <span class="dv">1</span>, median)</span>
<span id="cb178-7"><a href="estimators.html#cb178-7"></a>mean_of_medians &lt;-<span class="st"> </span><span class="kw">mean</span>(ests.median)</span>
<span id="cb178-8"><a href="estimators.html#cb178-8"></a></span>
<span id="cb178-9"><a href="estimators.html#cb178-9"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb178-10"><a href="estimators.html#cb178-10"></a><span class="kw">hist</span>(ests.mean)</span>
<span id="cb178-11"><a href="estimators.html#cb178-11"></a><span class="kw">abline</span>(<span class="dt">v =</span> mean_of_means, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb178-12"><a href="estimators.html#cb178-12"></a><span class="kw">hist</span>(ests.median)</span>
<span id="cb178-13"><a href="estimators.html#cb178-13"></a><span class="kw">abline</span>(<span class="dt">v =</span> mean_of_medians, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="06_point-estimators_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>The mean and the median both appear unbiased - that is, their sampling distributions are centered on 0.</p>
</div>
</div>
<div id="variance" class="section level2">
<h2><span class="header-section-number">6.2</span> Variance</h2>
<p>An estimator’s variance measures the <em>precision</em> of the estimator. We define it in the same way that we defined the variance of a random variable:</p>
<p><span class="math display">\[
\begin{aligned}
\text{Var}(\hat{\theta}_n) =&amp; ~ E[(\hat{\theta}_n(D) - E[\hat{\theta}_n(D)])^2] \\
=&amp; ~ E[(\hat{\theta}_n(D)^2] - (E[\hat{\theta}_n(D)])^2 \\
\end{aligned}
\]</span></p>
<div id="exercise-set-6-2" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Exercise set 6-2</h3>
<ol style="list-style-type: decimal">
<li><p>The variance of the sample mean for <span class="math inline">\(X_i \sim N(\theta, 1)\)</span> is <span class="math inline">\(\frac{1}{n}\)</span>. If we don’t know the distribution from which <span class="math inline">\(X_i\)</span> are drawn, just that it has a finite variance, <span class="math inline">\(\sigma^2\)</span>, the variance of the sample means is <span class="math inline">\(\frac{\sigma^2}{n}\)</span>. See handwritten notes.</p></li>
<li><p>Does the sample median have a larger or smaller variance than the mean, when data are drawn from a normal distribution?</p></li>
</ol>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="estimators.html#cb179-1"></a><span class="kw">library</span>(stfspack) </span>
<span id="cb179-2"><a href="estimators.html#cb179-2"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb179-3"><a href="estimators.html#cb179-3"></a>n_sims &lt;-<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb179-4"><a href="estimators.html#cb179-4"></a>s.mat &lt;-<span class="st"> </span><span class="kw">mat.samps</span>(<span class="dt">n =</span> <span class="dv">25</span>, <span class="dt">nsim =</span> n_sims)</span>
<span id="cb179-5"><a href="estimators.html#cb179-5"></a><span class="kw">str</span>(s.mat)</span></code></pre></div>
<pre><code>##  num [1:10000, 1:25] 1.414 0.468 -0.785 -0.257 -0.205 ...</code></pre>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="estimators.html#cb181-1"></a><span class="co">## Sampling distribution of means</span></span>
<span id="cb181-2"><a href="estimators.html#cb181-2"></a>ests.mean &lt;-<span class="st"> </span><span class="kw">apply</span>(s.mat, <span class="dv">1</span>, mean)</span>
<span id="cb181-3"><a href="estimators.html#cb181-3"></a>var_of_means &lt;-<span class="st"> </span><span class="kw">var</span>(ests.mean)</span>
<span id="cb181-4"><a href="estimators.html#cb181-4"></a></span>
<span id="cb181-5"><a href="estimators.html#cb181-5"></a><span class="co">## Sampling distribution of medians</span></span>
<span id="cb181-6"><a href="estimators.html#cb181-6"></a>ests.median &lt;-<span class="st"> </span><span class="kw">apply</span>(s.mat, <span class="dv">1</span>, median)</span>
<span id="cb181-7"><a href="estimators.html#cb181-7"></a>var_of_medians &lt;-<span class="st"> </span><span class="kw">var</span>(ests.median)</span>
<span id="cb181-8"><a href="estimators.html#cb181-8"></a>df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">estimator =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;mean&quot;</span>, n_sims), <span class="kw">rep</span>(<span class="st">&quot;median&quot;</span>, n_sims)), </span>
<span id="cb181-9"><a href="estimators.html#cb181-9"></a>                 <span class="dt">value =</span> <span class="kw">c</span>(ests.mean, ests.median))</span>
<span id="cb181-10"><a href="estimators.html#cb181-10"></a></span>
<span id="cb181-11"><a href="estimators.html#cb181-11"></a>df <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb181-12"><a href="estimators.html#cb181-12"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(value, <span class="dt">fill =</span> estimator)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb181-13"><a href="estimators.html#cb181-13"></a><span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">alpha =</span> <span class="fl">0.3</span>)</span></code></pre></div>
<p><img src="06_point-estimators_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>The sample median has a larger variance than the sample mean.</p>
</div>
</div>
<div id="mean-squared-error" class="section level2">
<h2><span class="header-section-number">6.3</span> Mean squared error</h2>
<p>The <em>mean squared error</em> is a metric that combines bias (accuracy) and variance (precision) in one metric. It is the expected squared difference between the value of an estimator and the true quantity:</p>
<p><span class="math display">\[
\begin{aligned}
\text{MSE}(\hat{\theta}_n) =&amp; ~ E[(\hat{\theta}_n(D) - \theta)^2] \\
=&amp; ~ \text{B}(\hat{\theta}_n)^2 + \text{Var}(\hat{\theta}_n)
\end{aligned}
\]</span></p>
<div id="exercise-set-6-3" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Exercise set 6-3</h3>
<ol style="list-style-type: decimal">
<li><p>See handwritten notes or Edge solution.</p></li>
<li><p>When estimating the first parameter of a normal distribution, the sample mean will have a lower mean squared error. This is because although both are unbiased estimators, the sample median has a higher variance. If bias for both is equal to zero, then the mean square error is simply the variance of the estimator.</p></li>
</ol>
</div>
</div>
<div id="consistency" class="section level2">
<h2><span class="header-section-number">6.4</span> Consistency</h2>
<p>As we collect more data, it would be preferable for the estimator to get closer and closer to the estimand; that is, we wish the estimator to be <em>consistent</em>. A consistent estimator is one that converges in probability to the true value, and is defined formally as:</p>
<p><span class="math display">\[
\begin{aligned}
\text{lim}_{n \rightarrow \infty} \text{P}(|\hat{\theta}_n - \theta| &gt; \delta) = 0
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\delta\)</span> is any positive number.</p>
<p>It turns out (if we complete a proof) that an estimator <span class="math inline">\(\hat{\theta}_n\)</span> is consistent if:</p>
<p><span class="math display">\[
\begin{aligned}
\text{lim}_{n \rightarrow \infty} \text{MSE}(\hat{\theta}_n) = 0
\end{aligned}
\]</span></p>
<p>that is, an estimator is consistent if the mean squared error goes to zero as the sample size increases to infinity. Both biased and unbiased estimators can be consistent, Edge’s figure 6.2 demonstrates this nicely.</p>
<div id="exercise-set-6-4" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Exercise set 6-4</h3>
<ol style="list-style-type: decimal">
<li>Yes, the sample mean is consistent as an estimator of the first parameter (the population mean) of a normal distribution. We know this because the MSE is the sum of the bias squared and the variance. As <span class="math inline">\(n\)</span> increases, variance goes to zero (because <span class="math inline">\(\frac{\sigma^2}{n}\)</span>). And we already showed (in ex. 6-1-2) that at large <em>n</em>, the bias of the sample mean is zero (this is also true at small <em>n</em>).</li>
</ol>
<ul>
<li>The first parameter of a normal distribution is also its expectation - so can we extend the sample mean as a consistent estimator of any distribution, as long as it has a finite variance? Yes, because the law of large numbers applies to any distribution. See Edge explanation.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Use simulations to show that the sample median is a consistent estimator of the first parameter of a normal distribution. We already know that the sample median is unbiased, so the following simulations focus on the variance.</li>
</ol>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="estimators.html#cb182-1"></a><span class="kw">library</span>(stfspack)</span>
<span id="cb182-2"><a href="estimators.html#cb182-2"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb182-3"><a href="estimators.html#cb182-3"></a>n_sims &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb182-4"><a href="estimators.html#cb182-4"></a>n_samps &lt;-<span class="st"> </span><span class="dv">25</span></span>
<span id="cb182-5"><a href="estimators.html#cb182-5"></a></span>
<span id="cb182-6"><a href="estimators.html#cb182-6"></a>get_var_of_median &lt;-<span class="st"> </span><span class="cf">function</span>(n_sims, n_samps){</span>
<span id="cb182-7"><a href="estimators.html#cb182-7"></a>  s.mat &lt;-<span class="st"> </span><span class="kw">mat.samps</span>(<span class="dt">n =</span> n_samps, <span class="dt">nsim =</span> n_sims)</span>
<span id="cb182-8"><a href="estimators.html#cb182-8"></a>  ests.median &lt;-<span class="st"> </span><span class="kw">apply</span>(s.mat, <span class="dv">1</span>, median)</span>
<span id="cb182-9"><a href="estimators.html#cb182-9"></a>  var_of_medians &lt;-<span class="st"> </span><span class="kw">var</span>(ests.median)</span>
<span id="cb182-10"><a href="estimators.html#cb182-10"></a>  <span class="kw">return</span>(var_of_medians)</span>
<span id="cb182-11"><a href="estimators.html#cb182-11"></a>}</span>
<span id="cb182-12"><a href="estimators.html#cb182-12"></a></span>
<span id="cb182-13"><a href="estimators.html#cb182-13"></a>n_samps_vector &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">25</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">500</span>) </span>
<span id="cb182-14"><a href="estimators.html#cb182-14"></a>vector_length &lt;-<span class="st"> </span><span class="kw">length</span>(n_samps_vector)</span>
<span id="cb182-15"><a href="estimators.html#cb182-15"></a>var_vector &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, vector_length)</span>
<span id="cb182-16"><a href="estimators.html#cb182-16"></a></span>
<span id="cb182-17"><a href="estimators.html#cb182-17"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>vector_length){</span>
<span id="cb182-18"><a href="estimators.html#cb182-18"></a>  var_vector[i] &lt;-<span class="st"> </span><span class="kw">get_var_of_median</span>(n_sims, <span class="dt">n_samps =</span> n_samps_vector[i])</span>
<span id="cb182-19"><a href="estimators.html#cb182-19"></a>}</span>
<span id="cb182-20"><a href="estimators.html#cb182-20"></a></span>
<span id="cb182-21"><a href="estimators.html#cb182-21"></a><span class="kw">plot</span>(n_samps_vector, var_vector, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">pch =</span> <span class="dv">20</span>, </span>
<span id="cb182-22"><a href="estimators.html#cb182-22"></a>     <span class="dt">xlab =</span> <span class="st">&quot;Number of samples&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Variance of medians&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;b&quot;</span>) </span>
<span id="cb182-23"><a href="estimators.html#cb182-23"></a><span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">0</span>, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;gray&quot;</span>)</span></code></pre></div>
<p><img src="06_point-estimators_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<ol start="3" style="list-style-type: decimal">
<li>Considering <span class="math inline">\(n\)</span> i.i.d. random variables <span class="math inline">\(X_i\)</span>, which are normally distributed with a mean <span class="math inline">\(\theta\)</span> and variance of 1 (<span class="math inline">\(X_i \sim N(\theta, 1)\)</span>), are the following estimators (i) unbiased; and (ii) consistent?</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li><p>The sample mean is unbiased and consistent, we have shown these already.</p></li>
<li><p>A shifted sample mean will be biased (e.g., because we are adding a constant to each value prior to taking the average), and will not be consistent because the mean squared error will never approach 0 (because it is biased). Also see Edge’s explanation.</p></li>
<li><p>The first observation, <span class="math inline">\(X_1\)</span> will be an unbiased estimator because it has an expectation of <span class="math inline">\(\theta\)</span>. It has a variance of 1, and thus it is not consistent (because the variance does not approach zero as the number of samples increases).</p></li>
<li><p>A ‘shrunk’ sample mean is biased and consistent; see handwritten notes for solution.</p></li>
</ol>
<ol start="4" style="list-style-type: decimal">
<li>Prove that if Eq. 6.7 holds, then Eq. 6.6 also holds. SKIPPED; see Edge for solution.</li>
</ol>
</div>
</div>
<div id="efficiency" class="section level2">
<h2><span class="header-section-number">6.5</span> Efficiency</h2>
<p>In addition to consistency, we’d like it if we our estimator was <em>efficient</em> - that is, it takes fewer samples to achieve a desired level of performance. One way to define the relative efficiency of two estimators is:</p>
<p><span class="math display">\[
\begin{aligned}
\text{RE}(\hat{\theta}_n, \tilde{\theta}_n) =&amp; ~ \frac{\text{MSE}(\hat{\theta}_n)} {\text{MSE}(\tilde{\theta}_n)} \\
\end{aligned}
\]</span></p>
<p>Check out Edge’s Figure 6-3 for a visual.</p>
<p>We can also consider the relative efficiency as the sample size goes to infinity, otherwise known as the <em>asymptotic relative efficiency</em>:</p>
<p><span class="math display">\[
\begin{aligned}
\text{ARE}(\hat{\theta}_n, \tilde{\theta}_n) =&amp; ~ \text{lim}_{n \rightarrow \infty} \frac{\text{MSE}(\hat{\theta}_n)} {\text{MSE}(\tilde{\theta}_n)} \\
\end{aligned}
\]</span></p>
<div id="exercise-set-6-5" class="section level3">
<h3><span class="header-section-number">6.5.1</span> Exercise set 6-5</h3>
<ol style="list-style-type: decimal">
<li>Exploring the relative efficiency of the mean and median for normal samples of different sizes. Consider a Normal(0, 1) distribution.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Estimate MSE using 10000 normal samples of size 5.</li>
</ol>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="estimators.html#cb183-1"></a>mu &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb183-2"><a href="estimators.html#cb183-2"></a>s.mat &lt;-<span class="st"> </span><span class="kw">mat.samps</span>(<span class="dt">n =</span> <span class="dv">5</span>, <span class="dt">nsim =</span> <span class="dv">10000</span>)</span>
<span id="cb183-3"><a href="estimators.html#cb183-3"></a>ests.mean &lt;-<span class="st"> </span><span class="kw">apply</span>(s.mat, <span class="dv">1</span>, mean)</span>
<span id="cb183-4"><a href="estimators.html#cb183-4"></a>ests.median &lt;-<span class="st"> </span><span class="kw">apply</span>(s.mat, <span class="dv">1</span>, median)</span>
<span id="cb183-5"><a href="estimators.html#cb183-5"></a></span>
<span id="cb183-6"><a href="estimators.html#cb183-6"></a><span class="co">#The relative efficiency is estimated as the quotient of the</span></span>
<span id="cb183-7"><a href="estimators.html#cb183-7"></a><span class="co">#MSEs. The relative efficiency of the sample mean vs. the</span></span>
<span id="cb183-8"><a href="estimators.html#cb183-8"></a><span class="co">#sample median has the MSE of the sample mean in the</span></span>
<span id="cb183-9"><a href="estimators.html#cb183-9"></a><span class="co">#denominator.</span></span>
<span id="cb183-10"><a href="estimators.html#cb183-10"></a>re &lt;-<span class="st"> </span><span class="kw">mean</span>((ests.median <span class="op">-</span><span class="st"> </span>mu)<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">mean</span>((ests.mean <span class="op">-</span><span class="st"> </span>mu)<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb183-11"><a href="estimators.html#cb183-11"></a>re</span></code></pre></div>
<pre><code>## [1] 1.446713</code></pre>
<ol start="2" style="list-style-type: lower-alpha">
<li>Repeat for different sample sizes:</li>
</ol>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="estimators.html#cb185-1"></a>n &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">20</span>,<span class="dv">50</span>,<span class="dv">100</span>,<span class="dv">200</span>,<span class="dv">500</span>)</span>
<span id="cb185-2"><a href="estimators.html#cb185-2"></a>nsims &lt;-<span class="st"> </span><span class="dv">2000</span></span>
<span id="cb185-3"><a href="estimators.html#cb185-3"></a>mu &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb185-4"><a href="estimators.html#cb185-4"></a>sigma &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb185-5"><a href="estimators.html#cb185-5"></a>re &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="kw">length</span>(n))</span>
<span id="cb185-6"><a href="estimators.html#cb185-6"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(n)){</span>
<span id="cb185-7"><a href="estimators.html#cb185-7"></a>  x &lt;-<span class="st"> </span><span class="kw">mat.samps</span>(<span class="dt">n =</span> n[i], <span class="dt">nsim =</span> nsims, <span class="dt">mean =</span> mu, <span class="dt">sd =</span> sigma)</span>
<span id="cb185-8"><a href="estimators.html#cb185-8"></a>  ests.median &lt;-<span class="st"> </span><span class="kw">apply</span>(x, <span class="dv">1</span>, median)</span>
<span id="cb185-9"><a href="estimators.html#cb185-9"></a>  ests.mean &lt;-<span class="st"> </span><span class="kw">apply</span>(x, <span class="dv">1</span>, mean)</span>
<span id="cb185-10"><a href="estimators.html#cb185-10"></a>  re[i] &lt;-<span class="st"> </span><span class="kw">mean</span>((ests.median <span class="op">-</span><span class="st"> </span>mu)<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">mean</span>((ests.mean <span class="op">-</span><span class="st"> </span>mu)<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb185-11"><a href="estimators.html#cb185-11"></a>}</span>
<span id="cb185-12"><a href="estimators.html#cb185-12"></a></span>
<span id="cb185-13"><a href="estimators.html#cb185-13"></a><span class="kw">plot</span>(n, re, <span class="dt">xlab =</span> <span class="st">&quot;sample size&quot;</span>, </span>
<span id="cb185-14"><a href="estimators.html#cb185-14"></a>     <span class="dt">ylab =</span> <span class="st">&quot;Relative efficiency&quot;</span>, </span>
<span id="cb185-15"><a href="estimators.html#cb185-15"></a>     <span class="dt">main =</span> <span class="st">&quot;Sample mean vs. median for normal data&quot;</span>)</span></code></pre></div>
<p><img src="06_point-estimators_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<ol start="2" style="list-style-type: decimal">
<li>Repeat problem 1, but using the Laplace distribution.</li>
</ol>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="estimators.html#cb186-1"></a>n &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">20</span>,<span class="dv">50</span>,<span class="dv">100</span>,<span class="dv">200</span>,<span class="dv">500</span>)</span>
<span id="cb186-2"><a href="estimators.html#cb186-2"></a>nsims &lt;-<span class="st"> </span><span class="dv">2000</span></span>
<span id="cb186-3"><a href="estimators.html#cb186-3"></a>mu &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb186-4"><a href="estimators.html#cb186-4"></a>sigma &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb186-5"><a href="estimators.html#cb186-5"></a>re &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="kw">length</span>(n))</span>
<span id="cb186-6"><a href="estimators.html#cb186-6"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(n)){</span>
<span id="cb186-7"><a href="estimators.html#cb186-7"></a>  x &lt;-<span class="st"> </span><span class="kw">mat.samps</span>(<span class="dt">n =</span> n[i], <span class="dt">nsim =</span> nsims, <span class="dt">mu =</span> mu, <span class="dt">sigma =</span> sigma, <span class="dt">rx =</span> rlaplace)</span>
<span id="cb186-8"><a href="estimators.html#cb186-8"></a>  ests.median &lt;-<span class="st"> </span><span class="kw">apply</span>(x, <span class="dv">1</span>, median)</span>
<span id="cb186-9"><a href="estimators.html#cb186-9"></a>  ests.mean &lt;-<span class="st"> </span><span class="kw">apply</span>(x, <span class="dv">1</span>, mean)</span>
<span id="cb186-10"><a href="estimators.html#cb186-10"></a>  re[i] &lt;-<span class="st"> </span><span class="kw">mean</span>((ests.median <span class="op">-</span><span class="st"> </span>mu)<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">mean</span>((ests.mean <span class="op">-</span><span class="st"> </span>mu)<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb186-11"><a href="estimators.html#cb186-11"></a>}</span>
<span id="cb186-12"><a href="estimators.html#cb186-12"></a></span>
<span id="cb186-13"><a href="estimators.html#cb186-13"></a><span class="kw">plot</span>(n, re, <span class="dt">xlab =</span> <span class="st">&quot;sample size&quot;</span>, </span>
<span id="cb186-14"><a href="estimators.html#cb186-14"></a>     <span class="dt">ylab =</span> <span class="st">&quot;Relative efficiency&quot;</span>, </span>
<span id="cb186-15"><a href="estimators.html#cb186-15"></a>     <span class="dt">main =</span> <span class="st">&quot;Sample mean vs. median for heavy tailed data&quot;</span>)</span></code></pre></div>
<p><img src="06_point-estimators_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
</div>
<div id="statistical-decision-theory-and-risk" class="section level2">
<h2><span class="header-section-number">6.6</span> Statistical decision theory and risk</h2>
<p>Skimmed; skipped exercises.</p>
</div>
<div id="robustness" class="section level2">
<h2><span class="header-section-number">6.7</span> Robustness</h2>
<p>So far, we have implicitly assumed that we are confident in our model - for example, that a normal distribution represents the data generating process. But what if we were wrong? This may be the case, or at least, there are often competing data generating processes for the same set of observed data. In this reality, we value a statistical procedure that is <em>robust</em> - that is, it continues to give approximately correct answers even when the underlying assumptions are incorrect.</p>
<p><em>Robust</em> against what? This needs defining. For example, we might consider an estimator as robust against:</p>
<ul>
<li>outliers (e.g., the median)</li>
<li>misspecification of the probability distribution (e.g., the assumption of normality)</li>
</ul>
<div id="exercise-set-6-8" class="section level3">
<h3><span class="header-section-number">6.7.1</span> Exercise set 6-8</h3>
<ol style="list-style-type: decimal">
<li>Estiming <span class="math inline">\(\theta\)</span> using independent samples from a Normal(<span class="math inline">\(\theta\)</span>, 1) distribution. But our observations might be contaminated by data drawn from a different distribution.</li>
</ol>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="estimators.html#cb187-1"></a>rnorm.contam</span></code></pre></div>
<pre><code>## function (n, mu = 0, sigma = 1, contam.p = 0.01, contam.mu = -5, 
##     contam.sigma = 0) 
## {
##     ncontam &lt;- rbinom(1, n, contam.p)
##     c(rnorm(n - ncontam, mu, sigma), rnorm(ncontam, contam.mu, 
##         contam.sigma))
## }
## &lt;bytecode: 0x7fb67200c120&gt;
## &lt;environment: namespace:stfspack&gt;</code></pre>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="estimators.html#cb189-1"></a>dat &lt;-<span class="st"> </span><span class="kw">mat.samps</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="dt">nsim =</span> <span class="dv">1000</span>, <span class="dt">rx =</span> rnorm.contam, </span>
<span id="cb189-2"><a href="estimators.html#cb189-2"></a>                 <span class="dt">contam.p =</span> <span class="fl">0.1</span>, <span class="dt">contam.mu =</span> <span class="dv">300</span>, <span class="dt">contam.sigma =</span> <span class="dv">1</span>)</span>
<span id="cb189-3"><a href="estimators.html#cb189-3"></a>means &lt;-<span class="st"> </span><span class="kw">apply</span>(dat,<span class="dv">2</span>,mean)</span>
<span id="cb189-4"><a href="estimators.html#cb189-4"></a>medians &lt;-<span class="st"> </span><span class="kw">apply</span>(dat,<span class="dv">2</span>,median)</span>
<span id="cb189-5"><a href="estimators.html#cb189-5"></a><span class="kw">mean</span>(means)</span></code></pre></div>
<pre><code>## [1] 29.99532</code></pre>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="estimators.html#cb191-1"></a><span class="kw">var</span>(means)</span></code></pre></div>
<pre><code>## [1] 9.049853</code></pre>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="estimators.html#cb193-1"></a><span class="kw">mean</span>(medians)</span></code></pre></div>
<pre><code>## [1] 0.1431885</code></pre>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="estimators.html#cb195-1"></a><span class="kw">var</span>(medians)</span></code></pre></div>
<pre><code>## [1] 0.001828098</code></pre>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="estimators.html#cb197-1"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb197-2"><a href="estimators.html#cb197-2"></a><span class="kw">hist</span>(means, <span class="dt">breaks =</span> <span class="dv">10</span>)</span>
<span id="cb197-3"><a href="estimators.html#cb197-3"></a><span class="kw">hist</span>(medians, <span class="dt">breaks =</span> <span class="dv">10</span>)</span></code></pre></div>
<p><img src="06_point-estimators_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
</div>
<div id="estimators-for-simple-linear-regression" class="section level2">
<h2><span class="header-section-number">6.8</span> Estimators for simple linear regression</h2>
<div id="exercise-set-6-9" class="section level3">
<h3><span class="header-section-number">6.8.1</span> Exercise set 6-9</h3>
<ol style="list-style-type: decimal">
<li>Exploring the properties of least-squares and least-absolute-errors lines as estimators of a linear regression model with disturbance terms that are distributed normally (and constant variance).</li>
</ol>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="estimators.html#cb198-1"></a>sim.lm</span></code></pre></div>
<pre><code>## function (n, a, b, sigma.disturb = 1, mu.x = 8, sigma.x = 2, 
##     rdisturb = rnorm, rx = rnorm, het.coef = 0) 
## {
##     x &lt;- sort(rx(n, mu.x, sigma.x))
##     disturbs &lt;- rdisturb(n, 0, sapply(sigma.disturb + scale(x) * 
##         het.coef, max, 0))
##     y &lt;- a + b * x + disturbs
##     cbind(x, y)
## }
## &lt;bytecode: 0x7fb66bfd7cb0&gt;
## &lt;environment: namespace:stfspack&gt;</code></pre>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="estimators.html#cb200-1"></a>ests &lt;-<span class="st"> </span><span class="kw">sim.lm.ests</span>(<span class="dt">n =</span> <span class="dv">50</span>, <span class="dt">nsim =</span> <span class="dv">1000</span>, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> <span class="fl">0.5</span>)</span>
<span id="cb200-2"><a href="estimators.html#cb200-2"></a><span class="kw">colMeans</span>(ests)</span></code></pre></div>
<pre><code>## [1] 3.0015118 0.4992189</code></pre>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb202-1"><a href="estimators.html#cb202-1"></a><span class="kw">apply</span>(ests, <span class="dv">2</span>, var)</span></code></pre></div>
<pre><code>## [1] 0.39426172 0.00573011</code></pre>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb204-1"><a href="estimators.html#cb204-1"></a><span class="co"># Histogram of slope estimates</span></span>
<span id="cb204-2"><a href="estimators.html#cb204-2"></a><span class="kw">hist</span>(ests[,<span class="dv">2</span>])</span></code></pre></div>
<p><img src="06_point-estimators_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<ol style="list-style-type: lower-alpha">
<li>Simulate samples of 10, 50, 100, and 1000. Describe the bias and consistency of these least-squares estimators.</li>
</ol>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="estimators.html#cb205-1"></a>n_sims &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb205-2"><a href="estimators.html#cb205-2"></a>n_samps &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>, <span class="dv">1000</span>)</span>
<span id="cb205-3"><a href="estimators.html#cb205-3"></a>matrix_means &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="ot">NA</span>, <span class="dt">nrow =</span> <span class="kw">length</span>(n_samps), <span class="dt">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb205-4"><a href="estimators.html#cb205-4"></a>matrix_var &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="ot">NA</span>, <span class="dt">nrow =</span> <span class="kw">length</span>(n_samps), <span class="dt">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb205-5"><a href="estimators.html#cb205-5"></a></span>
<span id="cb205-6"><a href="estimators.html#cb205-6"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(n_samps)){</span>
<span id="cb205-7"><a href="estimators.html#cb205-7"></a>  ests &lt;-<span class="st"> </span><span class="kw">sim.lm.ests</span>(<span class="dt">n =</span> n_samps[i], <span class="dt">nsim =</span> n_sims, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> <span class="fl">0.5</span>)</span>
<span id="cb205-8"><a href="estimators.html#cb205-8"></a>  matrix_means[i, ] &lt;-<span class="st"> </span><span class="kw">colMeans</span>(ests)</span>
<span id="cb205-9"><a href="estimators.html#cb205-9"></a>  matrix_var[i, ] &lt;-<span class="st"> </span><span class="kw">apply</span>(ests, <span class="dv">2</span>, var)</span>
<span id="cb205-10"><a href="estimators.html#cb205-10"></a>}</span>
<span id="cb205-11"><a href="estimators.html#cb205-11"></a></span>
<span id="cb205-12"><a href="estimators.html#cb205-12"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb205-13"><a href="estimators.html#cb205-13"></a><span class="kw">plot</span>(n_samps, matrix_means[,<span class="dv">1</span>], <span class="dt">ylab =</span> <span class="st">&quot;a&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Mean&quot;</span>)</span>
<span id="cb205-14"><a href="estimators.html#cb205-14"></a><span class="kw">plot</span>(n_samps, matrix_means[,<span class="dv">2</span>], <span class="dt">ylab =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Mean&quot;</span>)</span>
<span id="cb205-15"><a href="estimators.html#cb205-15"></a><span class="kw">plot</span>(n_samps, matrix_var[,<span class="dv">1</span>], <span class="dt">ylab =</span> <span class="st">&quot;a&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Variance&quot;</span>)</span>
<span id="cb205-16"><a href="estimators.html#cb205-16"></a><span class="kw">plot</span>(n_samps, matrix_var[,<span class="dv">2</span>], <span class="dt">ylab =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Variance&quot;</span>)</span></code></pre></div>
<p><img src="06_point-estimators_files/figure-html/normal-LSE-1.png" width="672" /></p>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="estimators.html#cb206-1"></a>matrix_var_LSE &lt;-<span class="st"> </span>matrix_var</span></code></pre></div>
<p>There appears to be no bias, because the sample means of the coefficients (a and b) are very close to the true values (3 and 0.5).</p>
<p>The estimators are consistent, because the variance decreases to zero for both a and b (and there is no bias).</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Same exercise, but using quantile regression (i.e., the least-absolute-errors line).</li>
</ol>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="estimators.html#cb207-1"></a><span class="kw">library</span>(quantreg)</span>
<span id="cb207-2"><a href="estimators.html#cb207-2"></a>n_sims &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb207-3"><a href="estimators.html#cb207-3"></a>n_samps &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>, <span class="dv">1000</span>)</span>
<span id="cb207-4"><a href="estimators.html#cb207-4"></a>matrix_means &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="ot">NA</span>, <span class="dt">nrow =</span> <span class="kw">length</span>(n_samps), <span class="dt">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb207-5"><a href="estimators.html#cb207-5"></a>matrix_var &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="ot">NA</span>, <span class="dt">nrow =</span> <span class="kw">length</span>(n_samps), <span class="dt">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb207-6"><a href="estimators.html#cb207-6"></a></span>
<span id="cb207-7"><a href="estimators.html#cb207-7"></a><span class="kw">set.seed</span>(<span class="dv">1201</span>)</span>
<span id="cb207-8"><a href="estimators.html#cb207-8"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(n_samps)){</span>
<span id="cb207-9"><a href="estimators.html#cb207-9"></a>  ests &lt;-<span class="st"> </span><span class="kw">sim.lm.ests</span>(<span class="dt">n =</span> n_samps[i], <span class="dt">nsim =</span> n_sims, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> <span class="fl">0.5</span>, <span class="dt">estfun =</span> rq)</span>
<span id="cb207-10"><a href="estimators.html#cb207-10"></a>  matrix_means[i, ] &lt;-<span class="st"> </span><span class="kw">colMeans</span>(ests)</span>
<span id="cb207-11"><a href="estimators.html#cb207-11"></a>  matrix_var[i, ] &lt;-<span class="st"> </span><span class="kw">apply</span>(ests, <span class="dv">2</span>, var)</span>
<span id="cb207-12"><a href="estimators.html#cb207-12"></a>}</span>
<span id="cb207-13"><a href="estimators.html#cb207-13"></a></span>
<span id="cb207-14"><a href="estimators.html#cb207-14"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb207-15"><a href="estimators.html#cb207-15"></a><span class="kw">plot</span>(n_samps, matrix_means[,<span class="dv">1</span>], <span class="dt">ylab =</span> <span class="st">&quot;a&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Mean&quot;</span>)</span>
<span id="cb207-16"><a href="estimators.html#cb207-16"></a><span class="kw">plot</span>(n_samps, matrix_means[,<span class="dv">2</span>], <span class="dt">ylab =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Mean&quot;</span>)</span>
<span id="cb207-17"><a href="estimators.html#cb207-17"></a><span class="kw">plot</span>(n_samps, matrix_var[,<span class="dv">1</span>], <span class="dt">ylab =</span> <span class="st">&quot;a&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Variance&quot;</span>)</span>
<span id="cb207-18"><a href="estimators.html#cb207-18"></a><span class="kw">plot</span>(n_samps, matrix_var[,<span class="dv">2</span>], <span class="dt">ylab =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Variance&quot;</span>)</span></code></pre></div>
<p><img src="06_point-estimators_files/figure-html/normal-LAE-1.png" width="672" /></p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="estimators.html#cb208-1"></a>matrix_var_LAE &lt;-<span class="st"> </span>matrix_var</span></code></pre></div>
<p>In some simulations, the least-absolute-errors line appears to be biased at very low sample sizes (e.g., 10). But this is just sampling variation, and I set the seed above to illustrate that there is no bias and this estimator is also consistent.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Which estimator is more efficient?</li>
</ol>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="estimators.html#cb209-1"></a>n_sims &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb209-2"><a href="estimators.html#cb209-2"></a>n_samps &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>, <span class="dv">1000</span>)</span>
<span id="cb209-3"><a href="estimators.html#cb209-3"></a>matrix_var &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="ot">NA</span>, <span class="dt">nrow =</span> <span class="kw">length</span>(n_samps), <span class="dt">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb209-4"><a href="estimators.html#cb209-4"></a></span>
<span id="cb209-5"><a href="estimators.html#cb209-5"></a><span class="kw">set.seed</span>(<span class="dv">1201</span>)</span>
<span id="cb209-6"><a href="estimators.html#cb209-6"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(n_samps)){</span>
<span id="cb209-7"><a href="estimators.html#cb209-7"></a>  ests &lt;-<span class="st"> </span><span class="kw">sim.lm.ests</span>(<span class="dt">n =</span> n_samps[i], <span class="dt">nsim =</span> n_sims, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> <span class="fl">0.5</span>)</span>
<span id="cb209-8"><a href="estimators.html#cb209-8"></a>  matrix_var[i, ] &lt;-<span class="st"> </span><span class="kw">apply</span>(ests, <span class="dv">2</span>, var)</span>
<span id="cb209-9"><a href="estimators.html#cb209-9"></a>}</span>
<span id="cb209-10"><a href="estimators.html#cb209-10"></a>matrix_var_LSE &lt;-<span class="st"> </span>matrix_var</span>
<span id="cb209-11"><a href="estimators.html#cb209-11"></a></span>
<span id="cb209-12"><a href="estimators.html#cb209-12"></a><span class="kw">set.seed</span>(<span class="dv">1201</span>)</span>
<span id="cb209-13"><a href="estimators.html#cb209-13"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(n_samps)){</span>
<span id="cb209-14"><a href="estimators.html#cb209-14"></a>  ests &lt;-<span class="st"> </span><span class="kw">sim.lm.ests</span>(<span class="dt">n =</span> n_samps[i], <span class="dt">nsim =</span> n_sims, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> <span class="fl">0.5</span>, <span class="dt">estfun =</span> rq)</span>
<span id="cb209-15"><a href="estimators.html#cb209-15"></a>  matrix_var[i, ] &lt;-<span class="st"> </span><span class="kw">apply</span>(ests, <span class="dv">2</span>, var)</span>
<span id="cb209-16"><a href="estimators.html#cb209-16"></a>}</span>
<span id="cb209-17"><a href="estimators.html#cb209-17"></a>matrix_var_LAE &lt;-<span class="st"> </span>matrix_var</span>
<span id="cb209-18"><a href="estimators.html#cb209-18"></a></span>
<span id="cb209-19"><a href="estimators.html#cb209-19"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb209-20"><a href="estimators.html#cb209-20"></a><span class="kw">plot</span>(n_samps, matrix_var_LSE[,<span class="dv">1</span>], <span class="dt">ylab =</span> <span class="st">&quot;a&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Variance (least-squares)&quot;</span>)</span>
<span id="cb209-21"><a href="estimators.html#cb209-21"></a><span class="kw">plot</span>(n_samps, matrix_var_LSE[,<span class="dv">2</span>], <span class="dt">ylab =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Variance (least-squares)&quot;</span>)</span>
<span id="cb209-22"><a href="estimators.html#cb209-22"></a><span class="kw">plot</span>(n_samps, matrix_var_LAE[,<span class="dv">1</span>], <span class="dt">ylab =</span> <span class="st">&quot;a&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Variance (least-absolute)&quot;</span>)</span>
<span id="cb209-23"><a href="estimators.html#cb209-23"></a><span class="kw">plot</span>(n_samps, matrix_var_LAE[,<span class="dv">2</span>], <span class="dt">ylab =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Variance (least-absolute)&quot;</span>)</span></code></pre></div>
<p><img src="06_point-estimators_files/figure-html/normal-LSE-LAE-1.png" width="672" /></p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="estimators.html#cb210-1"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb210-2"><a href="estimators.html#cb210-2"></a><span class="kw">plot</span>(n_samps, matrix_var_LSE[,<span class="dv">1</span>] <span class="op">/</span><span class="st"> </span>matrix_var_LAE[,<span class="dv">1</span>], <span class="dt">ylab =</span> <span class="st">&quot;a&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;RE (least-squares / least-absolute)&quot;</span>)</span>
<span id="cb210-3"><a href="estimators.html#cb210-3"></a><span class="kw">plot</span>(n_samps, matrix_var_LSE[,<span class="dv">2</span>] <span class="op">/</span><span class="st"> </span>matrix_var_LAE[,<span class="dv">2</span>], <span class="dt">ylab =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;RE (least-squares / least-absolute)&quot;</span>)</span></code></pre></div>
<p><img src="06_point-estimators_files/figure-html/normal-LSE-LAE-2.png" width="672" /></p>
<p>The least-squares estimator is more efficient; that is, it has a lower variance than the least-absolute estimator.</p>
<ol start="2" style="list-style-type: decimal">
<li>Use a heavy tailed distribution (Laplace) for the disturbance term.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Simulate some datasets and view:</li>
</ol>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="estimators.html#cb211-1"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb211-2"><a href="estimators.html#cb211-2"></a><span class="kw">plot</span>(<span class="kw">sim.lm</span>(<span class="dt">n =</span> <span class="dv">50</span>, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> <span class="fl">0.5</span>, <span class="dt">rdisturb =</span> rlaplace))</span>
<span id="cb211-3"><a href="estimators.html#cb211-3"></a><span class="kw">plot</span>(<span class="kw">sim.lm</span>(<span class="dt">n =</span> <span class="dv">50</span>, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> <span class="fl">0.5</span>, <span class="dt">rdisturb =</span> rlaplace))</span>
<span id="cb211-4"><a href="estimators.html#cb211-4"></a><span class="kw">plot</span>(<span class="kw">sim.lm</span>(<span class="dt">n =</span> <span class="dv">50</span>, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> <span class="fl">0.5</span>, <span class="dt">rdisturb =</span> rlaplace))</span>
<span id="cb211-5"><a href="estimators.html#cb211-5"></a><span class="kw">plot</span>(<span class="kw">sim.lm</span>(<span class="dt">n =</span> <span class="dv">50</span>, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> <span class="fl">0.5</span>, <span class="dt">rdisturb =</span> rlaplace))</span></code></pre></div>
<p><img src="06_point-estimators_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Repeat problem 1 with a laplace distribution.</li>
</ol>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="estimators.html#cb212-1"></a>n_sims &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb212-2"><a href="estimators.html#cb212-2"></a>n_samps &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>, <span class="dv">1000</span>)</span>
<span id="cb212-3"><a href="estimators.html#cb212-3"></a></span>
<span id="cb212-4"><a href="estimators.html#cb212-4"></a>matrix_means &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="ot">NA</span>, <span class="dt">nrow =</span> <span class="kw">length</span>(n_samps), <span class="dt">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb212-5"><a href="estimators.html#cb212-5"></a>matrix_var &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="ot">NA</span>, <span class="dt">nrow =</span> <span class="kw">length</span>(n_samps), <span class="dt">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb212-6"><a href="estimators.html#cb212-6"></a><span class="kw">set.seed</span>(<span class="dv">1201</span>)</span>
<span id="cb212-7"><a href="estimators.html#cb212-7"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(n_samps)){</span>
<span id="cb212-8"><a href="estimators.html#cb212-8"></a>  ests &lt;-<span class="st"> </span><span class="kw">sim.lm.ests</span>(<span class="dt">n =</span> n_samps[i], <span class="dt">nsim =</span> n_sims, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> <span class="fl">0.5</span>, <span class="dt">rdisturb =</span> rlaplace)</span>
<span id="cb212-9"><a href="estimators.html#cb212-9"></a>  matrix_means[i, ] &lt;-<span class="st"> </span><span class="kw">colMeans</span>(ests)</span>
<span id="cb212-10"><a href="estimators.html#cb212-10"></a>  matrix_var[i, ] &lt;-<span class="st"> </span><span class="kw">apply</span>(ests, <span class="dv">2</span>, var)</span>
<span id="cb212-11"><a href="estimators.html#cb212-11"></a>}</span>
<span id="cb212-12"><a href="estimators.html#cb212-12"></a>matrix_means_LSE &lt;-<span class="st"> </span>matrix_means</span>
<span id="cb212-13"><a href="estimators.html#cb212-13"></a>matrix_var_LSE &lt;-<span class="st"> </span>matrix_var</span>
<span id="cb212-14"><a href="estimators.html#cb212-14"></a></span>
<span id="cb212-15"><a href="estimators.html#cb212-15"></a>matrix_means &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="ot">NA</span>, <span class="dt">nrow =</span> <span class="kw">length</span>(n_samps), <span class="dt">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb212-16"><a href="estimators.html#cb212-16"></a>matrix_var &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="ot">NA</span>, <span class="dt">nrow =</span> <span class="kw">length</span>(n_samps), <span class="dt">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb212-17"><a href="estimators.html#cb212-17"></a><span class="kw">set.seed</span>(<span class="dv">1201</span>)</span>
<span id="cb212-18"><a href="estimators.html#cb212-18"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(n_samps)){</span>
<span id="cb212-19"><a href="estimators.html#cb212-19"></a>  ests &lt;-<span class="st"> </span><span class="kw">sim.lm.ests</span>(<span class="dt">n =</span> n_samps[i], <span class="dt">nsim =</span> n_sims, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> <span class="fl">0.5</span>, <span class="dt">estfun =</span> rq, <span class="dt">rdisturb =</span> rlaplace)</span>
<span id="cb212-20"><a href="estimators.html#cb212-20"></a>  matrix_means[i, ] &lt;-<span class="st"> </span><span class="kw">colMeans</span>(ests)</span>
<span id="cb212-21"><a href="estimators.html#cb212-21"></a>  matrix_var[i, ] &lt;-<span class="st"> </span><span class="kw">apply</span>(ests, <span class="dv">2</span>, var)</span>
<span id="cb212-22"><a href="estimators.html#cb212-22"></a>}</span>
<span id="cb212-23"><a href="estimators.html#cb212-23"></a>matrix_means_LAE &lt;-<span class="st"> </span>matrix_means</span>
<span id="cb212-24"><a href="estimators.html#cb212-24"></a>matrix_var_LAE &lt;-<span class="st"> </span>matrix_var</span>
<span id="cb212-25"><a href="estimators.html#cb212-25"></a></span>
<span id="cb212-26"><a href="estimators.html#cb212-26"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb212-27"><a href="estimators.html#cb212-27"></a><span class="kw">plot</span>(n_samps, matrix_means_LSE[,<span class="dv">1</span>], <span class="dt">ylab =</span> <span class="st">&quot;a&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Mean (least-squares)&quot;</span>)</span>
<span id="cb212-28"><a href="estimators.html#cb212-28"></a><span class="kw">plot</span>(n_samps, matrix_means_LSE[,<span class="dv">2</span>], <span class="dt">ylab =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Mean (least-squares)&quot;</span>)</span>
<span id="cb212-29"><a href="estimators.html#cb212-29"></a><span class="kw">plot</span>(n_samps, matrix_means_LAE[,<span class="dv">1</span>], <span class="dt">ylab =</span> <span class="st">&quot;a&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Mean (least-absolute)&quot;</span>)</span>
<span id="cb212-30"><a href="estimators.html#cb212-30"></a><span class="kw">plot</span>(n_samps, matrix_means_LAE[,<span class="dv">2</span>], <span class="dt">ylab =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Mean (least-absolute)&quot;</span>)</span></code></pre></div>
<p><img src="06_point-estimators_files/figure-html/laplace-1.png" width="672" /></p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="estimators.html#cb213-1"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb213-2"><a href="estimators.html#cb213-2"></a><span class="kw">plot</span>(n_samps, matrix_var_LSE[,<span class="dv">1</span>], <span class="dt">ylab =</span> <span class="st">&quot;a&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Variance (least-squares)&quot;</span>)</span>
<span id="cb213-3"><a href="estimators.html#cb213-3"></a><span class="kw">plot</span>(n_samps, matrix_var_LSE[,<span class="dv">2</span>], <span class="dt">ylab =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Variance (least-squares)&quot;</span>)</span>
<span id="cb213-4"><a href="estimators.html#cb213-4"></a><span class="kw">plot</span>(n_samps, matrix_var_LAE[,<span class="dv">1</span>], <span class="dt">ylab =</span> <span class="st">&quot;a&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Variance (least-absolute)&quot;</span>)</span>
<span id="cb213-5"><a href="estimators.html#cb213-5"></a><span class="kw">plot</span>(n_samps, matrix_var_LAE[,<span class="dv">2</span>], <span class="dt">ylab =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Variance (least-absolute)&quot;</span>)</span></code></pre></div>
<p><img src="06_point-estimators_files/figure-html/laplace-2.png" width="672" /></p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="estimators.html#cb214-1"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb214-2"><a href="estimators.html#cb214-2"></a><span class="kw">plot</span>(n_samps, matrix_var_LSE[,<span class="dv">1</span>] <span class="op">/</span><span class="st"> </span>matrix_var_LAE[,<span class="dv">1</span>], <span class="dt">ylab =</span> <span class="st">&quot;a&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;RE (least-squares / least-absolute)&quot;</span>)</span>
<span id="cb214-3"><a href="estimators.html#cb214-3"></a><span class="kw">plot</span>(n_samps, matrix_var_LSE[,<span class="dv">2</span>] <span class="op">/</span><span class="st"> </span>matrix_var_LAE[,<span class="dv">2</span>], <span class="dt">ylab =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;RE (least-squares / least-absolute)&quot;</span>)</span></code></pre></div>
<p><img src="06_point-estimators_files/figure-html/laplace-3.png" width="672" /></p>
<p>Both estimators are unbiased when using a heavy tailed distribution for the error term. However, the least-absolute estimator is more efficient than the the least-squares estimator, especially as sample size increases.</p>
<ol start="3" style="list-style-type: decimal">
<li>Examining robustness to data contamination.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Simulate some datasets and view:</li>
</ol>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="estimators.html#cb215-1"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb215-2"><a href="estimators.html#cb215-2"></a><span class="kw">plot</span>(<span class="kw">sim.lm</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> <span class="fl">0.5</span>, <span class="dt">rdisturb =</span> rnorm.contam))</span>
<span id="cb215-3"><a href="estimators.html#cb215-3"></a><span class="kw">plot</span>(<span class="kw">sim.lm</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> <span class="fl">0.5</span>, <span class="dt">rdisturb =</span> rnorm.contam))</span>
<span id="cb215-4"><a href="estimators.html#cb215-4"></a><span class="kw">plot</span>(<span class="kw">sim.lm</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> <span class="fl">0.5</span>, <span class="dt">rdisturb =</span> rnorm.contam))</span>
<span id="cb215-5"><a href="estimators.html#cb215-5"></a><span class="kw">plot</span>(<span class="kw">sim.lm</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> <span class="fl">0.5</span>, <span class="dt">rdisturb =</span> rnorm.contam))</span></code></pre></div>
<p><img src="06_point-estimators_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Repeat problem 1 with 1% contamination.</li>
</ol>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="estimators.html#cb216-1"></a>n_sims &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb216-2"><a href="estimators.html#cb216-2"></a>n_samps &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>, <span class="dv">1000</span>)</span>
<span id="cb216-3"><a href="estimators.html#cb216-3"></a></span>
<span id="cb216-4"><a href="estimators.html#cb216-4"></a>matrix_means &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="ot">NA</span>, <span class="dt">nrow =</span> <span class="kw">length</span>(n_samps), <span class="dt">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb216-5"><a href="estimators.html#cb216-5"></a>matrix_var &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="ot">NA</span>, <span class="dt">nrow =</span> <span class="kw">length</span>(n_samps), <span class="dt">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb216-6"><a href="estimators.html#cb216-6"></a><span class="kw">set.seed</span>(<span class="dv">1201</span>)</span>
<span id="cb216-7"><a href="estimators.html#cb216-7"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(n_samps)){</span>
<span id="cb216-8"><a href="estimators.html#cb216-8"></a>  ests &lt;-<span class="st"> </span><span class="kw">sim.lm.ests</span>(<span class="dt">n =</span> n_samps[i], <span class="dt">nsim =</span> n_sims, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> <span class="fl">0.5</span>, <span class="dt">rdisturb =</span> rnorm.contam)</span>
<span id="cb216-9"><a href="estimators.html#cb216-9"></a>  matrix_means[i, ] &lt;-<span class="st"> </span><span class="kw">colMeans</span>(ests)</span>
<span id="cb216-10"><a href="estimators.html#cb216-10"></a>  matrix_var[i, ] &lt;-<span class="st"> </span><span class="kw">apply</span>(ests, <span class="dv">2</span>, var)</span>
<span id="cb216-11"><a href="estimators.html#cb216-11"></a>}</span>
<span id="cb216-12"><a href="estimators.html#cb216-12"></a>matrix_means_LSE &lt;-<span class="st"> </span>matrix_means</span>
<span id="cb216-13"><a href="estimators.html#cb216-13"></a>matrix_var_LSE &lt;-<span class="st"> </span>matrix_var</span>
<span id="cb216-14"><a href="estimators.html#cb216-14"></a></span>
<span id="cb216-15"><a href="estimators.html#cb216-15"></a>matrix_means &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="ot">NA</span>, <span class="dt">nrow =</span> <span class="kw">length</span>(n_samps), <span class="dt">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb216-16"><a href="estimators.html#cb216-16"></a>matrix_var &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="ot">NA</span>, <span class="dt">nrow =</span> <span class="kw">length</span>(n_samps), <span class="dt">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb216-17"><a href="estimators.html#cb216-17"></a><span class="kw">set.seed</span>(<span class="dv">1201</span>)</span>
<span id="cb216-18"><a href="estimators.html#cb216-18"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(n_samps)){</span>
<span id="cb216-19"><a href="estimators.html#cb216-19"></a>  ests &lt;-<span class="st"> </span><span class="kw">sim.lm.ests</span>(<span class="dt">n =</span> n_samps[i], <span class="dt">nsim =</span> n_sims, <span class="dt">a =</span> <span class="dv">3</span>, <span class="dt">b =</span> <span class="fl">0.5</span>, <span class="dt">estfun =</span> rq, <span class="dt">rdisturb =</span> rnorm.contam)</span>
<span id="cb216-20"><a href="estimators.html#cb216-20"></a>  matrix_means[i, ] &lt;-<span class="st"> </span><span class="kw">colMeans</span>(ests)</span>
<span id="cb216-21"><a href="estimators.html#cb216-21"></a>  matrix_var[i, ] &lt;-<span class="st"> </span><span class="kw">apply</span>(ests, <span class="dv">2</span>, var)</span>
<span id="cb216-22"><a href="estimators.html#cb216-22"></a>}</span>
<span id="cb216-23"><a href="estimators.html#cb216-23"></a>matrix_means_LAE &lt;-<span class="st"> </span>matrix_means</span>
<span id="cb216-24"><a href="estimators.html#cb216-24"></a>matrix_var_LAE &lt;-<span class="st"> </span>matrix_var</span>
<span id="cb216-25"><a href="estimators.html#cb216-25"></a></span>
<span id="cb216-26"><a href="estimators.html#cb216-26"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb216-27"><a href="estimators.html#cb216-27"></a><span class="kw">plot</span>(n_samps, matrix_means_LSE[,<span class="dv">1</span>], <span class="dt">ylab =</span> <span class="st">&quot;a&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Mean (least-squares)&quot;</span>)</span>
<span id="cb216-28"><a href="estimators.html#cb216-28"></a><span class="kw">plot</span>(n_samps, matrix_means_LSE[,<span class="dv">2</span>], <span class="dt">ylab =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Mean (least-squares)&quot;</span>)</span>
<span id="cb216-29"><a href="estimators.html#cb216-29"></a><span class="kw">plot</span>(n_samps, matrix_means_LAE[,<span class="dv">1</span>], <span class="dt">ylab =</span> <span class="st">&quot;a&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Mean (least-absolute)&quot;</span>)</span>
<span id="cb216-30"><a href="estimators.html#cb216-30"></a><span class="kw">plot</span>(n_samps, matrix_means_LAE[,<span class="dv">2</span>], <span class="dt">ylab =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Mean (least-absolute)&quot;</span>)</span></code></pre></div>
<p><img src="06_point-estimators_files/figure-html/contam-1.png" width="672" /></p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="estimators.html#cb217-1"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb217-2"><a href="estimators.html#cb217-2"></a><span class="kw">plot</span>(n_samps, matrix_var_LSE[,<span class="dv">1</span>], <span class="dt">ylab =</span> <span class="st">&quot;a&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Variance (least-squares)&quot;</span>)</span>
<span id="cb217-3"><a href="estimators.html#cb217-3"></a><span class="kw">plot</span>(n_samps, matrix_var_LSE[,<span class="dv">2</span>], <span class="dt">ylab =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Variance (least-squares)&quot;</span>)</span>
<span id="cb217-4"><a href="estimators.html#cb217-4"></a><span class="kw">plot</span>(n_samps, matrix_var_LAE[,<span class="dv">1</span>], <span class="dt">ylab =</span> <span class="st">&quot;a&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Variance (least-absolute)&quot;</span>)</span>
<span id="cb217-5"><a href="estimators.html#cb217-5"></a><span class="kw">plot</span>(n_samps, matrix_var_LAE[,<span class="dv">2</span>], <span class="dt">ylab =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Variance (least-absolute)&quot;</span>)</span></code></pre></div>
<p><img src="06_point-estimators_files/figure-html/contam-2.png" width="672" /></p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="estimators.html#cb218-1"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb218-2"><a href="estimators.html#cb218-2"></a><span class="kw">plot</span>(n_samps, matrix_var_LSE[,<span class="dv">1</span>] <span class="op">/</span><span class="st"> </span>matrix_var_LAE[,<span class="dv">1</span>], <span class="dt">ylab =</span> <span class="st">&quot;a&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;RE (least-squares / least-absolute)&quot;</span>)</span>
<span id="cb218-3"><a href="estimators.html#cb218-3"></a><span class="kw">plot</span>(n_samps, matrix_var_LSE[,<span class="dv">2</span>] <span class="op">/</span><span class="st"> </span>matrix_var_LAE[,<span class="dv">2</span>], <span class="dt">ylab =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;RE (least-squares / least-absolute)&quot;</span>)</span></code></pre></div>
<p><img src="06_point-estimators_files/figure-html/contam-3.png" width="672" /></p>
<p>Looks like both estimators are biases (a is too high, b is too low); but the least-absolute estimator appears less biased. Even though the variance of both estimators declines with sample size, due to bias, neither of them are consistent. Their relative efficiecy follows a similar pattern to problem 2 - the least-absolute is more efficient, because it is less influence by outliers.</p>
<ol start="4" style="list-style-type: decimal">
<li>Omitted variable bias.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li><p>If X and Z are positively correlated, then if our model ignores Z, our estimate of the slope will overestimate the true effect of X - because our model wrongly attributed the effect of Z to X. If X and Z are negatively correlated, then the opposite is true - we have underestimated the slope, because the effect of Z is negating the effect of X. Finally, if X and Z are independent, then our estimate of the slope will be the same.</p></li>
<li><p>Test your conjectures.</p></li>
</ol>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="estimators.html#cb219-1"></a><span class="kw">library</span>(MASS)</span>
<span id="cb219-2"><a href="estimators.html#cb219-2"></a>sim<span class="fl">.2</span>var</span></code></pre></div>
<pre><code>## function (n, nsim, a, b1, b2 = 0, sigma.disturb = 1, correl = 0) 
## {
##     sig &lt;- matrix(c(1, correl, correl, 1), nrow = 2)
##     ivs &lt;- MASS::mvrnorm(n * nsim, mu = c(0, 0), sig)
##     x &lt;- ivs[, 1]
##     z &lt;- ivs[, 2]
##     disturb &lt;- rnorm(n * nsim, 0, sigma.disturb)
##     y &lt;- a + b1 * x + b2 * z + disturb
##     xmat &lt;- matrix(x, nrow = nsim)
##     ymat &lt;- matrix(y, nrow = nsim)
##     list(xmat, ymat)
## }
## &lt;bytecode: 0x7fb66dcff6c8&gt;
## &lt;environment: namespace:stfspack&gt;</code></pre>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="estimators.html#cb221-1"></a>n &lt;-<span class="st"> </span><span class="dv">50</span></span>
<span id="cb221-2"><a href="estimators.html#cb221-2"></a>nsims &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb221-3"><a href="estimators.html#cb221-3"></a>beta &lt;-<span class="st"> </span><span class="fl">.3</span></span>
<span id="cb221-4"><a href="estimators.html#cb221-4"></a>gamma &lt;-<span class="st"> </span><span class="fl">.4</span></span>
<span id="cb221-5"><a href="estimators.html#cb221-5"></a></span>
<span id="cb221-6"><a href="estimators.html#cb221-6"></a><span class="co"># Positive correlation</span></span>
<span id="cb221-7"><a href="estimators.html#cb221-7"></a>rho &lt;-<span class="st"> </span><span class="fl">.5</span></span>
<span id="cb221-8"><a href="estimators.html#cb221-8"></a>dat &lt;-<span class="st"> </span><span class="kw">sim.2var</span>(n, nsims, <span class="dv">0</span>, beta, gamma, <span class="dv">1</span>, rho)  </span>
<span id="cb221-9"><a href="estimators.html#cb221-9"></a>ests &lt;-<span class="st"> </span><span class="kw">numeric</span>(nsims)</span>
<span id="cb221-10"><a href="estimators.html#cb221-10"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nsims){</span>
<span id="cb221-11"><a href="estimators.html#cb221-11"></a>  ests[i] &lt;-<span class="st"> </span><span class="kw">lm</span>(dat[[<span class="dv">2</span>]][i,] <span class="op">~</span><span class="st"> </span>dat[[<span class="dv">1</span>]][i,])<span class="op">$</span>coef[<span class="dv">2</span>] }</span>
<span id="cb221-12"><a href="estimators.html#cb221-12"></a>ests_rho_positive &lt;-<span class="st"> </span>ests</span>
<span id="cb221-13"><a href="estimators.html#cb221-13"></a></span>
<span id="cb221-14"><a href="estimators.html#cb221-14"></a><span class="co"># Negative correlation</span></span>
<span id="cb221-15"><a href="estimators.html#cb221-15"></a>rho &lt;-<span class="st"> </span><span class="fl">-0.5</span></span>
<span id="cb221-16"><a href="estimators.html#cb221-16"></a>dat &lt;-<span class="st"> </span><span class="kw">sim.2var</span>(n, nsims, <span class="dv">0</span>, beta, gamma, <span class="dv">1</span>, rho)  </span>
<span id="cb221-17"><a href="estimators.html#cb221-17"></a>ests &lt;-<span class="st"> </span><span class="kw">numeric</span>(nsims)</span>
<span id="cb221-18"><a href="estimators.html#cb221-18"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nsims){</span>
<span id="cb221-19"><a href="estimators.html#cb221-19"></a>  ests[i] &lt;-<span class="st"> </span><span class="kw">lm</span>(dat[[<span class="dv">2</span>]][i,] <span class="op">~</span><span class="st"> </span>dat[[<span class="dv">1</span>]][i,])<span class="op">$</span>coef[<span class="dv">2</span>] }</span>
<span id="cb221-20"><a href="estimators.html#cb221-20"></a>ests_rho_negative &lt;-<span class="st"> </span>ests</span>
<span id="cb221-21"><a href="estimators.html#cb221-21"></a></span>
<span id="cb221-22"><a href="estimators.html#cb221-22"></a><span class="co"># No correlation</span></span>
<span id="cb221-23"><a href="estimators.html#cb221-23"></a>rho &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb221-24"><a href="estimators.html#cb221-24"></a>dat &lt;-<span class="st"> </span><span class="kw">sim.2var</span>(n, nsims, <span class="dv">0</span>, beta, gamma, <span class="dv">1</span>, rho)  </span>
<span id="cb221-25"><a href="estimators.html#cb221-25"></a>ests &lt;-<span class="st"> </span><span class="kw">numeric</span>(nsims)</span>
<span id="cb221-26"><a href="estimators.html#cb221-26"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nsims){</span>
<span id="cb221-27"><a href="estimators.html#cb221-27"></a>  ests[i] &lt;-<span class="st"> </span><span class="kw">lm</span>(dat[[<span class="dv">2</span>]][i,] <span class="op">~</span><span class="st"> </span>dat[[<span class="dv">1</span>]][i,])<span class="op">$</span>coef[<span class="dv">2</span>] }</span>
<span id="cb221-28"><a href="estimators.html#cb221-28"></a>ests_rho_none &lt;-<span class="st"> </span>ests</span></code></pre></div>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="estimators.html#cb222-1"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb222-2"><a href="estimators.html#cb222-2"></a></span>
<span id="cb222-3"><a href="estimators.html#cb222-3"></a><span class="kw">hist</span>(ests_rho_negative, <span class="dt">main =</span> <span class="st">&quot;Negative correlation&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;Estimates of b&quot;</span>)</span>
<span id="cb222-4"><a href="estimators.html#cb222-4"></a><span class="kw">abline</span>(<span class="dt">v =</span> beta, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb222-5"><a href="estimators.html#cb222-5"></a></span>
<span id="cb222-6"><a href="estimators.html#cb222-6"></a><span class="kw">hist</span>(ests_rho_none, <span class="dt">main =</span> <span class="st">&quot;No correlation&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;Estimates of b&quot;</span>)</span>
<span id="cb222-7"><a href="estimators.html#cb222-7"></a><span class="kw">abline</span>(<span class="dt">v =</span> beta, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb222-8"><a href="estimators.html#cb222-8"></a></span>
<span id="cb222-9"><a href="estimators.html#cb222-9"></a><span class="kw">hist</span>(ests_rho_positive, <span class="dt">main =</span> <span class="st">&quot;Positive correlation&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;Estimates of b&quot;</span>)</span>
<span id="cb222-10"><a href="estimators.html#cb222-10"></a><span class="kw">abline</span>(<span class="dt">v =</span> beta, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="06_point-estimators_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<ol start="3" style="list-style-type: lower-alpha">
<li>If we are ignoring the effect of Z (a potential confound), then it is unreasonable to use our estimate of the slope to make predictions of Y based on X alone, if X and Z are correlated.</li>
</ol>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="randomvars.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="intervals.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
