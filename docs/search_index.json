[
["index.html", "Notes on Statistical Thinking from Scratch Preface", " Notes on Statistical Thinking from Scratch Robin Elahi 2020-09-14 Preface This project is a set of notes for Statistical Thinking from Scratch, by M.D. Edge. The goal is to become more comfortable with the nitty gritty underlying the statistical tools I commonly use - essentially, regression. Edge takes a unique approach in that he takes a small dataset, and dissects regression ‘from scratch’. Each of these bookdown chapters corresponds to each of Edge’s 10 book chapters. The impetus for this bookdown project was the cancellation of my spring 2020 course Experimental Design and Probability due to COVID-19. So, I’ll be filling in some gaps in my stats toolkit and learning how to use bookdown. The stfs package can be installed from Github: install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;mdedge/stfspack&quot;) library(stfspack) As Edge points out in his Prelude, the typical introductory course in biological statistics (including my own) revolves around learning a series of tests: t-tests, regression, ANOVA . In STFS, the focus instead is on one procedure - simple linear regression - and takes ‘little for granted’ (translation: we’ll be working through the math, lightly). The primary dataset consists of 11 observations. To heck with big data. So in this project, you will find mostly code - I’ll leave the exposition to Edge, who does a nice job explaining the concepts in his textbook. I’ll chime in whenever it seems useful. The chapters will work through the following: 1. Probability 2. Estimation (using data to guess the values that describe a data generating process) 3. Inference (testing hypotheses about the processes that might have generated an observed dataset) How to publish a bookdown project on github https://community.rstudio.com/t/hosting-bookdown-in-github/20427 http://seankross.com/2016/11/17/How-to-Start-a-Bookdown-Book.html "],
["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction library(stfspack) library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) Figure 1.1 shows data on the amount of fertilizer applied to cropland (x-axis), and the cereal yields (y-axis), for each of 11 countries in Africa. There is a positive relationship, but is it strong? Is it weak? How are we to reason about these data? anscombe %&gt;% ggplot(aes(x1, y1)) + geom_point() + labs(x = &quot;Fertilizer consumption (kg/hectare)&quot;, y = &quot;Cereal yield (100kg/hectare)&quot;) Figure 1.1: Fertilizer consumption and cereal yield in 11 sub-Saharan African countries Statistics, that’s how! Statistics allows us to reason from data, and rests on a mathematical framework. It is worth understanding, even minimally, this framework. That’s why we are reading this book. Chapter 1 provides an overview of simple linear regression, which allows us to identify a line that ‘best’ fits the data. We can use the lm() function in R to fit a simple linear regression: mod.fit &lt;- lm(y1 ~ x1, data = anscombe) anscombe %&gt;% ggplot(aes(x1, y1)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(x = &quot;Fertilizer consumption (kg/hectare)&quot;, y = &quot;Cereal yield (100kg/hectare)&quot;) Figure 1.2: The agriculture data with a line of best fit from the simple linear regression summary(mod.fit) ## ## Call: ## lm(formula = y1 ~ x1, data = anscombe) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.92127 -0.45577 -0.04136 0.70941 1.83882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0001 1.1247 2.667 0.02573 * ## x1 0.5001 0.1179 4.241 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 9 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.6295 ## F-statistic: 17.99 on 1 and 9 DF, p-value: 0.00217 What do the following columns in the ‘Coefficients’ table refer to? the Estimates: (Intercept) and x1 Std. Error Pr(&gt;|t|) How do each of these relate to the ideas of estimation and inference? Next, a cautionary tale. Here we fit another regression model to a different set of (fake) data, that gives the exact same regression results. mod.fit2 &lt;- lm(y3 ~ x3, data = anscombe) anscombe %&gt;% ggplot(aes(x3, y3)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(x = &quot;Fertilizer consumption (kg/hectare)&quot;, y = &quot;Cereal yield (100kg/hectare)&quot;) Figure 1.3: The data underlying the analysis of the variables y3 and x3 in the anscombe data set You should treat the results of the table with suspicion, given the figure above. summary(mod.fit2) ## ## Call: ## lm(formula = y3 ~ x3, data = anscombe) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.1586 -0.6146 -0.2303 0.1540 3.2411 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0025 1.1245 2.670 0.02562 * ## x3 0.4997 0.1179 4.239 0.00218 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.236 on 9 degrees of freedom ## Multiple R-squared: 0.6663, Adjusted R-squared: 0.6292 ## F-statistic: 17.97 on 1 and 9 DF, p-value: 0.002176 The rest of the book will give us the foundation to interpret all of the values in the regression table, and the underlying assumptions of the linear model. "],
["r-eda.html", "Chapter 2 R and exploratory data analysis 2.1 Inspecting the dataframe 2.2 Histograms 2.3 Summarising data 2.4 Loops 2.5 Functions 2.6 Boxplots 2.7 Scatterplots 2.8 Exercise set 2-2", " Chapter 2 R and exploratory data analysis This chapter is about getting comfortable with R. Edge uses base R for data wrangling and plotting, but here I’ll also recreate the chapter exercises using the tidyverse. For a deeper dive into R and the tidyverse: Data carpentry’s ecology lesson R for data science 2.1 Inspecting the dataframe head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa glimpse(iris) ## Observations: 150 ## Variables: 5 ## $ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4… ## $ Sepal.Width &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3… ## $ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1… ## $ Petal.Width &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0… ## $ Species &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, … summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## 2.2 Histograms # Base hist(iris$Sepal.Length, xlab = &quot;Sepal Length&quot;, main = &quot;&quot;) # Tidyverse iris %&gt;% ggplot(aes(Sepal.Length)) + geom_histogram(bins = 8, col = &quot;white&quot;) + labs(x = &quot;Sepal Length&quot;) 2.3 Summarising data # Base tapply(iris$Sepal.Length, iris$Species, mean) ## setosa versicolor virginica ## 5.006 5.936 6.588 # Tidyverse iris %&gt;% group_by(Species) %&gt;% summarise(mean = mean(Sepal.Length)) ## # A tibble: 3 x 2 ## Species mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 5.01 ## 2 versicolor 5.94 ## 3 virginica 6.59 Note that the tidyverse output is a tibble (essentially a dataframe), which is a consistent feature of the tidy approach to data wrangling. Not that you can’t do this in base R with the aggregate function - which is how I used to do it BT (before tidyverse): # Base option 2 aggregate(iris$Sepal.Length, list(iris$Species), mean) ## Group.1 x ## 1 setosa 5.006 ## 2 versicolor 5.936 ## 3 virginica 6.588 # You can check the output using str() aggregate(iris$Sepal.Length, list(iris$Species), mean) %&gt;% str() ## &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ Group.1: Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 2 3 ## $ x : num 5.01 5.94 6.59 2.4 Loops for(i in 1:3){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 i &lt;- 1 print(i) ## [1] 1 unique(iris$Species) ## [1] setosa versicolor virginica ## Levels: setosa versicolor virginica for(i in unique(iris$Species)){ print(mean(iris$Sepal.Length[iris$Species == i])) } ## [1] 5.006 ## [1] 5.936 ## [1] 6.588 2.5 Functions conditional.mean &lt;- function(x, y){ for(i in unique(y)){ print(mean(x[y == i])) } } conditional.mean(x = iris$Sepal.Length, y = iris$Species) ## [1] 5.006 ## [1] 5.936 ## [1] 6.588 conditional.mean(x = iris$Sepal.Width, y = iris$Species) ## [1] 3.428 ## [1] 2.77 ## [1] 2.974 2.6 Boxplots # Base boxplot(iris$Sepal.Length ~ iris$Species, xlab = &quot;Species&quot;, ylab = &quot;Sepal length&quot;) # Tidyverse iris %&gt;% ggplot(aes(Species, Sepal.Length)) + geom_boxplot() + labs(x = &quot;Species&quot;, y = &quot;Sepal length&quot;) 2.7 Scatterplots # Base plot(iris$Sepal.Length, iris$Sepal.Width, xlab = &quot;Sepal length&quot;, ylab = &quot;Sepal width&quot;) # Tidyverse iris %&gt;% ggplot(aes(Sepal.Length, Sepal.Width)) + geom_point() + labs(x = &quot;Sepal length&quot;, y = &quot;Sepal width&quot;) With unique symbols for species: # Base plot(iris$Sepal.Length, iris$Sepal.Width, xlab = &quot;Sepal length&quot;, ylab = &quot;Sepal width&quot;, pch = as.numeric(iris$Species)) legend(&quot;topright&quot;, pch = c(1,2,3), legend = c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;)) # Tidyverse iris %&gt;% ggplot(aes(Sepal.Length, Sepal.Width, shape = Species)) + geom_point() + labs(x = &quot;Sepal length&quot;, y = &quot;Sepal width&quot;) 2.8 Exercise set 2-2 Repeat the above analyses (histogram, summarising data, boxplots) for Petal.Width. Install and load a new package gpairs. Run the following line of code. What do you see? gpairs(iris, scatter.pars = list(col = as.numeric(iris$Species))) Install and load the package stfspack if you have not already done so. install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;mdedge/stfspack&quot;) library(stfspack) "],
["best-fit-line.html", "Chapter 3 Line of best fit 3.1 Exercise set 3-1 3.2 Exercise set 3-2", " Chapter 3 Line of best fit 3.1 Exercise set 3-1 Calculate least-squares intercept a and least-squares slope b for the anscombe data. I will use equations 3.8 and 3.9. x &lt;- anscombe$x1 y &lt;- anscombe$y1 xbar &lt;- mean(x) ybar &lt;- mean(y) b &lt;- sum((x - xbar)*(y - ybar)) / sum((x - xbar)^2) b ## [1] 0.5000909 a &lt;- ybar - b*xbar a ## [1] 3.000091 # Check using lm m1 &lt;- lm(y ~ x) summary(m1) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.92127 -0.45577 -0.04136 0.70941 1.83882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0001 1.1247 2.667 0.02573 * ## x 0.5001 0.1179 4.241 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 9 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.6295 ## F-statistic: 17.99 on 1 and 9 DF, p-value: 0.00217 3.2 Exercise set 3-2 On paper On paper On paper Comparing L1 and L2 lines. library(quantreg) x &lt;- anscombe$x1 y &lt;- anscombe$y1 mL1 &lt;- rq(y ~ x, tau = 0.5) mL2 &lt;- lm(y ~ x) plot(x, y) abline(mL2, lty = 1) abline(mL1, lty = 2) legend(&quot;topleft&quot;, legend = c(&quot;L2&quot;, &quot;L1&quot;), lty = c(1, 2)) Dataset with outlier. mL1 &lt;- rq(y3 ~ x3, tau = 0.5, data = anscombe) mL2 &lt;- lm(y3 ~ x3, data = anscombe) plot(y3 ~ x3, data = anscombe) abline(mL2, lty = 1) abline(mL1, lty = 2) legend(&quot;topleft&quot;, legend = c(&quot;L2&quot;, &quot;L1&quot;), lty = c(1, 2)) "],
["probability.html", "Chapter 4 Probability and random variables 4.1 Kolmogorov’s three axioms of probability 4.2 Conditional probability and independence 4.3 Bayes’ Theorem 4.4 Discrete random variables and distributions 4.5 Continuous random variables and distributions 4.6 Probability density functions 4.7 Families of distributions", " Chapter 4 Probability and random variables 4.0.1 Probability vs estimation Here are two quotes from STFS that encapsulate the distinction between probability and estimation: In probability theory, we think about processes that generate data, and we ask \"what can we say about the data generated by such a process? In statistical estimation and inference, we work in the opposite direction. We start with data, and we ask \"what can we say about the process that generated these data? So, we can define probability as the study of data generated by specified processes - and that’s the focus of this chapter. Later on, when we get to data - we’ll be assuming probabilistic models - but we have to accept the fact that the data we observed were not likely to be generated by the assumed model! But we do the best we can. 4.0.2 What is a probability? Frequency view the probability of a given event is simply the proportion of times it occurs over many trials difficult to apply to statements like “what is the probability that it will rain tomorrow” and “what is the probability that a meteor will hit the earth tomorrow” (i.e., one-shot events) Degrees-of-belief view what is the degree of belief in the occurrence of an event that a rational person would have? An aside: is the universe fundamentally probabilistic or deterministic? 4.0.3 Set notation A set (\\(\\Omega\\), in STFS) is an unordered collection of objects (elements). Intersection (\\(\\cap\\)) of two sets is the set of elements that appear in both sets. Union (\\(\\cup\\)) of two sets is the set that has every element that appears in either of the two original sets. The complement of a set (\\(S^C\\)) includes all of the elements not in the original set but present in the sample space (\\(\\Omega\\)) (which contains all possible elements). Subsets are referred to as S, and have the following properties: \\[ \\begin{aligned} S \\cup S^C = \\Omega \\\\ S \\cap S^C = \\emptyset \\end{aligned} \\] where \\(\\emptyset\\) is used to denote the empty set, or the set with no elements. A Venn diagram is useful here: The notation for the probability of an event is typically denoted using either \\(P()\\) or \\([]\\): \\[ \\begin{aligned} P(event) = [event] \\\\ \\end{aligned} \\] 4.1 Kolmogorov’s three axioms of probability Probabilities of any event i (\\(E_i\\)) cannot be negative: \\[ \\begin{aligned} \\ [E_i] \\geq 0 \\\\ \\end{aligned} \\] The probability of the event that includes every outcome is 1: \\[ \\begin{aligned} \\ [\\Omega] = 1 \\\\ \\end{aligned} \\] The probability of observing either of two mutually exclusive events is the sum of their individual probabilities: \\[ \\begin{aligned} \\text{If } E_1 \\cap E_2 = \\emptyset, \\\\ \\text{then } [E_1 \\cup E_2] = [E_1] + [E_2] \\end{aligned} \\] 4.1.1 Exercise set 4-1 Given P(\\(A\\)), we would like to know P(\\(A^C\\)). We already learned the following properties of complements: \\[ \\begin{aligned} A \\cup A^C = \\Omega \\\\ A \\cap A^C = \\emptyset \\end{aligned} \\] The intersection between \\(A\\) and \\(A^C\\) is zero, meaning they don’t share any elements. The union of \\(A\\) and \\(A^C\\) is \\([\\Omega]\\). We also learned the 2nd axiom of probability, \\([\\Omega] = 1\\). Therefore: \\[ \\begin{aligned} \\ [A] + [A^C] = 1 \\\\ \\ [A^C] = 1 - [A] \\end{aligned} \\] On paper. Below I’ve pasted an image of my handwritten version of Edge’s solution - I found it easier to think about once I visualized it using a Venn diagram. 4.2 Conditional probability and independence The law of conditional probability states that: \\[ \\begin{aligned} \\ [A \\mid B] = \\frac{[A \\cap B]}{[B]} \\\\ \\end{aligned} \\] If A and B are independent, then: \\[ \\begin{aligned} \\ [A \\mid B] = [A] \\\\ \\end{aligned} \\] 4.2.1 Exercise set 4-2 If we rearrange the law of probability, we get: \\[ \\begin{aligned} \\ [A \\mid B] [B] = [A \\cap B] \\\\ \\end{aligned} \\] By definition, if A and B are independent, then we can replace \\([A \\mid B]\\) with \\([A]\\) and get: \\[ \\begin{aligned} \\ [A \\cap B] = [A] [B] \\\\ \\end{aligned} \\] If we divide both sides by \\([A]\\), we get: \\[ \\begin{aligned} \\ [B] = \\frac{[A \\cap B]}{[A]} \\\\ \\end{aligned} \\] The right-hand side of the above equation is, by the law of conditional probability, equal to \\([B \\mid A]\\) and thus: \\[ \\begin{aligned} \\ [B] = [B \\mid A]\\\\ \\end{aligned} \\] Suppose you know \\([A \\mid B]\\), \\([A]\\), and \\([B]\\). Calculate \\([B \\mid A]\\). \\[ \\begin{aligned} \\ [A \\mid B] = \\frac{[A \\cap B]}{[B]} \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\ [A \\mid B] [B] = [A \\cap B] \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\ [B \\mid A] = \\frac{[A \\cap B]}{[A]} \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\ [B \\mid A] = \\frac{[A \\mid B] [B]}{[A]} \\\\ \\end{aligned} \\] 4.3 Bayes’ Theorem 4.4 Discrete random variables and distributions Random variable: e.g., the process of rolling a die, \\(X\\) \\(X\\) is random, can take integer values 1-6 \\(X\\) is not a number yet - it is the unrealized outcome of a random process The realization of that process is an instance - which is a number Capital \\(X\\): random variable Lower case \\(x\\): instance All the probability information is contained in its distribution. In our case, \\(X\\) is a discrete random variable, meaning that the number of outcomes is countable, in principle. Two ways to represent the distribution: Probability mass function (pmf), \\(f_X(x) = P(X = x)\\) Cumulative distribution function (cdf), \\(F_X(x) = P(X \\leq x)\\) The cdf is a series of partial sums of the pmf, \\[ \\begin{aligned} F_X(x) = P(X \\leq x) = \\sum_{x_i \\leq x} f_X(x_i) \\end{aligned} \\] and increases monotonically in \\(x\\). 4.4.1 Exercise set 4-3 Flipping a fair coin 3 times. X is a random variable that represents the number of heads observed, and the sample space \\(\\Omega\\) contains the elements {0, 1, 2, 3}. Here are all of the ways we can observe these elements: x = 0: (T, T, T) x = 1: (H, T, T); (T, H, T); (T, T, H) x = 2: (H, H, T); (H, T, H); (T, H, H) x = 3: (H, H, H) There are 8 possible instances. Thus, the probability mass function is: \\[ \\begin{aligned} f_X(0) =&amp; f_X(3) = 1/8 \\\\ f_X(1) =&amp; f_X(2) = 3/8 \\\\ f_X(x) =&amp; 0 \\text{ for all other } x \\\\ \\end{aligned} \\] If we sum \\(f_X(x_i)\\) for all possible \\(x_i\\), the sum would be 1 (1/8 + 1/8 + 3/8 + 3/8). In terms of \\(F_X\\), what is \\(P(a \\lt X \\le b)\\), if \\(b \\gt a\\)? Hint: notice that \\(a \\lt X \\le b\\) if and only if \\(X \\leq b\\) and \\(X &gt; a\\). Note that the cdf, \\(F_X\\), for \\(x = a\\) and \\(x = b\\) are given by the following: \\[ \\begin{aligned} F_X(a) =&amp; P(X \\le a) \\\\ F_X(b) =&amp; P(X \\le b) \\\\ \\end{aligned} \\] We want the probability of observing a value \\(x\\) between \\(a\\) and \\(b\\), so: \\[ \\begin{aligned} P(a \\lt X \\le b) = F_X(b) - F_X(a) \\\\ \\end{aligned} \\] 4.5 Continuous random variables and distributions What about values that are not countable - anything with a decimal? Impossible to get a specific number, or instance (e.g., the probability a person weighs exactly 70kg), and thus we cannot use the pmf. But we can still use the cdf: \\[ \\begin{aligned} F_X(x) = P(X \\leq x) \\end{aligned} \\] That is, we can ask about the probability that a person will weigh 70kg or less, or whether a person will weigh between 70 and 71kg. 4.5.1 Exercise set 4-4 \\(X\\) is a random variable that takes values in the interval [0, 1], and the probability distribution is uniform. Draw the cumulative distribution function \\(F_X(x)\\) for \\(x \\in [-1, 2]\\). x &lt;- c(0,1) Fx &lt;- x plot(x, Fx, type = &quot;l&quot;, xlim = c(-1,2)) lines(c(-1, 0), c(0, 0)) lines(c(1,2), c(1, 1)) If \\(X\\) were more likely to land in [0.4, 0.6] than in any other region of length 0.2, the line between 0.4 and 0.6 would be steeper. x1 &lt;- c(0,0.4) x2 &lt;- c(0.4, 0.6) x3 &lt;- c(0.6, 1) Fx1 &lt;- c(0,0.3) Fx2 &lt;- c(0.3, 0.7) Fx3 &lt;- c(0.7, 1) plot(x1, Fx1, type = &quot;l&quot;, xlim = c(-1,2), ylim = c(0,1), xlab = &quot;x&quot;, ylab = &quot;Fx&quot;) lines(x2, Fx2) lines(x3, Fx3) lines(c(-1, 0), c(0, 0)) lines(c(1,2), c(1, 1)) 4.6 Probability density functions For a continuous random variable, the pdf is the derivative of the cdf Recall that the cdf for a discrete random variable is a series of partial sums, given by: \\[ \\begin{aligned} F_X(x) = P(X \\leq x) = \\sum_{x_i \\leq x} f_X(x_i) \\end{aligned} \\] In an analogous fashion, we can integrate a continuous function to get the cdf of a continous random variable. We define the probability density function \\(f_X\\) (pdf) of a continuous random variable as: \\[ \\begin{aligned} F_X(x) =&amp; \\int_{- \\infty}^{x} f_X(u) du \\\\ \\end{aligned} \\] Below I have re-created Fig 4-4, with a pdf in the upper panel and a cdf in the lower panel - for an exponential random variable with rate 1. 4.6.1 Additional viz We can visualize the principle of question 3 from Exercise Set 4-3 using the cdf of a standard normal distribution, where \\(a = -1\\) and \\(b = 1\\): 4.6.2 Exercise set 4-5 If \\(f_X(x)\\) is a probability density funcion, then total area under \\(f_X(x)\\) is 1. Yes, \\(f_x\\) can be a probability density funcion, because the area under the function is 1. This situation is different from a probability mass function, because the y-axis values for a pdf can be &gt; 1 (in this case, the maximum y is 10). 4.7 Families of distributions Two requirements for mass or density functions: Must be non-negative Sum of all values is 1 (mass), or area under curve is 1 (density) Distribution family similarly shaped distributions summaries of their behavior can be computed from the same functions but parameter values differ 4.7.1 Exercise set 4-6 The probability mass function of the Poisson distribution is \\(P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\). Plugging in the appropriate values for \\(k\\) and \\(\\lambda\\) gives: (i). \\(e^{-5}\\) (ii). \\(5e^{-5}\\) (iii). \\(\\frac{25}{2}e^{-5}\\) Use the probability mass function of the geometric distribution with parameter 1/2. If our first “heads” occurs on the 6th flip, then we have five tails before it. We plug \\(p\\) = 1/2 and \\(k\\) = 5 into \\(P(X = k) = (1-p)^kp\\) to get 1/64. Consider a Poisson distribution with parameter \\(\\lambda\\) = 5. If we want to know the value of the probability mass function for x = 2, \\(f_X(2)\\), we use the dpois() function: dpois(2, lambda = 5) ## [1] 0.08422434 To get the value of the cumulative distribution function \\(F_X(2)\\), we use ppois(): ppois(2, lambda = 5) ## [1] 0.124652 If we want to know the inverse of the cumulative distribution function. That is, we want to know the value of \\(q\\) that solves the equation \\(F_X(q) = p\\). What is the number \\(q\\) such that the probability tha the random variable is less than or equal to \\(q\\) is \\(p\\); \\(q\\) is the \\(pth\\) percentile of the distribution. We can get this using qpois(): qpois(0.124652, lambda = 5) ## [1] 2 The inverse of the cdf is also called the quantile function. Using the standard normal distribution (mean = 0, sd = 1), plot the probability density function for x \\(\\in\\) [-3, 3]. x &lt;- seq(-3, 3, by = 0.1) fx &lt;- dnorm(x = x) plot(x, fx, type = &quot;l&quot;, main = &quot;PDF&quot;, col = &quot;red&quot;) Plot the cdf for x \\(\\in\\) [-3, 3]. x &lt;- seq(-3, 3, by = 0.1) fx &lt;- pnorm(q = x) plot(x, fx, type = &quot;l&quot;, main = &quot;CDF&quot;, col = &quot;red&quot;) What value of \\(x\\) is at the 97.5th percentile of the standard normal? qnorm(p = 0.975) ## [1] 1.959964 Simulating from a normal distribution and from a uniform distribution: n &lt;- 1000 x &lt;- rnorm(n) hist(x) x &lt;- runif(n, min = 0, max = 1) hist(x) Now take those values between 0 and 1, and feed them into the qnorm function to get the values at which we see those quantiles: y &lt;- qnorm(p = x, mean = 0, sd = 1) hist(y) These values are normally distributed around 0. This plot is saying that most of the probability (in fact ~95%) lies between -2 &lt; y &lt; 2 (2 standard deviations). r &lt;- seq(-3, 3, length.out = 1000) cdf &lt;- pnorm(r) #Draw the normal cumulative distribution function. plot(r, cdf, type = &quot;l&quot;, xaxs = &quot;i&quot;, yaxs = &quot;i&quot;, xlim = c(-3, 3), xlab = expression(italic(x)), ylab = expression(paste(italic(F[X]), &quot;(&quot;, italic(x), &quot;)&quot;, sep = &quot;&quot;)), lwd = 2) #Draw light grey lines representing random samples from the #standard normal distribution. x &lt;- rnorm(100) for(i in x){ lines(c(i,i), c(min(x), pnorm(i)), col = rgb(190, 190, 190, alpha = 60, max = 255)) lines(c(min(x)-1,i), c(pnorm(i), pnorm(i)), col = rgb(190, 190, 190, alpha = 60, max = 255)) } 4.7.2 Additional exercise (Courtesy Blondin &amp; Goodman) Assume that destructive earthquakes occur in California every 20 years. You’d like to know how probable an earthquake is between now and some future date. What distribution family describes the waiting time until California’s next earthquake? What value should we use for this distribution’s parameter? [Hint: convert 20 years to a rate] Use the exponential distribution, where \\(\\lambda = 1/20\\): \\[ \\begin{aligned} f_X(x) = \\lambda e ^{-\\lambda x} \\\\ \\end{aligned} \\] What is the probability that an earthquake will happen in the next 10 years? [Hint: You need to find a cumulative distribution function for the distribution you chose, either online or by integration]. Rules for integrating “e raised to the x power”: \\[ \\begin{aligned} \\int e^xdx =&amp; e^x + c \\\\ \\int e^{ax}dx =&amp; \\frac{1}{a} e^{ax} + c \\\\ \\int be^{ax}dx =&amp; \\frac{b}{a} e^{ax} + c \\\\ \\end{aligned} \\] With these rules in hand, we can get the cdf by integrating the pdf for the exponential distribution: \\[ \\begin{aligned} f_X(x) =&amp; \\lambda e ^{-\\lambda x} \\\\ F_X(X = x) =&amp; \\int_{0}^{x} \\lambda e^{-\\lambda x} dx \\\\ =&amp; \\frac{1}{- \\lambda} \\lambda e^{-\\lambda x} \\bigg\\rvert_{0}^{x} \\\\ =&amp; -e^{-\\lambda x} \\rvert_{0}^{x} \\\\ =&amp; -e^{-\\lambda x} \\rvert_{0}^{x} \\\\ =&amp; -e^{-\\lambda x} - (-e^{-\\lambda 0}) \\\\ =&amp; -e^{-\\lambda x} - (-1) \\\\ =&amp; 1 -e^{-\\lambda x} \\\\ \\end{aligned} \\] Then, plug in \\(x = 10\\) and \\(\\lambda = 1/20\\): 1 - exp(-(1/20) * (10)) ## [1] 0.3934693 Check to make sure we’ve done this right using the pexp function in R: pexp(q = 10, rate = 1/20) ## [1] 0.3934693 What is the probability that an earthquake will occur between 10 and 20 years from now? pexp(q = 20, rate = 1/20) - pexp(q = 10, rate = 1/20) ## [1] 0.2386512 If you have time, write the CDF of your chosen distribution as an R function. It should take a value x and a parameter, and return a probability P(X ≤ x). Use your function to plot the cumulative distribution as a function of x. exp_cdf &lt;- function(x, lambda) 1 - exp(-lambda * x) x &lt;- seq(0, 80, by = 0.1) p &lt;- exp_cdf(x = x, lambda = 1/20) plot(p ~ x, type = &quot;l&quot;, xlab = &quot;x&quot;, ylab = &quot;FX(x)&quot;) "],
["randomvars.html", "Chapter 5 Properties of random variables 5.1 Expected values and the law of large numbers 5.2 Variance and standard deviation 5.3 Joint distributions, covariance, and correlation 5.4 Conditional distribution, expectation, variance 5.5 The central limit theorem 5.6 A probabilistic model for simple linear regression", " Chapter 5 Properties of random variables 5.1 Expected values and the law of large numbers When summarizing a probability distribution, it is useful to have a measure of: Location (Expectation; E(\\(X\\))) Dispersal (Variance; Var(\\(X\\))) In this section, we’re focusing on the expectation. The expectation of a discrete random variable is the average: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\sum_{i = 1}^{k}x_i P(X = x_i) \\\\ =&amp; \\sum_{i = 1}^{k}x_i f_X(x_i) \\\\ \\end{aligned} \\] If \\(Y\\) represents a six-sided die, then: \\[ \\begin{aligned} \\text{E}(Y) =&amp; \\sum_{i = 1}^{k}y_i f_Y(y_i) \\\\ =&amp; 1(1/6) + 2(1/6) + 3(1/6) + 4(1/6) + 5(1/6) + 6(1/6) \\\\ =&amp; 21/6 \\\\ =&amp; 7/2 \\\\ \\end{aligned} \\] If \\(X\\) is continuous: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\int_{- \\infty}^{\\infty} x f_X(x) dx \\\\ \\end{aligned} \\] Here we are integrating over the probability density function, rather than summing over the mass density function. 5.1.1 Weak law of large numbers The expectation is more like a long-term average, rather than an actual instance (7/2 is not a possible instance of a dice roll). \\(X_i\\) are i.i.d. Assume E(\\(X_1\\)) = E(\\(X_2\\)) = … = E(\\(X_n\\)) = \\(\\mu\\) Define \\(\\overline{X}_n\\) as the mean of the observations: \\[ \\begin{aligned} \\overline{X}_n = \\frac{1}{n} (X_1 + X_2 + X_3 + ... + X_n) \\end{aligned} \\] As \\(n \\rightarrow \\infty\\), \\(\\overline{X}_n\\) “converges in probability” to \\(\\mu\\). This means for any positive constant \\(\\delta\\), \\[ \\begin{aligned} lim_{n \\rightarrow \\infty} \\text{P}(|\\overline{X}_n - \\mu| &gt; \\delta) = 0 \\end{aligned} \\] 5.1.2 Handy facts about expectations The expectation of a constant times a random variable is the constant times the expectation of the random variable: \\[ \\begin{aligned} \\text{E}(aX) = a \\text{E}(X) \\end{aligned} \\] The expectation of a constant is the constant: \\[ \\begin{aligned} \\text{E}(c) = c \\end{aligned} \\] The expectation of a sum of random variables is the sum of the expectations of those random variables: \\[ \\begin{aligned} \\text{E}(X + Y) = \\text{E}(X) + \\text{E}(Y) \\end{aligned} \\] Putting all these facts together, we can calculate the expectation of two random variables \\(X\\) and \\(Y\\) as: \\[ \\begin{aligned} \\text{E}(aX + bY + c) = a \\text{E}(X) + b \\text{E}(Y) + c \\end{aligned} \\] This is called the linearity of expectation, which we will use frequently in the exercises. Linearity does not hold for other measures of location (e.g., median, mode). This fact accounts, in part, for the privileged status of the mean in statistics. To calculate the expectation of a function: \\[ \\begin{aligned} \\text{E}[g(X)] = \\sum_{i = 1}^{k} g(x_i)f_X(x_i) \\end{aligned} \\] \\[ \\begin{aligned} \\text{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x)f_X(x)dx \\end{aligned} \\] 5.1.3 Exercise set 5-1 1a. Expected value of a Bernoulli random variable with parameter p? \\[ \\begin{aligned} f_X(x) = \\text{P}(X = x) = p^x(1 - p)^{1-x} \\text{ for } x \\in \\text{{0, 1}} \\\\ \\end{aligned} \\] Because there are only two outcomes (0 or 1), we can compute the expectation directly: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\sum_0^1 x f_X(x) \\\\ =&amp; \\sum_0^1 x p^x(1 - p)^{1-x} \\\\ =&amp; 0 p^0(1 - p)^{1-0} + 1 p^1(1 - p)^{1-1} \\\\ =&amp; 0 p^0(1 - p)^{1} + 1 p^1(1 - p)^{0} \\\\ =&amp; 0 (1) (1-p) + p(1) \\\\ =&amp; 0 + p \\\\ =&amp; p \\end{aligned} \\] 1b. What is the expected value of a binomial random variable with parameters \\(n\\) and \\(p\\)? Here’s the pmf for the binomial distribution: \\[ \\begin{aligned} f_X(x) = \\text{P}(X = x) = \\binom{n}{x} p^x(1 - p)^{n - x} \\text{ for } x \\in \\text{{0, 1, 2, ..., n}} \\\\ \\end{aligned} \\] If we plug that into the equation for E(\\(X\\)), we get: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\sum_0^n x f_X(x) \\\\ =&amp; \\sum_0^n x \\binom{n}{x} p^x(1 - p)^{n - x} \\\\ \\end{aligned} \\] Well, I don’t know how to evaluate this sum directly, considering the upper limit of \\(n\\) is infinite. So we’ll use the fact that the binomial is the sum of \\(n\\) independent Bernoulli trials (\\(X_i\\)). \\[ \\begin{aligned} \\text{E}(X) =&amp; \\text{E}(\\sum_{i=1}^nX_i) \\end{aligned} \\] Because the expectation is linear, the expectation of the sum is the sum of the expectations; we can rearrange: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\sum_{i=1}^n \\text{E}(X_i) \\end{aligned} \\] From 1a, we can substitute \\(p\\) for \\(\\text{E}(X_i)\\): \\[ \\begin{aligned} \\text{E}(X) =&amp; \\sum_{i=1}^n p \\\\ =&amp; np \\end{aligned} \\] 1c. What is the expected value of a discrete uniform random variable with parameters \\(a\\) and \\(b\\)? The probability mass function is: \\[ \\begin{aligned} \\text{P}(X = k) =&amp; \\frac{1}{b - a + 1} \\\\ \\end{aligned} \\] The expectation is: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\sum_{x = a}^b x f_X(x) \\\\ =&amp; \\sum_{x = a}^b x \\frac{1}{b - a + 1} \\\\ =&amp; \\frac{1}{b - a + 1} \\sum_{x = a}^b x \\\\ \\end{aligned} \\] We were given a hint that is useful now: for integers \\(a\\) and \\(b\\) with \\(b &gt; a\\), the sum of all the integers including \\(a\\) and \\(b\\), is: \\[ \\begin{aligned} \\sum_{k = a}^b k =&amp; \\frac{(a + b)(b - a + 1)}{2} \\\\ \\end{aligned} \\] So, plugging that hint in we get: \\[ \\begin{aligned} =&amp; \\frac{1}{b - a + 1} \\times \\frac{(a + b)(b - a + 1)}{2} \\\\ =&amp; \\frac{a + b}{2} \\\\ \\end{aligned} \\] 1d. What is the expected value of a continuous uniform random variable with parameters \\(a\\) and \\(b\\)? The probability density function is: \\[ \\begin{aligned} \\text{P}(X) =&amp; \\frac{1}{b - a} \\\\ \\end{aligned} \\] The expectation is: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\int_{a}^b x f_X(x) dx \\\\ =&amp; \\int_{a}^b x \\frac{1}{b - a} dx \\\\ =&amp; \\frac{1}{b - a} \\int_{a}^b x dx \\\\ \\end{aligned} \\] Now we have to integrate the 2nd term: \\[ \\begin{aligned} =&amp; \\frac{1}{b - a} \\times \\frac{1}{2} x^2 \\bigg\\rvert_{a}^{b} \\\\ =&amp; \\frac{1}{b - a} \\times (\\frac{b^2}{2} - \\frac{a^2}{2}) \\\\ =&amp; \\frac{1}{b - a} \\times (\\frac{b^2 - a^2}{2}) \\\\ \\end{aligned} \\] We use the hint from earlier, that \\(b^2 - a^2 = (b-a)(b+a)\\): \\[ \\begin{aligned} =&amp; \\frac{1}{b - a} \\times (\\frac{(b-a)(b+a)}{2}) \\\\ =&amp; \\frac{a + b}{2} \\\\ \\end{aligned} \\] Exploring the law of large numbers by simulation. In Edge’s code block below, samp.size represents \\(n\\) in the weak law of large numbers (above); n.samps represents independent random variables \\(X_n\\). The expectation for all \\(X_i\\) is \\(\\mu\\). samp.size &lt;- 20 n.samps &lt;- 1000 samps &lt;- rnorm(samp.size * n.samps, mean = 0, sd = 1) # Each column represents a random variable, X_i # Each row represents a sample (instance) drawn from X_i samp.mat &lt;- matrix(samps, ncol = n.samps) str(samp.mat) ## num [1:20, 1:1000] 0.245 -1.993 -2.163 1.451 0.588 ... # Here we calculate the sample mean for each X_i (column) samp.means &lt;- colMeans(samp.mat) str(samp.means) ## num [1:1000] -0.3051 0.075 0.3764 -0.0987 0.2287 ... hist(samp.means) 2a. What happens if we change samp.size (i.e., \\(n\\))? n_vector &lt;- c(1, 5, 20, 50, 100, 1000) samp_means_mat &lt;- matrix(data = NA, nrow = n.samps, ncol = length(n_vector)) calculate_sample_means &lt;- function(samp.size = 20, n.samps = 1000){ samps &lt;- rnorm(samp.size * n.samps, mean = 0, sd = 1) samp.mat &lt;- matrix(samps, ncol = n.samps) samp.means &lt;- colMeans(samp.mat) return(samp.means) } par(mfrow = c(2,3)) set.seed(21) for(i in 1:length(n_vector)){ samp_size_i &lt;- n_vector[i] samp_means_i &lt;- calculate_sample_means(samp.size = samp_size_i) hist(samp_means_i, xlim = c(-3, 3), ylim = c(0, 250), xlab = &quot;Sample mean&quot;, main = paste(&quot;n = &quot;, samp_size_i, sep = &quot;&quot;), col = &quot;red&quot;) } 2b. Using the exponential distribution. n_vector &lt;- c(1, 5, 20, 50, 100, 1000) samp_means_mat &lt;- matrix(data = NA, nrow = n.samps, ncol = length(n_vector)) calculate_sample_means_exp &lt;- function(samp.size = 20, n.samps = 1000){ samps &lt;- rexp(samp.size * n.samps, rate = 1) samp.mat &lt;- matrix(samps, ncol = n.samps) samp.means &lt;- colMeans(samp.mat) return(samp.means) } par(mfrow = c(2,3)) set.seed(21) for(i in 1:length(n_vector)){ samp_size_i &lt;- n_vector[i] samp_means_i &lt;- calculate_sample_means_exp(samp.size = samp_size_i) hist(samp_means_i, xlab = &quot;Sample mean&quot;, main = paste(&quot;n = &quot;, samp_size_i, sep = &quot;&quot;), col = &quot;red&quot;) } 5.2 Variance and standard deviation The variance is a measurement of dispersal - i.e., how spread out is the distribution? And spread out from what, exactly? It is useful to think about the distance \\(X_i\\) takes from the expectation, E\\((X)\\): \\(X - \\text{E}(X)\\). What if we took the expectation of this - i.e., the average value of the distance from the mean? \\[ \\begin{aligned} \\text{E}(X - \\text{E}(X)) \\\\ \\text{by linearity of expectation, we get:} \\\\ \\text{E}(X) - \\text{E}(\\text{E}(X)) \\\\ \\text{E}(X) - \\text{E}(X) \\\\ 0 \\end{aligned} \\] This won’t work - we need to find a way to constrain the expression inside the parentheses to be non-negative. One way to do this is to use the mean absolute deviation, \\(|X - \\text{E}(X)|\\). Another way is to use the mean squared deviation, \\([X - \\text{E}(X)]^2\\). The squared term constrains the variance to be \\(\\ge 0\\): \\[ \\begin{aligned} \\text{Var}(X) =&amp; \\text{E}([X - \\text{E}(X)]^2) \\\\ \\end{aligned} \\] The mean squared deviation has two mathematical advantages: It is easier to compute mathematically than an analogous quanitity using absolute deviations (but why?) The variances of linear functions of random variables are ‘beautifully behaved’, whereas the analogous quantities for absolute deviations can be a hassle. I will just take Edge’s word on these two points for now. 5.2.1 Beautiful properties of the variance The variance can be rewritten as: \\[ \\begin{aligned} \\text{Var}(X) = \\text{E}(X^2) - [\\text{E}(X)]^2 \\\\ \\end{aligned} \\] which is generally easier to compute. Adding a constant to a random variable does not affect the variance: \\[ \\begin{aligned} \\text{Var}(a + cX) = c^2\\text{Var}(X) \\\\ \\end{aligned} \\] where \\(a\\) and \\(c\\) are constants. If \\(X\\) and \\(Y\\) are independent random variables, then: \\[ \\begin{aligned} \\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) \\\\ \\end{aligned} \\] One big problem with the variance is that is in the wrong (\\(X^2\\)) units. To fix this, we calculate the standard deviation: \\[ \\begin{aligned} \\text{SD}(X) = \\sqrt{\\text{Var}(X)} \\\\ \\end{aligned} \\] SD is usually larger (never smaller) than MAD, and is more sensitive to large deviations. 5.2.2 Exercise set 5-2 I had to walk through Edge’s solutions bit by bit; my handwritten version is here. 5.3 Joint distributions, covariance, and correlation This section covers four key concepts: Joint probability distribution: the probability distribution of the joint occurrence of \\(X\\) and \\(Y\\) Marginal distribution of X: the probability distribution of \\(X\\), summing (integrating) over all values of \\(Y\\) Covariance: a measurement of the extent to which \\(X\\) and \\(Y\\) depart from independence Correlation: covariance rescaled to go from -1 to 1 5.3.1 Joint probability distributions Joint probability distribution: the probability distribution of the joint occurrence of \\(X\\) and \\(Y\\) The joint cumulative probability distribution of two random variables \\(X\\) and \\(Y\\) is given by: \\[ \\begin{aligned} F_{X,Y}(x, y) = \\text{P} (X \\leq x ~\\cap ~ Y \\leq y) \\end{aligned} \\] Here is the corresponding joint probability mass function: \\[ \\begin{aligned} f_{X,Y}(x, y) = \\text{P} (X = x ~\\cap ~ Y = y) \\end{aligned} \\] And for two continuous variables, we can recover the cumulative distribution function by integrating the probability density function with respect to \\(X\\) and \\(Y\\): \\[ \\begin{aligned} F_{X,Y}(x, y) =&amp; ~ \\text{P}(X \\leq x ~\\cap ~ Y \\leq y) \\\\ =&amp; \\int_{- \\infty}^{x} \\int_{- \\infty}^{y} f_{X,Y}(x, y) dx dy \\end{aligned} \\] 5.3.2 Marginal distributions Marginal distribution of X: the probability distribution of \\(X\\), summing (integrating) over all values of \\(Y\\) For discrete random variables, the marginal distribution of \\(X\\) is: \\[ \\begin{aligned} f_{X}(x) =&amp; ~ \\text{P} (X = x) \\\\ =&amp; ~ \\sum_{y} \\text{P} (X = x ~\\cap ~ Y = y) \\\\ =&amp; \\sum_{y} f_{X,Y}(x,y) \\end{aligned} \\] For continuous random variables, the marginal distribution of \\(X\\) is: \\[ \\begin{aligned} f_{X}(x) =&amp; \\int_{- \\infty}^{\\infty} f_{X,Y}(x, y) dy \\end{aligned} \\] 5.3.3 Covariance Covariance is a measurement of the extent to which \\(X\\) and \\(Y\\) depart from independence Such a measure should have two basic properties: The number should be positive when \\(X\\) and \\(Y\\) increase or decrease together The number should be negative when \\(X\\) increases and \\(Y\\) decreases (and vice versa). Consider the random variable \\([X - \\text{E}(X)][Y - \\text{E}(Y)]\\). If we sample a joint probability distribution, resulting in a set (\\(\\Omega\\)) of pairs of \\((x, y)\\), ask yourself: Is the sign positive or negative when most of the pairs \\((x, y)\\) are such that \\(x &gt; \\text{E}(X)\\) and \\(y &gt; \\text{E}(Y)\\)? Is the sign positive or negative when most of the pairs \\((x, y)\\) are such that \\(x &lt; \\text{E}(X)\\) and \\(y &lt; \\text{E}(Y)\\)? Is the sign positive or negative when most of the pairs \\((x, y)\\) are such that \\(x &gt; \\text{E}(X)\\) and \\(y &lt; \\text{E}(Y)\\)? Is the sign positive or negative when most of the pairs \\((x, y)\\) are such that \\(x &lt; \\text{E}(X)\\) and \\(y &gt; \\text{E}(Y)\\)? Hopefully, you have convinced yourself that the random variable \\([X - \\text{E}(X)][Y - \\text{E}(Y)]\\) satisfies the two aforementioned properties. Now we take the expectation of this random variable to arrive at the covariance. Conveniently (or purposefully?), the covariance is an extension of the variance: \\[ \\begin{aligned} \\text{Cov}(X,Y) =&amp; ~ \\text{E}([X - \\text{E}(X)][Y - \\text{E}(Y)]) \\\\ =&amp; ~ \\text{E}(XY) - \\text{E}(X)\\text{E}(Y) \\end{aligned} \\] If you replace \\(\\text{E}(Y)\\) with \\(\\text{E}(X)\\) in the above equation, you should recover the definition of Var\\((X)\\), \\(\\text{E}(X^2) - [\\text{E}(X)]^2\\). If \\(X\\) and \\(Y\\) are independent, then \\(\\text{Cov}(X,Y) = 0\\) (we showed this in an earlier problem set). However, if \\(\\text{Cov}(X,Y) = 0\\), that does not necessarily imply that \\(X\\) and \\(Y\\) are independent. 5.3.4 Correlation Correlation: covariance rescaled to go from -1 to 1 The covariance is not a pure measure of the linear dependence between two variables, because it is sensitive to the scaling of the variables. Therefore, we cannot use the covariance to compare the strengths of different bivariate relationships. In other words, we cannot use the covariance to answer the question: Is the relationship between cereal yield and fertilizer consumption stronger than the relationship between career earnings and college GPA?. Instead, we calculate the correlation: \\[ \\begin{aligned} \\text{Cor}(X,Y) =&amp; ~ \\rho_{X,Y} \\\\ =&amp; ~ \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}} \\\\ =&amp; ~ \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\\\\ \\end{aligned} \\] You can prove to yourself that the correlation is bounded from -1 to 1 using a simple heuristic. Which variable should be the most correlated with \\(X\\)? Well, that would be \\(X\\). Plugging in \\(X\\) for \\(Y\\), we get: \\[ \\begin{aligned} \\text{Cor}(X,X) =&amp; ~ \\frac{\\text{Cov}(X,X)}{\\sigma_X \\sigma_X} \\\\ =&amp; ~ \\frac{\\text{Var}(X)}{\\text{Var}(X)} \\\\ =&amp; ~ 1 \\end{aligned} \\] Using the same logic, \\(-X\\), is the least correlated with \\(X\\). Try working through the algebra, you should get -1: \\[ \\begin{aligned} \\text{Cor}(X,-X) =&amp; ~ \\frac{\\text{Cov}(X,-X)}{\\sqrt{\\text{Var}(X)\\text{Var}(-X)}} \\\\ =&amp; ~ \\frac{\\text{E}(X \\times -X) - \\text{E}(X)\\text{E}(-X)}{\\sqrt{\\text{Var}(X)\\text{Var}(-X)}} \\\\ \\end{aligned} \\] 5.3.5 Additional exercise I will add one problem, to reinforce the concepts of joint and marginal distributions, with two discrete random variables. This problem covers similar ideas to Edge’s first exercise in set 5-3. You watched 100 female birds last spring, and recorded the number of offspring per bird (X; 1, 2, or 3 chicks). You also recorded the age of each mom (Y; 1, 2, or 3 years). You observed: 10 1-yr olds, all with one chick. 27 2-yr olds; 13 had one chick, 12 had two chicks, and 2 had three chicks. 63 3-yr olds; 23 had one chick, 36 had two chicks, and 4 had three chicks. Calculate: The probability of observing each possible outcome (e.g., a 1-yr old bird has 1 chick; a 1-yr old bird has 2 chicks; etc.). The probability of observing a 1-yr old bird; a 2-yr old bird; and a 3-yr old bird. The probability of observing 1 chick per mom; 2 chicks per mom; 3 chicks per mom. STOP! NO PEEKING ! ANSWER IS BELOW: Wait for it… …wait for it … …here it is: an excel (gasp!) plot! The key here is to recognize that yellow represents the joint probabilities of X and Y; the green and blue represents the marginal probabilities of X and Y, respectively. Stare at this until it clicks. A similar principle applies to continuous distributions, but rather than summing across Y, we integrate across Y to get the marginal distribution of X. 5.3.6 Exercise set 5-3 I had to walk through Edge’s solutions bit by bit; my handwritten version is here. 5.4 Conditional distribution, expectation, variance For two discrete random variables, the conditional probability mass function is: \\[ \\begin{aligned} f_{X|Y}(x |Y = y) =&amp; \\text{P}(X = x | Y = y) \\\\ =&amp; \\frac{\\text{P} (X = x ~\\cap ~ Y = y)}{\\text{P}(Y = y)} \\\\ =&amp; \\frac{f_{X,Y}(x,y)}{f_{Y}(y)} \\end{aligned} \\] For two continuous random variables, the conditional probability density function is defined similarly: \\[ \\begin{aligned} f_{X|Y}(x |Y = y) =&amp; \\frac{f_{X,Y}(x,y)}{f_{Y}(y)} \\end{aligned} \\] Edge has a nice visualization and explanation of conditional distribution in his Fig 5-5. 5.5 The central limit theorem Natural populations are large, so we usually gather just a sample and use that as a surrogate for the whole population. If we take \\(n\\) samples, then another \\(n\\) samples, and then another \\(n\\) samples, and calculate \\(\\overline{X}_1\\), \\(\\overline{X}_2\\), and \\(\\overline{X}_3\\), differences in our estimate of \\(\\overline{X}\\) are due to sampling variation. The weak law of large numbers (above) tells us that as \\(n\\) approaches \\(\\infty\\), our estimate of \\(\\overline{X}_n\\) approaches the true population mean, \\(\\mu\\), and that the Var(\\(\\overline{X}_n\\)) = \\(\\sigma^2 / n\\), approaches 0. But what is the shape of this distribution? That is where the central limit theorem (CLT) comes in. As \\(n\\) approaches \\(\\infty\\), the distribution of \\(\\overline{X}_n\\) converges to a normal distribution with expectation \\(\\mu\\) and variance \\(\\sigma^2 / n\\). An importance consequence of the CLT is the surpising result that the distribution of sample means \\(\\overline{X}_n\\) is approximately normal even when the distribution of the individual observations are not normally distributed! The implications of the CLT are huge: it allows us to use the normal distribution (and the powerful set of analytical tools that depend on it) in real-world situations where the underlying data are not normally distributed, as long as we have enough samples. What is enough? A general rule of thumb is 30, but will vary with the underlying probability distribution of the population. You will explore this using simulations in the problem set below. 5.5.1 Exercise set 5-4 Bean machine in action! library(animation) nball &lt;- 500 #change the number of balls nlayer &lt;- 10 #change the number of rows of pegs on the board rate &lt;- 10 #change the speed at which the balls fall ani.options(nmax = nball + nlayer - 2, interval = 1/rate) quincunx(balls = nball, layers = nlayer) Exploring the beta distribution To see what the beta distribution looks like for a given set of shape parameters, set the sample size to 1. For example: library(stfspack) # dosm.beta.hist(n = 1, nsim = 10000, shape1 = 1, shape2 = 1) will give you a histogram of 10,000 observations from a beta distribution with parameters 1 and 1. If you increase the sample size, then the distribution of the sample mean gets closer to normality. Try this — starting with samples of size 1 and increasing the sample size — with the following sets of parameter values: (1, 1), (0.2, 0.2), (2, 0.5), (0.5, 2), (3, 3). Feel free to try other parameter sets — it’s fun. What do you notice? sims &lt;- 1000 s1 &lt;- 0.2 # change this s2 &lt;- 0.2 # change this par(mfrow = c(2,3)) dosm.beta.hist(n = 1, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.4976551 0.4193736 0.1758742 dosm.beta.hist(n = 4, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.51873441 0.21324943 0.04547532 dosm.beta.hist(n = 8, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.50331631 0.14985449 0.02245637 dosm.beta.hist(n = 16, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.49738079 0.10502289 0.01102981 dosm.beta.hist(n = 32, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.500954728 0.075780685 0.005742712 dosm.beta.hist(n = 64, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.500007026 0.053298265 0.002840705 Let’s deconstruct what is going on with this function, where n = 1 (we simulate 10000 observations from a single set of parameter values). dosm.beta.hist ## function (n, nsim, shape1 = 1, shape2 = 1, ...) ## { ## samps &lt;- rbeta(n * nsim, shape1, shape2) ## sim.mat &lt;- matrix(samps, nrow = nsim) ## dosm &lt;- rowMeans(sim.mat) ## hist(dosm, freq = FALSE, ...) ## x &lt;- seq(0, 1, length.out = 1000) ## lines(x, dnorm(x, mean = mean(dosm), sd = sd(dosm))) ## c(`mean of DOSM` = mean(dosm), `SD of DOSM` = sd(dosm), `var of DOSM` = var(dosm)) ## } ## &lt;bytecode: 0x7fa56c6afa08&gt; ## &lt;environment: namespace:stfspack&gt; nsim &lt;- 10000 n &lt;- 1 s1 &lt;- 0.2 # change this s2 &lt;- 0.2 # change this samps &lt;- rbeta(n * nsim, shape1 = s1, shape2 = s2) str(samps) # here are 10,000 samples ## num [1:10000] 0.13472 0.70805 0.99361 0.00268 0.58459 ... # We are converting the vector into a matrix # So that we can easily calculate the mean of each row sim.mat &lt;- matrix(samps, nrow = nsim) dim(sim.mat); head(sim.mat) ## [1] 10000 1 ## [,1] ## [1,] 0.13471512 ## [2,] 0.70804823 ## [3,] 0.99361002 ## [4,] 0.00268066 ## [5,] 0.58459079 ## [6,] 0.99641568 # Calculate rowmeans - with n=1, this doesn&#39;t change anything # But change n to anything bigger and inspect the dimensions of the objects dosm &lt;- rowMeans(sim.mat) str(dosm) # compare these values to sim.mat ## num [1:10000] 0.13472 0.70805 0.99361 0.00268 0.58459 ... par(mfrow = c(1,1)) hist(dosm, freq = FALSE) # plotting the simulated values # Set up a vector that goes from 0 to 1 to overlay a normal distribution on the histogram x &lt;- seq(0, 1, length.out = 1000) # Now plot a normal distribution, using the mean and sd of the simulated values lines(x, dnorm(x, mean = mean(dosm), sd = sd(dosm)), col = &quot;red&quot;) The Pareto distribution is a skewed, heavy-tailed, power-law distribution used in description of social, scientific, geophysical, actuarial, and many other types of observable phenomena. It was applied originally to the distribution of wealth in a society, fitting the observation that a large portion of wealth is held by a small fraction of the population. Named after the Italian civil engineer, economist, and sociologist Vilfredo Pareto. Parameters of the rpareto function: a: shape (on the web as \\(\\alpha\\)) b: scale (on the web as \\(x_m\\)) If the shape parameter is \\(\\leq\\) 1, \\(E(X)\\) is \\(\\infty\\). If the shape parameter is \\(\\leq\\) 2, \\(Var(X)\\) is \\(\\infty\\). First we simulate many sampes of size 1000 from a Pareto distribution with shape = 4. # experiment with n and the parameters a and b n &lt;- 100 n_sims &lt;- 10000 a &lt;- 1 b &lt;- 4 x &lt;- rpareto(n = n, a = a, b = b) summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.076 6.120 9.899 29.924 20.622 928.145 # Calculate mean and sd mu &lt;- mean(x) stdev &lt;- sd(x) hist(x, freq = FALSE) # Set up a vector that goes from 0 to 1 to overlay a normal distribution on the histogram x_vals &lt;- seq(min(x), max(x), length.out = 1000) # Now plot a normal distribution, using the mean and sd of the simulated values lines(x_vals, dnorm(x_vals, mean = mu, sd = stdev), col = &quot;red&quot;) # Compare tail to normal compare.tail.to.normal ## function (x, k, mu, sigma) ## { ## mean(x &lt; (mu - k * sigma) | x &gt; (mu + k * sigma))/(1 - (pnorm(k) - ## pnorm(-k))) ## } ## &lt;bytecode: 0x7fa56d7caf70&gt; ## &lt;environment: namespace:stfspack&gt; k &lt;- 2 # sds compare.tail.to.normal(x = x, k = k, mu = mu, sigma = stdev) ## [1] 0.2197789 summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.076 6.120 9.899 29.924 20.622 928.145 mu ## [1] 29.92446 stdev ## [1] 95.49789 # This gives the value of the mean, minus the value k*stdev # (i.e., an extreme negative value) # Below I will use my object stdev in place of sigma (the parameter from Edge&#39;s function) (mu - k * stdev) ## [1] -161.0713 # Extreme positive value (mu + k * stdev) ## [1] 220.9202 # This statement asks whether the value in x is an extreme value # The operator &#39;|&#39; is &#39;OR&#39; # Is x extreme negative OR extreme positive? x &lt; (mu - k * stdev) | x &gt; (mu + k * stdev) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [61] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [97] FALSE FALSE FALSE FALSE # We can get the frequencies of this logical vector using table table(x &lt; (mu - k * stdev) | x &gt; (mu + k * stdev)) ## ## FALSE TRUE ## 99 1 # Or, as Edge, does, calculate the average of TRUEs - which is simply the proportion of TRUEs mean(x &lt; (mu - k * stdev) | x &gt; (mu + k * stdev)) ## [1] 0.01 # What proportion/probability of TRUEs would we expect under a normal probability distribution? pnorm(k) # probability of observing a value less than k standard deviations above the mean ## [1] 0.9772499 pnorm(-k) # probability of observing a value less than k standard deviations below the mean ## [1] 0.02275013 (1 - (pnorm(k) - pnorm(-k))) # probability of observing an extreme value ## [1] 0.04550026 # So putting it all together, we have the ratio of: # the probability of observing an extreme value in the data, over the # the probability of observing an extreme value in a normal distribution: mean(x &lt; (mu - k * stdev) | x &gt; (mu + k * stdev))/(1 - (pnorm(k) - pnorm(-k))) ## [1] 0.2197789 compare.tail.to.normal(x = x, k = k, mu = mu, sigma = stdev) ## [1] 0.2197789 # If this ratio is &lt; 1, then the data have fewer extreme values than suggested by a normal # If this ratio is &gt; 1, then the data have more extreme values than suggested by a normal Above, I haven’t computed the means of many simulations - which is the crux of the question! So here I just paste Edge’s solution. In it, he calculates \\(E(X)\\) and \\(Var(X)\\) using the Pareto probability distribution. I have changed n and n.sim to match my values above. #Sample size per simulation (n) and number of simulations. n &lt;- 100 n.sim &lt;- 10000 #Pareto parameters. Variance is finite, and so #CLT applies, if a &gt; 2. For large a, convergence to #normal is better. With small a, convergence is slow, #especially in the tails. a &lt;- 4 b &lt;- 1 #Compute the expectation and variance of the distribution #of the sample mean. a must be above 2 for these expressions #to hold. expec.par &lt;- a*b/(a-1) var.par &lt;- a*b^2 / ((a-1)^2 * (a-2)) sd.mean &lt;- sqrt(var.par / n) #Simulate data sim &lt;- matrix(rpareto(n*n.sim, a, b), nrow = n.sim) # Each column represents ith sample taken per simulation # Each row represents a different simulation sim[1:3, 1:10] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 1.146496 1.029705 1.334946 1.060409 1.013868 1.038984 1.804409 1.299151 ## [2,] 1.342433 1.205436 1.003869 3.416952 2.248103 1.023516 1.006624 1.044363 ## [3,] 1.076866 1.027499 1.061844 1.071026 1.425200 1.069973 1.456830 1.757125 ## [,9] [,10] ## [1,] 1.153548 1.276303 ## [2,] 1.074918 1.304583 ## [3,] 1.042708 1.099849 # Compute sample means. means.sim &lt;- rowMeans(sim) str(means.sim) ## num [1:10000] 1.38 1.34 1.33 1.28 1.34 ... #Draw a histogram of the sample means along with the approximate #normal pdf that follows from the CLT. hist(means.sim, prob = TRUE) curve(dnorm(x, expec.par, sd.mean), add = TRUE, col = &#39;red&#39;) compare.tail.to.normal(means.sim, 1/2, expec.par, sd.mean) ## [1] 0.9639022 compare.tail.to.normal(means.sim, 1, expec.par, sd.mean) ## [1] 0.9407189 compare.tail.to.normal(means.sim, 2, expec.par, sd.mean) ## [1] 0.9384561 compare.tail.to.normal(means.sim, 3, expec.par, sd.mean) ## [1] 2.18535 compare.tail.to.normal(means.sim, 4, expec.par, sd.mean) ## [1] 25.25951 compare.tail.to.normal(means.sim, 5, expec.par, sd.mean) ## [1] 348.8556 compare.tail.to.normal(means.sim, 6, expec.par, sd.mean) ## [1] 0 5.6 A probabilistic model for simple linear regression In this last section, we are going to apply the probabilistic concepts to our linear function: \\[ \\begin{aligned} Y = \\alpha + \\beta X + \\epsilon \\\\ \\end{aligned} \\] \\(X, Y, \\epsilon\\): random variables (probability distributions) \\(\\alpha, \\beta\\): fixed coefficients (scalars) In words, we are trying to model \\(Y\\) as a linear function of \\(\\alpha, \\beta, X\\), plus a random disturbance, \\(\\epsilon\\). In what follows, we are going to make some simplifying assumptions about the random variables \\(X\\) and \\(\\epsilon\\). Why? Because in doing so, we can make some claims about \\(Y\\) - its expectation, and variance; as well as Cov(\\(X, Y\\)). Why is it useful to make these claims? Because in doing so, we will be able to decompose the variance of \\(Y\\) into two components: the effect of \\(X\\) on \\(Y\\), and the disturbance \\(\\epsilon\\). That is, we’ll be able to generate predictions for \\(Y\\) given \\(X\\) - maybe we can predict \\(Y\\) really well, or maybe not. I will briefly restate the assumptions below. 5.6.1 Assumptions of the linear model \\(X\\) has a known expectation, and the disturbance term has an expectation of 0: \\[ \\begin{aligned} \\text{E}(X) =&amp; ~ \\mu_X \\\\ \\text{E}(\\epsilon) =&amp; ~ 0 \\\\ \\end{aligned} \\] Consequence: we know the expectation of \\(Y\\) The expectation of the disturbance term is 0 for all values of \\(X\\): \\[ \\begin{aligned} \\text{E}(\\epsilon | X = x) =&amp; ~ 0 \\\\ \\end{aligned} \\] Consequence: the conditional expectation of \\(Y\\), given any \\(x\\), can be predicted using a line with a slope \\(\\beta\\) and intercept \\(\\alpha\\). Let that sink in…if this is not true, then the relationship between X and Y is not linear! The variance of the disturbance is constant, for every \\(x\\): \\[ \\begin{aligned} \\text{Var}(\\epsilon | X = x) =&amp; ~ \\sigma^2_\\epsilon \\\\ \\end{aligned} \\] Consequence: the variance of Y, for any x, is constant \\(X\\) and \\(\\epsilon\\) are independent. Consequence: the variance of Y is due to the variation in X, plus the variation due to the disturbance 5.6.2 Important claims that follow from these assumptions The correlation between \\(X\\) and \\(Y\\) is: \\[ \\begin{aligned} \\rho_{X, Y} = \\beta \\frac{\\sigma_X}{\\sigma_Y}\\\\ \\end{aligned} \\] The proportion of variance in \\(Y\\) that is explained by \\(X\\) (\\(r^2\\)) is: \\[ \\begin{aligned} \\rho_{X, Y}^2 = 1 - \\frac{\\text{Var}(Y|X = x)}{\\text{Var}(Y)}\\\\ \\end{aligned} \\] 5.6.3 Checking these assumptions So you have run a linear model in R using lm(). Now what? Check your assumptions, of course! The most important assumptions are linearity (# 2 above) and homoscedasticity (#3 above). To check these, plot the residuals of your model against the fitted values. plot(y1 ~ x1, data = anscombe) lm1 &lt;- lm(y1 ~ x1, data = anscombe) plot(lm1, which = 1) There should be no trend in the residuals (indicative of linearity), and the scatter of the residuals should be consistent across the range of fitted values (indicative of homoscedasticity). In the plot above, both of these assumptions are satisfied. Hopefully your residuals will look like this (if not, you have more work to do!). Here is a figure (7.5) from Faraway (2002) that illustrates clear violations of these two assumptions: 5.6.4 Exercise set 5-5 Write the square of the correlation coefficient (eq. 5.30) in terms of the variance of Y (eq. 5.32) and the conditional variance of Y given X (eq. 5.31). \\[ \\begin{aligned} \\text{eq. 5.30: } \\rho_{X,Y} =&amp; \\beta \\frac{\\sigma_X}{\\sigma_Y} \\\\ \\text{eq. 5.31: } Var(Y) =&amp; \\beta^2 \\sigma_X^2 + \\sigma_{\\epsilon}^2 \\\\ \\text{eq. 5.32: } Var(Y \\mid X = x) =&amp; \\sigma_{\\epsilon}^2 \\\\ \\end{aligned} \\] Squaring \\(\\rho_{X,Y}\\), and expressing \\(Var(Y)\\) using the definition from above: \\[ \\begin{aligned} \\rho_{X,Y}^2 = \\beta^2 \\frac{\\sigma_X^2}{\\sigma_Y^2} = \\beta^2 \\frac{\\sigma_X^2}{Var(Y)} \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\rho_{X,Y}^2 = \\beta^2 \\frac{\\sigma_X^2}{\\beta^2 \\sigma_X^2 + \\sigma_{\\epsilon}^2} \\\\ \\end{aligned} \\] Now, we have to employ an algebraic slight of hand. We rewrite the terms on the right as a single fraction, and by adding and subtracting the term \\(\\sigma_{\\epsilon}^2\\) to the numerator, the numerator remains unchanged, but in a useful form to separate the single fraction into two, with the lefthand fraction equaling 1: \\[ \\begin{aligned} \\rho_{X,Y}^2 =&amp; \\frac{\\beta^2 \\sigma_X^2}{\\beta^2 \\sigma_X^2 + \\sigma_{\\epsilon}^2} \\\\ =&amp; \\frac{(\\beta^2 \\sigma_X^2 + \\sigma_{\\epsilon}^2) - \\sigma_{\\epsilon}^2}{\\beta^2 \\sigma_X^2 + \\sigma_{\\epsilon}^2} \\\\ =&amp; \\frac{\\beta^2 \\sigma_X^2 + \\sigma_{\\epsilon}^2}{\\beta^2 \\sigma_X^2 + \\sigma_{\\epsilon}^2} - \\frac{\\sigma_{\\epsilon}^2}{\\beta^2 \\sigma_X^2 + \\sigma_{\\epsilon}^2} \\\\ =&amp; 1 - \\frac{\\sigma_{\\epsilon}^2}{\\beta^2 \\sigma_X^2 + \\sigma_{\\epsilon}^2} \\\\ \\end{aligned} \\] And we use the formulas from above again to restate as: \\[ \\begin{aligned} \\rho_{X,Y}^2 =&amp; 1 - \\frac{Var(Y \\mid X = x)}{Var(Y)} \\\\ \\end{aligned} \\] which gives us the ‘proportion of variance explained’. So if there isn’t much variance left in \\(Y\\) after conditioning on \\(X\\) (i.e., the numerator is small relative to the denominator), if we subtract it from 1, we get a high \\(r^2\\). And vice versa. Simulating a regression. library(stfspack) sim.lm ## function (n, a, b, sigma.disturb = 1, mu.x = 8, sigma.x = 2, ## rdisturb = rnorm, rx = rnorm, het.coef = 0) ## { ## x &lt;- sort(rx(n, mu.x, sigma.x)) ## disturbs &lt;- rdisturb(n, 0, sapply(sigma.disturb + scale(x) * ## het.coef, max, 0)) ## y &lt;- a + b * x + disturbs ## cbind(x, y) ## } ## &lt;bytecode: 0x7fa569fa7860&gt; ## &lt;environment: namespace:stfspack&gt; sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1) head(sim_0_1) ## x y ## [1,] 3.427344 2.949380 ## [2,] 3.616894 3.644965 ## [3,] 3.815677 4.193751 ## [4,] 3.912910 3.364430 ## [5,] 4.458928 6.025582 ## [6,] 5.078764 3.985045 plot(sim_0_1[,1], sim_0_1[,2]) Still using all the default values for parameters, but I have made all the arguments explicit in the function call: sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1, sigma.disturb = 1, mu.x = 8, sigma.x = 2, rdisturb = rnorm, rx = rnorm, het.coef = 0) plot(sim_0_1[,1], sim_0_1[,2]) Now I’ll change one at a time. Here, I’ve doubled sigma.disturb, the error term: sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1, sigma.disturb = 2, mu.x = 8, sigma.x = 2, rdisturb = rnorm, rx = rnorm, het.coef = 0) plot(sim_0_1[,1], sim_0_1[,2]) Next, I have doubled E(\\(X\\)): sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1, sigma.disturb = 1, mu.x = 16, sigma.x = 2, rdisturb = rnorm, rx = rnorm, het.coef = 0) plot(sim_0_1[,1], sim_0_1[,2]) Next, I have doubled \\(\\sigma_X\\): sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1, sigma.disturb = 1, mu.x = 8, sigma.x = 4, rdisturb = rnorm, rx = rnorm, het.coef = 0) plot(sim_0_1[,1], sim_0_1[,2]) Finally, here I have changed the distribution of the error term to a laplace distribution: sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1, sigma.disturb = 1, mu.x = 8, sigma.x = 2, rdisturb = rlaplace, rx = rnorm, het.coef = 0) plot(sim_0_1[,1], sim_0_1[,2]) "],
["estimators.html", "Chapter 6 Properties of point estimators 6.1 Bias 6.2 Variance 6.3 Mean squared error 6.4 Consistency 6.5 Efficiency 6.6 Statistical decision theory and risk 6.7 Robustness 6.8 Estimators for simple linear regression", " Chapter 6 Properties of point estimators Estimator (\\(\\hat{\\theta}\\)): a function that can be applied to data to produce estimates of a true quantity Estimate (\\(\\theta\\)): the value of a quantity returned by an estimator applied to data (e.g., a sample mean, a parameter of a probability distribution, a regression coefficient) Estimand (\\(\\theta\\)): the quantity we are trying to estimate Before we see the data, they can be represented as a random variable, \\(D\\). After we see the data, they are no longer random, and represented by \\(d\\). If we apply an estimator to \\(D\\), the result is \\(\\hat{\\theta}(D)\\), which can vary from one dataset to the next. The distribution of an estimator is sometimes called the sampling distribution. The sample size of \\(D\\) will affect the estimate, and we use \\(n\\) to denote the sample size in the expression, \\(\\hat{\\theta}_n(D)\\). In the first example, we apply the following estimator to \\(n\\) i.i.d. random variables \\(X_i\\), which are normally distributed with a mean \\(\\theta\\) and variance of 1 (\\(X_i \\sim N(\\theta, 1)\\)): \\[ \\begin{aligned} \\hat{\\theta}_n(D) = \\frac{1}{n} \\sum_{i = 1}^n X_i \\\\ \\end{aligned} \\] That is, we would like to estimate the mean of our random variables, \\(X_i\\). We can do this in R, for 25 random variables that have a mean of 0 and a variance of 1: set.seed(12) d &lt;- rnorm(n = 25, mean = 0, sd = 1) mean(d) ## [1] -0.1883626 6.1 Bias The bias of an estimator is the difference between the true value of the quantity we’re interested in (e.g., \\(\\theta\\)), and the expectation of our estimator (\\(E[\\hat{\\theta}_n(D)]\\)). That is, our estimator is unbiased when it does not consistently over- or under- estimate the expectation; that is to say, it is accurate. Formally, the bias is: \\[ \\begin{aligned} \\text{B}(\\hat{\\theta}_n) = E[\\hat{\\theta}_n(D)] - \\theta \\\\ \\end{aligned} \\] We prefer estimators whose bias is 0 (i.e., unbiased). 6.1.1 Exercise set 6-1 The bias of the sample mean for \\(X_i \\sim N(\\theta, 1)\\) is 0. See handwritten notes. Examining estimators for a normal distribution: library(stfspack) s.mat &lt;- mat.samps(n = 25, nsim = 10000) str(s.mat) ## num [1:10000, 1:25] 0.435 -1.023 -1.746 1.36 -0.642 ... ## Sampling distribution of means ests.mean &lt;- apply(s.mat, 1, mean) mean_of_means &lt;- mean(ests.mean) ## Sampling distribution of medians ests.median &lt;- apply(s.mat, 1, median) mean_of_medians &lt;- mean(ests.median) par(mfrow = c(1,2)) hist(ests.mean) abline(v = mean_of_means, col = &quot;red&quot;) hist(ests.median) abline(v = mean_of_medians, col = &quot;red&quot;) The mean and the median both appear unbiased - that is, their sampling distributions are centered on 0. 6.2 Variance An estimator’s variance measures the precision of the estimator. We define it in the same way that we defined the variance of a random variable: \\[ \\begin{aligned} \\text{Var}(\\hat{\\theta}_n) =&amp; ~ E[(\\hat{\\theta}_n(D) - E[\\hat{\\theta}_n(D)])^2] \\\\ =&amp; ~ E[(\\hat{\\theta}_n(D)^2] - (E[\\hat{\\theta}_n(D)])^2 \\\\ \\end{aligned} \\] 6.2.1 Exercise set 6-2 The variance of the sample mean for \\(X_i \\sim N(\\theta, 1)\\) is \\(\\frac{1}{n}\\). If we don’t know the distribution from which \\(X_i\\) are drawn, just that it has a finite variance, \\(\\sigma^2\\), the variance of the sample means is \\(\\frac{\\sigma^2}{n}\\). See handwritten notes. Does the sample median have a larger or smaller variance than the mean, when data are drawn from a normal distribution? library(stfspack) library(tidyverse) n_sims &lt;- 10000 s.mat &lt;- mat.samps(n = 25, nsim = n_sims) str(s.mat) ## num [1:10000, 1:25] 1.414 0.468 -0.785 -0.257 -0.205 ... ## Sampling distribution of means ests.mean &lt;- apply(s.mat, 1, mean) var_of_means &lt;- var(ests.mean) ## Sampling distribution of medians ests.median &lt;- apply(s.mat, 1, median) var_of_medians &lt;- var(ests.median) df &lt;- data.frame(estimator = c(rep(&quot;mean&quot;, n_sims), rep(&quot;median&quot;, n_sims)), value = c(ests.mean, ests.median)) df %&gt;% ggplot(aes(value, fill = estimator)) + geom_density(alpha = 0.3) The sample median has a larger variance than the sample mean. 6.3 Mean squared error The mean squared error is a metric that combines bias (accuracy) and variance (precision) in one metric. It is the expected squared difference between the value of an estimator and the true quantity: \\[ \\begin{aligned} \\text{MSE}(\\hat{\\theta}_n) =&amp; ~ E[(\\hat{\\theta}_n(D) - \\theta)^2] \\\\ =&amp; ~ \\text{B}(\\hat{\\theta}_n)^2 + \\text{Var}(\\hat{\\theta}_n) \\end{aligned} \\] 6.3.1 Exercise set 6-3 See handwritten notes or Edge solution. When estimating the first parameter of a normal distribution, the sample mean will have a lower mean squared error. This is because although both are unbiased estimators, the sample median has a higher variance. If bias for both is equal to zero, then the mean square error is simply the variance of the estimator. 6.4 Consistency As we collect more data, it would be preferable for the estimator to get closer and closer to the estimand; that is, we wish the estimator to be consistent. A consistent estimator is one that converges in probability to the true value, and is defined formally as: \\[ \\begin{aligned} \\text{lim}_{n \\rightarrow \\infty} \\text{P}(|\\hat{\\theta}_n - \\theta| &gt; \\delta) = 0 \\end{aligned} \\] where \\(\\delta\\) is any positive number. It turns out (if we complete a proof) that an estimator \\(\\hat{\\theta}_n\\) is consistent if: \\[ \\begin{aligned} \\text{lim}_{n \\rightarrow \\infty} \\text{MSE}(\\hat{\\theta}_n) = 0 \\end{aligned} \\] that is, an estimator is consistent if the mean squared error goes to zero as the sample size increases to infinity. Both biased and unbiased estimators can be consistent, Edge’s figure 6.2 demonstrates this nicely. 6.4.1 Exercise set 6-4 Yes, the sample mean is consistent as an estimator of the first parameter (the population mean) of a normal distribution. We know this because the MSE is the sum of the bias squared and the variance. As \\(n\\) increases, variance goes to zero (because \\(\\frac{\\sigma^2}{n}\\)). And we already showed (in ex. 6-1-2) that at large n, the bias of the sample mean is zero (this is also true at small n). The first parameter of a normal distribution is also its expectation - so can we extend the sample mean as a consistent estimator of any distribution, as long as it has a finite variance? Yes, because the law of large numbers applies to any distribution. See Edge explanation. Use simulations to show that the sample median is a consistent estimator of the first parameter of a normal distribution. We already know that the sample median is unbiased, so the following simulations focus on the variance. library(stfspack) library(tidyverse) n_sims &lt;- 1000 n_samps &lt;- 25 get_var_of_median &lt;- function(n_sims, n_samps){ s.mat &lt;- mat.samps(n = n_samps, nsim = n_sims) ests.median &lt;- apply(s.mat, 1, median) var_of_medians &lt;- var(ests.median) return(var_of_medians) } n_samps_vector &lt;- c(5, 10, 25, 50, 100, 200, 500) vector_length &lt;- length(n_samps_vector) var_vector &lt;- rep(NA, vector_length) for(i in 1:vector_length){ var_vector[i] &lt;- get_var_of_median(n_sims, n_samps = n_samps_vector[i]) } plot(n_samps_vector, var_vector, col = &quot;red&quot;, pch = 20, xlab = &quot;Number of samples&quot;, ylab = &quot;Variance of medians&quot;, type = &quot;b&quot;) abline(h = 0, lty = 2, col = &quot;gray&quot;) Considering \\(n\\) i.i.d. random variables \\(X_i\\), which are normally distributed with a mean \\(\\theta\\) and variance of 1 (\\(X_i \\sim N(\\theta, 1)\\)), are the following estimators (i) unbiased; and (ii) consistent? The sample mean is unbiased and consistent, we have shown these already. A shifted sample mean will be biased (e.g., because we are adding a constant to each value prior to taking the average), and will not be consistent because the mean squared error will never approach 0 (because it is biased). Also see Edge’s explanation. The first observation, \\(X_1\\) will be an unbiased estimator because it has an expectation of \\(\\theta\\). It has a variance of 1, and thus it is not consistent (because the variance does not approach zero as the number of samples increases). A ‘shrunk’ sample mean is biased and consistent; see handwritten notes for solution. Prove that if Eq. 6.7 holds, then Eq. 6.6 also holds. SKIPPED; see Edge for solution. 6.5 Efficiency In addition to consistency, we’d like it if we our estimator was efficient - that is, it takes fewer samples to achieve a desired level of performance. One way to define the relative efficiency of two estimators is: \\[ \\begin{aligned} \\text{RE}(\\hat{\\theta}_n, \\tilde{\\theta}_n) =&amp; ~ \\frac{\\text{MSE}(\\hat{\\theta}_n)} {\\text{MSE}(\\tilde{\\theta}_n)} \\\\ \\end{aligned} \\] Check out Edge’s Figure 6-3 for a visual. We can also consider the relative efficiency as the sample size goes to infinity, otherwise known as the asymptotic relative efficiency: \\[ \\begin{aligned} \\text{ARE}(\\hat{\\theta}_n, \\tilde{\\theta}_n) =&amp; ~ \\text{lim}_{n \\rightarrow \\infty} \\frac{\\text{MSE}(\\hat{\\theta}_n)} {\\text{MSE}(\\tilde{\\theta}_n)} \\\\ \\end{aligned} \\] 6.5.1 Exercise set 6-5 Exploring the relative efficiency of the mean and median for normal samples of different sizes. Consider a Normal(0, 1) distribution. Estimate MSE using 10000 normal samples of size 5. mu &lt;- 0 s.mat &lt;- mat.samps(n = 5, nsim = 10000) ests.mean &lt;- apply(s.mat, 1, mean) ests.median &lt;- apply(s.mat, 1, median) #The relative efficiency is estimated as the quotient of the #MSEs. The relative efficiency of the sample mean vs. the #sample median has the MSE of the sample mean in the #denominator. re &lt;- mean((ests.median - mu)^2)/mean((ests.mean - mu)^2) re ## [1] 1.446713 Repeat for different sample sizes: n &lt;- c(2,5,10,20,50,100,200,500) nsims &lt;- 2000 mu &lt;- 0 sigma &lt;- 1 re &lt;- numeric(length(n)) for(i in 1:length(n)){ x &lt;- mat.samps(n = n[i], nsim = nsims, mean = mu, sd = sigma) ests.median &lt;- apply(x, 1, median) ests.mean &lt;- apply(x, 1, mean) re[i] &lt;- mean((ests.median - mu)^2)/mean((ests.mean - mu)^2) } plot(n, re, xlab = &quot;sample size&quot;, ylab = &quot;Relative efficiency&quot;, main = &quot;Sample mean vs. median for normal data&quot;) Repeat problem 1, but using the Laplace distribution. n &lt;- c(2,5,10,20,50,100,200,500) nsims &lt;- 2000 mu &lt;- 0 sigma &lt;- 1 re &lt;- numeric(length(n)) for(i in 1:length(n)){ x &lt;- mat.samps(n = n[i], nsim = nsims, mu = mu, sigma = sigma, rx = rlaplace) ests.median &lt;- apply(x, 1, median) ests.mean &lt;- apply(x, 1, mean) re[i] &lt;- mean((ests.median - mu)^2)/mean((ests.mean - mu)^2) } plot(n, re, xlab = &quot;sample size&quot;, ylab = &quot;Relative efficiency&quot;, main = &quot;Sample mean vs. median for heavy tailed data&quot;) 6.6 Statistical decision theory and risk Skimmed; skipped exercises. 6.7 Robustness So far, we have implicitly assumed that we are confident in our model - for example, that a normal distribution represents the data generating process. But what if we were wrong? This may be the case, or at least, there are often competing data generating processes for the same set of observed data. In this reality, we value a statistical procedure that is robust - that is, it continues to give approximately correct answers even when the underlying assumptions are incorrect. Robust against what? This needs defining. For example, we might consider an estimator as robust against: outliers (e.g., the median) misspecification of the probability distribution (e.g., the assumption of normality) 6.7.1 Exercise set 6-8 Estiming \\(\\theta\\) using independent samples from a Normal(\\(\\theta\\), 1) distribution. But our observations might be contaminated by data drawn from a different distribution. rnorm.contam ## function (n, mu = 0, sigma = 1, contam.p = 0.01, contam.mu = -5, ## contam.sigma = 0) ## { ## ncontam &lt;- rbinom(1, n, contam.p) ## c(rnorm(n - ncontam, mu, sigma), rnorm(ncontam, contam.mu, ## contam.sigma)) ## } ## &lt;bytecode: 0x7fb67200c120&gt; ## &lt;environment: namespace:stfspack&gt; dat &lt;- mat.samps(n = 100, nsim = 1000, rx = rnorm.contam, contam.p = 0.1, contam.mu = 300, contam.sigma = 1) means &lt;- apply(dat,2,mean) medians &lt;- apply(dat,2,median) mean(means) ## [1] 29.99532 var(means) ## [1] 9.049853 mean(medians) ## [1] 0.1431885 var(medians) ## [1] 0.001828098 par(mfrow = c(1,2)) hist(means, breaks = 10) hist(medians, breaks = 10) 6.8 Estimators for simple linear regression 6.8.1 Exercise set 6-9 Exploring the properties of least-squares and least-absolute-errors lines as estimators of a linear regression model with disturbance terms that are distributed normally (and constant variance). sim.lm ## function (n, a, b, sigma.disturb = 1, mu.x = 8, sigma.x = 2, ## rdisturb = rnorm, rx = rnorm, het.coef = 0) ## { ## x &lt;- sort(rx(n, mu.x, sigma.x)) ## disturbs &lt;- rdisturb(n, 0, sapply(sigma.disturb + scale(x) * ## het.coef, max, 0)) ## y &lt;- a + b * x + disturbs ## cbind(x, y) ## } ## &lt;bytecode: 0x7fb66bfd7cb0&gt; ## &lt;environment: namespace:stfspack&gt; ests &lt;- sim.lm.ests(n = 50, nsim = 1000, a = 3, b = 0.5) colMeans(ests) ## [1] 3.0015118 0.4992189 apply(ests, 2, var) ## [1] 0.39426172 0.00573011 # Histogram of slope estimates hist(ests[,2]) Simulate samples of 10, 50, 100, and 1000. Describe the bias and consistency of these least-squares estimators. n_sims &lt;- 1000 n_samps &lt;- c(10, 50, 100, 500, 1000) matrix_means &lt;- matrix(data = NA, nrow = length(n_samps), ncol = 2) matrix_var &lt;- matrix(data = NA, nrow = length(n_samps), ncol = 2) for(i in 1:length(n_samps)){ ests &lt;- sim.lm.ests(n = n_samps[i], nsim = n_sims, a = 3, b = 0.5) matrix_means[i, ] &lt;- colMeans(ests) matrix_var[i, ] &lt;- apply(ests, 2, var) } par(mfrow = c(2,2)) plot(n_samps, matrix_means[,1], ylab = &quot;a&quot;, main = &quot;Mean&quot;) plot(n_samps, matrix_means[,2], ylab = &quot;b&quot;, main = &quot;Mean&quot;) plot(n_samps, matrix_var[,1], ylab = &quot;a&quot;, main = &quot;Variance&quot;) plot(n_samps, matrix_var[,2], ylab = &quot;b&quot;, main = &quot;Variance&quot;) matrix_var_LSE &lt;- matrix_var There appears to be no bias, because the sample means of the coefficients (a and b) are very close to the true values (3 and 0.5). The estimators are consistent, because the variance decreases to zero for both a and b (and there is no bias). Same exercise, but using quantile regression (i.e., the least-absolute-errors line). library(quantreg) n_sims &lt;- 1000 n_samps &lt;- c(10, 50, 100, 500, 1000) matrix_means &lt;- matrix(data = NA, nrow = length(n_samps), ncol = 2) matrix_var &lt;- matrix(data = NA, nrow = length(n_samps), ncol = 2) set.seed(1201) for(i in 1:length(n_samps)){ ests &lt;- sim.lm.ests(n = n_samps[i], nsim = n_sims, a = 3, b = 0.5, estfun = rq) matrix_means[i, ] &lt;- colMeans(ests) matrix_var[i, ] &lt;- apply(ests, 2, var) } par(mfrow = c(2,2)) plot(n_samps, matrix_means[,1], ylab = &quot;a&quot;, main = &quot;Mean&quot;) plot(n_samps, matrix_means[,2], ylab = &quot;b&quot;, main = &quot;Mean&quot;) plot(n_samps, matrix_var[,1], ylab = &quot;a&quot;, main = &quot;Variance&quot;) plot(n_samps, matrix_var[,2], ylab = &quot;b&quot;, main = &quot;Variance&quot;) matrix_var_LAE &lt;- matrix_var In some simulations, the least-absolute-errors line appears to be biased at very low sample sizes (e.g., 10). But this is just sampling variation, and I set the seed above to illustrate that there is no bias and this estimator is also consistent. Which estimator is more efficient? n_sims &lt;- 1000 n_samps &lt;- c(10, 50, 100, 500, 1000) matrix_var &lt;- matrix(data = NA, nrow = length(n_samps), ncol = 2) set.seed(1201) for(i in 1:length(n_samps)){ ests &lt;- sim.lm.ests(n = n_samps[i], nsim = n_sims, a = 3, b = 0.5) matrix_var[i, ] &lt;- apply(ests, 2, var) } matrix_var_LSE &lt;- matrix_var set.seed(1201) for(i in 1:length(n_samps)){ ests &lt;- sim.lm.ests(n = n_samps[i], nsim = n_sims, a = 3, b = 0.5, estfun = rq) matrix_var[i, ] &lt;- apply(ests, 2, var) } matrix_var_LAE &lt;- matrix_var par(mfrow = c(2,2)) plot(n_samps, matrix_var_LSE[,1], ylab = &quot;a&quot;, main = &quot;Variance (least-squares)&quot;) plot(n_samps, matrix_var_LSE[,2], ylab = &quot;b&quot;, main = &quot;Variance (least-squares)&quot;) plot(n_samps, matrix_var_LAE[,1], ylab = &quot;a&quot;, main = &quot;Variance (least-absolute)&quot;) plot(n_samps, matrix_var_LAE[,2], ylab = &quot;b&quot;, main = &quot;Variance (least-absolute)&quot;) par(mfrow = c(1,2)) plot(n_samps, matrix_var_LSE[,1] / matrix_var_LAE[,1], ylab = &quot;a&quot;, main = &quot;RE (least-squares / least-absolute)&quot;) plot(n_samps, matrix_var_LSE[,2] / matrix_var_LAE[,2], ylab = &quot;b&quot;, main = &quot;RE (least-squares / least-absolute)&quot;) The least-squares estimator is more efficient; that is, it has a lower variance than the least-absolute estimator. Use a heavy tailed distribution (Laplace) for the disturbance term. Simulate some datasets and view: par(mfrow = c(2,2)) plot(sim.lm(n = 50, a = 3, b = 0.5, rdisturb = rlaplace)) plot(sim.lm(n = 50, a = 3, b = 0.5, rdisturb = rlaplace)) plot(sim.lm(n = 50, a = 3, b = 0.5, rdisturb = rlaplace)) plot(sim.lm(n = 50, a = 3, b = 0.5, rdisturb = rlaplace)) Repeat problem 1 with a laplace distribution. n_sims &lt;- 1000 n_samps &lt;- c(10, 50, 100, 500, 1000) matrix_means &lt;- matrix(data = NA, nrow = length(n_samps), ncol = 2) matrix_var &lt;- matrix(data = NA, nrow = length(n_samps), ncol = 2) set.seed(1201) for(i in 1:length(n_samps)){ ests &lt;- sim.lm.ests(n = n_samps[i], nsim = n_sims, a = 3, b = 0.5, rdisturb = rlaplace) matrix_means[i, ] &lt;- colMeans(ests) matrix_var[i, ] &lt;- apply(ests, 2, var) } matrix_means_LSE &lt;- matrix_means matrix_var_LSE &lt;- matrix_var matrix_means &lt;- matrix(data = NA, nrow = length(n_samps), ncol = 2) matrix_var &lt;- matrix(data = NA, nrow = length(n_samps), ncol = 2) set.seed(1201) for(i in 1:length(n_samps)){ ests &lt;- sim.lm.ests(n = n_samps[i], nsim = n_sims, a = 3, b = 0.5, estfun = rq, rdisturb = rlaplace) matrix_means[i, ] &lt;- colMeans(ests) matrix_var[i, ] &lt;- apply(ests, 2, var) } matrix_means_LAE &lt;- matrix_means matrix_var_LAE &lt;- matrix_var par(mfrow = c(2,2)) plot(n_samps, matrix_means_LSE[,1], ylab = &quot;a&quot;, main = &quot;Mean (least-squares)&quot;) plot(n_samps, matrix_means_LSE[,2], ylab = &quot;b&quot;, main = &quot;Mean (least-squares)&quot;) plot(n_samps, matrix_means_LAE[,1], ylab = &quot;a&quot;, main = &quot;Mean (least-absolute)&quot;) plot(n_samps, matrix_means_LAE[,2], ylab = &quot;b&quot;, main = &quot;Mean (least-absolute)&quot;) par(mfrow = c(2,2)) plot(n_samps, matrix_var_LSE[,1], ylab = &quot;a&quot;, main = &quot;Variance (least-squares)&quot;) plot(n_samps, matrix_var_LSE[,2], ylab = &quot;b&quot;, main = &quot;Variance (least-squares)&quot;) plot(n_samps, matrix_var_LAE[,1], ylab = &quot;a&quot;, main = &quot;Variance (least-absolute)&quot;) plot(n_samps, matrix_var_LAE[,2], ylab = &quot;b&quot;, main = &quot;Variance (least-absolute)&quot;) par(mfrow = c(1,2)) plot(n_samps, matrix_var_LSE[,1] / matrix_var_LAE[,1], ylab = &quot;a&quot;, main = &quot;RE (least-squares / least-absolute)&quot;) plot(n_samps, matrix_var_LSE[,2] / matrix_var_LAE[,2], ylab = &quot;b&quot;, main = &quot;RE (least-squares / least-absolute)&quot;) Both estimators are unbiased when using a heavy tailed distribution for the error term. However, the least-absolute estimator is more efficient than the the least-squares estimator, especially as sample size increases. Examining robustness to data contamination. Simulate some datasets and view: par(mfrow = c(2,2)) plot(sim.lm(n = 100, a = 3, b = 0.5, rdisturb = rnorm.contam)) plot(sim.lm(n = 100, a = 3, b = 0.5, rdisturb = rnorm.contam)) plot(sim.lm(n = 100, a = 3, b = 0.5, rdisturb = rnorm.contam)) plot(sim.lm(n = 100, a = 3, b = 0.5, rdisturb = rnorm.contam)) Repeat problem 1 with 1% contamination. n_sims &lt;- 1000 n_samps &lt;- c(10, 50, 100, 500, 1000) matrix_means &lt;- matrix(data = NA, nrow = length(n_samps), ncol = 2) matrix_var &lt;- matrix(data = NA, nrow = length(n_samps), ncol = 2) set.seed(1201) for(i in 1:length(n_samps)){ ests &lt;- sim.lm.ests(n = n_samps[i], nsim = n_sims, a = 3, b = 0.5, rdisturb = rnorm.contam) matrix_means[i, ] &lt;- colMeans(ests) matrix_var[i, ] &lt;- apply(ests, 2, var) } matrix_means_LSE &lt;- matrix_means matrix_var_LSE &lt;- matrix_var matrix_means &lt;- matrix(data = NA, nrow = length(n_samps), ncol = 2) matrix_var &lt;- matrix(data = NA, nrow = length(n_samps), ncol = 2) set.seed(1201) for(i in 1:length(n_samps)){ ests &lt;- sim.lm.ests(n = n_samps[i], nsim = n_sims, a = 3, b = 0.5, estfun = rq, rdisturb = rnorm.contam) matrix_means[i, ] &lt;- colMeans(ests) matrix_var[i, ] &lt;- apply(ests, 2, var) } matrix_means_LAE &lt;- matrix_means matrix_var_LAE &lt;- matrix_var par(mfrow = c(2,2)) plot(n_samps, matrix_means_LSE[,1], ylab = &quot;a&quot;, main = &quot;Mean (least-squares)&quot;) plot(n_samps, matrix_means_LSE[,2], ylab = &quot;b&quot;, main = &quot;Mean (least-squares)&quot;) plot(n_samps, matrix_means_LAE[,1], ylab = &quot;a&quot;, main = &quot;Mean (least-absolute)&quot;) plot(n_samps, matrix_means_LAE[,2], ylab = &quot;b&quot;, main = &quot;Mean (least-absolute)&quot;) par(mfrow = c(2,2)) plot(n_samps, matrix_var_LSE[,1], ylab = &quot;a&quot;, main = &quot;Variance (least-squares)&quot;) plot(n_samps, matrix_var_LSE[,2], ylab = &quot;b&quot;, main = &quot;Variance (least-squares)&quot;) plot(n_samps, matrix_var_LAE[,1], ylab = &quot;a&quot;, main = &quot;Variance (least-absolute)&quot;) plot(n_samps, matrix_var_LAE[,2], ylab = &quot;b&quot;, main = &quot;Variance (least-absolute)&quot;) par(mfrow = c(1,2)) plot(n_samps, matrix_var_LSE[,1] / matrix_var_LAE[,1], ylab = &quot;a&quot;, main = &quot;RE (least-squares / least-absolute)&quot;) plot(n_samps, matrix_var_LSE[,2] / matrix_var_LAE[,2], ylab = &quot;b&quot;, main = &quot;RE (least-squares / least-absolute)&quot;) Looks like both estimators are biases (a is too high, b is too low); but the least-absolute estimator appears less biased. Even though the variance of both estimators declines with sample size, due to bias, neither of them are consistent. Their relative efficiecy follows a similar pattern to problem 2 - the least-absolute is more efficient, because it is less influence by outliers. Omitted variable bias. If X and Z are positively correlated, then if our model ignores Z, our estimate of the slope will overestimate the true effect of X - because our model wrongly attributed the effect of Z to X. If X and Z are negatively correlated, then the opposite is true - we have underestimated the slope, because the effect of Z is negating the effect of X. Finally, if X and Z are independent, then our estimate of the slope will be the same. Test your conjectures. library(MASS) sim.2var ## function (n, nsim, a, b1, b2 = 0, sigma.disturb = 1, correl = 0) ## { ## sig &lt;- matrix(c(1, correl, correl, 1), nrow = 2) ## ivs &lt;- MASS::mvrnorm(n * nsim, mu = c(0, 0), sig) ## x &lt;- ivs[, 1] ## z &lt;- ivs[, 2] ## disturb &lt;- rnorm(n * nsim, 0, sigma.disturb) ## y &lt;- a + b1 * x + b2 * z + disturb ## xmat &lt;- matrix(x, nrow = nsim) ## ymat &lt;- matrix(y, nrow = nsim) ## list(xmat, ymat) ## } ## &lt;bytecode: 0x7fb66dcff6c8&gt; ## &lt;environment: namespace:stfspack&gt; n &lt;- 50 nsims &lt;- 1000 beta &lt;- .3 gamma &lt;- .4 # Positive correlation rho &lt;- .5 dat &lt;- sim.2var(n, nsims, 0, beta, gamma, 1, rho) ests &lt;- numeric(nsims) for(i in 1:nsims){ ests[i] &lt;- lm(dat[[2]][i,] ~ dat[[1]][i,])$coef[2] } ests_rho_positive &lt;- ests # Negative correlation rho &lt;- -0.5 dat &lt;- sim.2var(n, nsims, 0, beta, gamma, 1, rho) ests &lt;- numeric(nsims) for(i in 1:nsims){ ests[i] &lt;- lm(dat[[2]][i,] ~ dat[[1]][i,])$coef[2] } ests_rho_negative &lt;- ests # No correlation rho &lt;- 0 dat &lt;- sim.2var(n, nsims, 0, beta, gamma, 1, rho) ests &lt;- numeric(nsims) for(i in 1:nsims){ ests[i] &lt;- lm(dat[[2]][i,] ~ dat[[1]][i,])$coef[2] } ests_rho_none &lt;- ests par(mfrow = c(1,3)) hist(ests_rho_negative, main = &quot;Negative correlation&quot;, xlab = &quot;Estimates of b&quot;) abline(v = beta, col = &quot;red&quot;, lty = 2, lwd = 2) hist(ests_rho_none, main = &quot;No correlation&quot;, xlab = &quot;Estimates of b&quot;) abline(v = beta, col = &quot;red&quot;, lty = 2, lwd = 2) hist(ests_rho_positive, main = &quot;Positive correlation&quot;, xlab = &quot;Estimates of b&quot;) abline(v = beta, col = &quot;red&quot;, lty = 2, lwd = 2) If we are ignoring the effect of Z (a potential confound), then it is unreasonable to use our estimate of the slope to make predictions of Y based on X alone, if X and Z are correlated. "],
["intervals.html", "Chapter 7 Interval estimation and inference 7.1 Standard error 7.2 Confidence intervals 7.3 Frequentist inference I: null hypotheses, test statistics, and p values 7.4 Frequentist inference II: alternative hypotheses and the rejection framework 7.5 Connecting hypothesis tests and confidence intervals 7.6 NHST and the abuse of tests 7.7 Frequentist inference III: power 7.8 Putting it together: what happens when the sample size increases? 7.9 Chapter summary", " Chapter 7 Interval estimation and inference A single estimate is not enough, we need uncertainty. This chapter is about estimating intervals to go with our point estimates, and the related procedures of hypothesis testing (i.e., statistical inference). 7.1 Standard error The variance of an estimator, \\(\\text{Var}(\\hat{\\theta}_n)\\), describes the spread of the estimator’s distribution. We define it in the same way that we defined the variance of a random variable: \\[ \\begin{aligned} \\text{Var}(\\hat{\\theta}_n) =&amp; ~ E[(\\hat{\\theta}_n(D) - E[\\hat{\\theta}_n(D)])^2] \\\\ =&amp; ~ E[(\\hat{\\theta}_n(D)^2] - (E[\\hat{\\theta}_n(D)])^2 \\\\ \\end{aligned} \\] Compared to the expectation, or mean, of the estimator, the variance is in squared units. So we’d prefer to take the square root of the variance (\\(\\sigma^2\\)), otherwise known as the standard deviation (\\(\\sigma\\)). Rather than call the standard deviation of an estimator’s distribution the standard deviation, we call it the standard error instead: \\[ \\begin{aligned} \\text{SE}(\\hat{\\theta}_n) =&amp; \\sqrt{\\text{Var}(\\hat{\\theta}_n)} \\\\ \\end{aligned} \\] This is confusing, but to paraphrase Edge: the term ‘standard error’ is used to describe the measure of spread of an estimator’s sampling distribution, whereas the term ‘standard deviation’ is reserved to describe the spread in distribution of the data. For example, when we describe a dataset using the mean, median, or mode - we are applying an estimator (a function) to the data to get a point estimate of the data. But in your introductory stats class, you probably did not refer to these functions as estimators - rather, they are often described as summary statistics. Then, to describe the uncertainty around the mean, we can calculate the standard error of the mean, which again, is just the standard deviation of the distribution of sample means. This distribution of sample means is known as a sampling distribution, but this term is not restricted to the mean - it applies to any estimator. Phew. Cogitate on this until it makes sense. 7.1.1 Exercise set 7-1 See Edge solution or handwritten notes. If we instead use the median, instead of the mean, as the estimator of \\(\\theta\\), \\(text{SE}(\\hat{\\theta}_n)\\) is 0.25 (higher than the standard erro of the mean for 25 samples, 0.2). # the value of mu does not affect the answer # (adding a constant to a random variable does not affect its variance) mu &lt;- 0 stdev &lt;- 1 n &lt;- 25 n_sims &lt;- 10000 set.seed(101) s.mat &lt;- mat.samps(n = n, nsim = n_sims) ests.median &lt;- apply(s.mat, 1, median) sd(ests.median) ## [1] 0.2478706 # Confirming the answer to part c: ests.mean &lt;- apply(s.mat, 1, mean) sd(ests.mean) ## [1] 0.1984093 See Edge solution for a cogent explanation. Under a normal distribution, the probability that the estimate (still a random variable) lies between plus and minus 1 SE of the mean is ~68% - we can use old-school look up tables or the pnorm function in R. mu &lt;- 0 w &lt;- 1 # this is the standard error of the mean # Pnorm gives the cumulative probability distribution, where q is the z-score # One tailed probabilities pnorm(q = 1, mean = mu, sd = w) ## [1] 0.8413447 pnorm(q = -1, mean = mu, sd = w) ## [1] 0.1586553 # To get the probability between the upper and lower w: pnorm(q = 1, mean = mu, sd = w) - pnorm(q = -1, mean = mu, sd = w) ## [1] 0.6826895 Under a normal distribution, the probability that the standard error lies between plus and minus 2 SE of the mean is ~95% - we can use old-school look up tables or the pnorm function in R. mu &lt;- 0 w &lt;- 0.5 # this is the standard error of the mean # Pnorm gives the cumulative probability distribution, where q is the z-score # One tailed probabilities pnorm(q = 1, mean = mu, sd = w) # note that this value of q is twice w (the SE) ## [1] 0.9772499 pnorm(q = -1, mean = mu, sd = w) ## [1] 0.02275013 # To get the probability between the upper and lower w: pnorm(q = 1, mean = mu, sd = w) - pnorm(q = -1, mean = mu, sd = w) ## [1] 0.9544997 7.2 Confidence intervals Confidence interval for an estimand \\(\\theta\\): \\[ \\begin{aligned} \\text{P}(V_1 &lt; \\theta &lt; V_2) \\geq 1 - \\alpha \\\\ \\end{aligned} \\] where \\(V_1\\) and \\(V_2\\) are random variables that represent the upper and lower bounds of the interval, respectively. Therefore, the interval is also a random variable. But, the estimand (\\(\\theta\\)) is not - it is a fixed quantity. So in words, we can say that \\(1 - \\alpha\\) is the probability that the interval ranging from \\(V_1\\) to \\(V_2\\) covers \\(\\theta\\); often called a coverage probability. Another way to write the equation is: \\[ \\begin{aligned} \\text{P}(V_1 &lt; \\theta \\cap V_2 &gt; \\theta) \\geq 1 - \\alpha \\\\ \\end{aligned} \\] To refer to a confidence interval already calculated from data, we use (\\(v_1, v_2\\)). In the plot below, I recreated Edge’s Figure 7-1. I took 20 samples from a standard normal distribution (mean = 0, sd = 1), and then calculated the sample mean, standard deviation, and standard error of the mean. One of the most commonly used 1 - \\(\\alpha\\) confidence intervals is: \\[ \\begin{aligned} (\\hat{\\theta} - \\omega z_{\\alpha / 2}, \\hat{\\theta} + \\omega z_{\\alpha / 2}) \\\\ \\end{aligned} \\] where \\(\\omega\\) is the standard error, and \\(z_{\\alpha / 2}\\) represents the z-score, or the number of standard deviations for a standard normal distribution (i.e., a standardized distribution). The product of these two values represents the amount we add to the point estimate (i.e., the mean) to get the upper bound, and subtract from the point estimate to get the lower bound. To get the desired z-score, we have to work backwards from our desired \\(\\alpha\\) level. Here, we are using 0.1, so we take the inverse of the cumulative distribution function in R using qnorm - or you could do this using look-up tables. Either way, the z-score that returns a 90% probability is ~1.64 for a standard normal distribution. Here’s the equation: \\[ \\begin{aligned} z_{\\alpha / 2} =&amp; \\phi^{-1} (1 - \\alpha / 2) \\\\ \\phi^{-1} =&amp; ~\\text{inverse of the cumulative distribution function of a Normal(0,1) distribution} \\end{aligned} \\] mu &lt;- 0 stdev &lt;- 1 n &lt;- 20 n_sims &lt;- 50 set.seed(99) s.mat &lt;- mat.samps(n = n, nsim = n_sims) ests_mean &lt;- apply(s.mat, 1, mean) ests_sd &lt;- apply(s.mat, 1, sd) # 90% confidence intervals my_z &lt;- qnorm(0.95, mu, stdev) d &lt;- tibble(sim = 1:n_sims, mean = ests_mean, sd = ests_sd, se = ests_sd / sqrt(n), upper = mean + se*my_z, lower = mean - se*my_z, sig = ifelse(upper &lt; 0 | lower &gt; 0, &quot;different&quot;, &quot;not different&quot;)) d %&gt;% ggplot(aes(x = sim, y = mean, color = sig)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + geom_point() + geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) + labs(y = expression(paste(&quot;Confidence interval for &quot;, theta, sep = &quot;&quot;)), x = &quot;Simulation number&quot;, title = expression(paste(&quot;True &quot;, theta, &quot; is 0&quot;, sep = &quot;&quot;))) + theme(legend.position = &quot;none&quot;) In this set of simulations, the confidence intervals estimated from 46 of 50 (92%) of the simulations overlapped with the true value of \\(\\theta\\), 0. I still struggle with how to articulate the meaning of an already calculated confidence interval that is consistent with the frequentist philosophy. Even after reading the explanation in Edge, I am left unsatisfied. Perhaps we can consider the calculated confidence interval as a future confidence interval with the same upper and lower bounds that will cover the estimand? So you’re not alone if you don’t quite get it. 7.2.1 Exercise set 7-2 50% confidence interval; upper = 4, lower = 2. What is \\(z_{\\alpha / 2}\\) for \\(\\alpha = 0.5\\)? my_alpha &lt;- 0.5 my_p &lt;- 1 - my_alpha / 2 my_z &lt;- qnorm(p = my_p, mean = 0, sd = 1) my_z ## [1] 0.6744898 # Plot a standard normal distribution x &lt;- seq(-3.5, 3.5, length.out = 10000) fx &lt;- dnorm(x) plot(x, fx, type = &quot;l&quot;) # Shade in the appropriate area x.zs &lt;- seq(-my_z, my_z, length.out = 10000) fx.zs &lt;- dnorm(x.zs) polygon(c(-my_z, x.zs, my_z), c(0, fx.zs, 0), col = &quot;grey&quot;, border = FALSE) The area of the shaded region is 0.5. In equation 7.3, the point estimate is halfway between the upper and lower bounds, and thus is 3. The standard error is 1 / 0.674 = 1.48. 95% CI is roughly (0, 6). my_se &lt;- 1 / my_z z_95 &lt;- qnorm(p = 0.975, 0, 1) int_95 &lt;- my_se * z_95 3 + int_95; 3 - int_95 ## [1] 5.905847 ## [1] 0.09415305 How many SE’s from 0 is \\(\\theta\\)? 3 / my_se ## [1] 2.023469 We want the lower bound to sit on zero - this means the value that we subtract from our estimand is 3. The value that we subtract is the product of the SE and z-score. We know the SE - it is 1.48. So we calculate 3 / 1.48 = 2.03. This value represents the z-score for \\(z_{\\alpha / 2}\\). But what is \\(\\alpha\\), the probability level we need to be 2.03 SDs below the mean? This is where pnorm comes in: (1 - pnorm(q = 2.03)) * 2 ## [1] 0.04235654 Assume \\(\\theta\\) = 0. What is the probability of obtaining an estimate such that it is larger than the estimate considered in this example? To get a value of 3, if the true estimate is 0, we’d need a z-score of 2.03 again. This equates to a probability of 0.043 to retrieve something greater than 3, or less than -3. mu &lt;- 0 test_value &lt;- 3 my_z &lt;- test_value / my_se (1 - pnorm(q = my_z)) * 2 ## [1] 0.04302479 7.3 Frequentist inference I: null hypotheses, test statistics, and p values Null hypothesis Test statistic Hypothesis test 7.3.1 Exercise set 7-3 Three reasons why the outcome of the hypothesis test is indecisive about the truth of the theory (that smaller people will be attracted to hotter climates because smaller bodies shed heat more efficiently): Arizona may not be representative of hotter climates (i.e., need different geographic locations) women may be different from men, and thus the result does not apply to people generally the test does not let us say anything about the mechanism (heat shedding), but only the pattern (size variation) see Edge for a general explanation that touches on some of the above Average nail length is 100 mm. SD is 2 mm. I measure 4 nails. SE = sd / sqrt(n); SE = 2 / sqrt(4) = 1 1.96 * SE; 95% CI for \\(\\mu\\) is (98, 102) Anything less than 98 or more than 102, following from previous answer. Or, more precisely: # left tail qnorm(0.025, mean = 100, sd = 1) ## [1] 98.04004 # right tail qnorm(0.975, mean = 100, sd = 1) ## [1] 101.96 Simulating data assuming the null hypothesis is true. twotailed.p.normal &lt;- function(x.bar, mu, stand.err){ abs.diff &lt;- abs(x.bar - mu) 2 * pnorm(mu - abs.diff, mean = mu, sd = stand.err) } n &lt;- 10000 x &lt;- mat.samps(n = 4, nsim = n, rx = rnorm, mean = 100, sd = 2) x_mean &lt;- apply(x, MARGIN = 1, FUN = mean) x_p &lt;- sapply(x_mean, FUN = twotailed.p.normal, mu = 100, stand.err = 1) hist(x_p) # Values less than 0.05 length(x_p[x_p &lt; 0.05]) / n ## [1] 0.0516 mean(x_p &lt; 0.05) ## [1] 0.0516 # Values less than 0.1 length(x_p[x_p &lt; 0.1]) / n ## [1] 0.0988 Simulating data assuming the null hypothesis is false (mean = 101): twotailed.p.normal &lt;- function(x.bar, mu, stand.err){ abs.diff &lt;- abs(x.bar - mu) 2 * pnorm(mu - abs.diff, mean = mu, sd = stand.err) } n &lt;- 10000 x &lt;- mat.samps(n = 4, nsim = n, rx = rnorm, mean = 101, sd = 2) # CHANGED x_mean &lt;- apply(x, MARGIN = 1, FUN = mean) x_p &lt;- sapply(x_mean, FUN = twotailed.p.normal, mu = 100, stand.err = 1) hist(x_p) # Values less than 0.05 length(x_p[x_p &lt; 0.05]) / n ## [1] 0.166 mean(x_p &lt; 0.05) # same result! ## [1] 0.166 # Values less than 0.1 length(x_p[x_p &lt; 0.1]) / n ## [1] 0.2596 Simulating data assuming the null hypothesis is false (mean = 102). twotailed.p.normal &lt;- function(x.bar, mu, stand.err){ abs.diff &lt;- abs(x.bar - mu) 2 * pnorm(mu - abs.diff, mean = mu, sd = stand.err) } n &lt;- 10000 x &lt;- mat.samps(n = 4, nsim = n, rx = rnorm, mean = 102, sd = 2) # CHANGED x_mean &lt;- apply(x, MARGIN = 1, FUN = mean) x_p &lt;- sapply(x_mean, FUN = twotailed.p.normal, mu = 100, stand.err = 1) hist(x_p) # Values less than 0.05 length(x_p[x_p &lt; 0.05]) / n ## [1] 0.5088 # Values less than 0.1 length(x_p[x_p &lt; 0.1]) / n ## [1] 0.6352 Simulating data assuming the null hypothesis is false (mean = 101, n = 16). This means the standard error is now 2 / sqrt(16) = 0.5 twotailed.p.normal &lt;- function(x.bar, mu, stand.err){ abs.diff &lt;- abs(x.bar - mu) 2 * pnorm(mu - abs.diff, mean = mu, sd = stand.err) } n &lt;- 10000 x &lt;- mat.samps(n = 16, nsim = n, rx = rnorm, mean = 101, sd = 2) # CHANGED x_mean &lt;- apply(x, MARGIN = 1, FUN = mean) x_p &lt;- sapply(x_mean, FUN = twotailed.p.normal, mu = 100, stand.err = 0.5) hist(x_p) # Values less than 0.05 length(x_p[x_p &lt; 0.05]) / n ## [1] 0.5109 # Values less than 0.1 length(x_p[x_p &lt; 0.1]) / n ## [1] 0.6351 7.4 Frequentist inference II: alternative hypotheses and the rejection framework 7.4.1 Exercise set 7-4 All on paper. 7.5 Connecting hypothesis tests and confidence intervals 7.6 NHST and the abuse of tests 7.6.1 Exercise set 7-5 ps &lt;- many.outcome.sim(n = 20, nsim = 10000, n.dv = 7, correl = 0.7) sigs &lt;- ps &lt; 0.05 colMeans(sigs) ## [1] 0.0516 0.0482 0.0500 0.0520 0.0490 0.0500 0.0502 mean(rowMeans(sigs) &gt; 0) ## [1] 0.1784 n = 20, n.dv = 7, correl = 0.7: 18% of simulations have &gt; 1 significant result. There is a significant effect about 5% of the time for each dependent variable. 18% of the simulations have at least one significant result. Changing dependent variables (n.dv) and correlation: ps &lt;- many.outcome.sim(n = 20, nsim = 10000, n.dv = 7, correl = 0.7) sigs &lt;- ps &lt; 0.05 colMeans(sigs) ## [1] 0.0526 0.0526 0.0526 0.0520 0.0528 0.0492 0.0506 mean(rowMeans(sigs) &gt; 0) ## [1] 0.1865 n = 20, n.dv = 15, correl = 0.7: 26% of simulations have &gt; 1 significant result. n = 20, n.dv = 7, correl = 0.35: 27% of simulations have &gt; 1 significant result. n = 10, n.dv = 7, correl = 0.7: 18% of simulations have &gt; 1 significant result. So in summary: Increasing the # of dependent variables increases the probability of Type 1 error Decreasing the correlation between variables increases the probability of Type 1 error Changing the size of each group does not change the probability of Type 1 error Serial testing ps &lt;- serial.testing.sim(ns = c(20, 30, 40, 50), nsim = 10000) ps &lt;- serial.testing.sim(ns = seq(20, 200, by = 10), nsim = 10000) sigs &lt;- ps &lt; 0.05 colMeans(sigs) ## [1] 0.0496 0.0504 0.0487 0.0504 0.0518 0.0527 0.0525 0.0524 0.0525 0.0518 ## [11] 0.0529 0.0501 0.0531 0.0536 0.0531 0.0525 0.0526 0.0531 0.0531 mean(rowMeans(sigs) &gt; 0) ## [1] 0.229 If we sample at 4 times (starting with 20, ending with 50 people), there is a 12% change that at least one of those time points is significant. If we sample 19 times (20 to 200), then there is a 22% change one of the time points is significant. 7.7 Frequentist inference III: power 7.7.1 Exercise set 7-6 Plotting a power function for n = 5, 25, 75 with alpha = 0.05. d &lt;- power.sim.1sz(n = 25, nsim = 5000, d = 0.2, lev = 0.05) d &lt;- seq(-2, 2, by = 0.02) d_5 &lt;- sapply(X = d, FUN = power.sim.1sz, n = 5, nsim = 1000, lev = 0.05) d_25 &lt;- sapply(X = d, FUN = power.sim.1sz, n = 25, nsim = 1000, lev = 0.05) d_75 &lt;- sapply(X = d, FUN = power.sim.1sz, n = 75, nsim = 1000, lev = 0.05) plot(d, d_25, type = &quot;l&quot;, xlab = &quot;Effect size (d)&quot;, ylab = &quot;Power&quot;) lines(d, d_5, type = &quot;l&quot;, col = &quot;red&quot;) lines(d, d_75, type = &quot;l&quot;, col = &quot;blue&quot;) As n increases, estimated d declines to the true value, and power increases. At larger effect sizes, the power function converges to the truth much more quickly with sample size. d &lt;- wincurse.sim.1sz(n = 1, nsim = 10000, d = 0.3, lev = 0.05) d ## true d estimated d power ## 0.300000 1.470967 0.057900 d = 0.1, n = 250: estimated d = 0.16, power = 0.35 d = 0.1, n = 50: estimated d = 0.32, power = 0.11 d = 0.1, n = 25: estimated d = 0.39, power = 0.08 d = 0.1, n = 5: estimated d = 0.55, power = 0.05 d = 0.5, n = 500: estimated d = 0.50, power = 1.00 d = 0.5, n = 50: estimated d = 0.52, power = 0.94 d = 0.5, n = 25: estimated d = 0.60, power = 0.70 d = 0.5, n = 5: estimated d = 1.12, power = 0.19 Plotting estimated effect size vs sample size. Plotting the size of the winner’s curse effect n_vector &lt;- seq(1, 100, by = 1) true_d &lt;- 0.3 est_ds &lt;- numeric(length(n_vector)) pows &lt;- numeric(length(n_vector)) for(i in 1:length(n_vector)){ wc &lt;- wincurse.sim.1sz(n = n_vector[i], nsim = 10000, d = true_d) est_ds[i] &lt;- wc[2] pows[i] &lt;- wc[3] } plot(n_vector, est_ds, type = &quot;l&quot;, xlab = &quot;Sample size (n)&quot;, ylab = &quot;Estimated d&quot;) lines(n_vector, rep(true_d, length(n_vector)), lty = 2) curse_size &lt;- est_ds - true_d plot(pows, curse_size, type = &quot;l&quot;, xlab = &quot;Power&quot;, ylab = &quot;Winner&#39;s curse effect&quot;) 7.8 Putting it together: what happens when the sample size increases? 7.9 Chapter summary "],
["semiparametric.html", "Chapter 8 Semiparametric estimation and inference 8.1 Semiparametric point estimation using the method of moments 8.2 Semiparametric interval estimation using the bootstrap 8.3 Semiparametric hypothesis testing using permutation tests 8.4 Chapter summary", " Chapter 8 Semiparametric estimation and inference Parametric: governed by parameters; e.g., normal distributions Nonparametric: not governed by parameters; used when we cannot assume that the distribution of a random variable follows a particular probability distribution Semiparametric: a model whose behavior is partially governed by parameters Empirical distribution function: summarizes all the information that the data provide about a random variable’s distribution The empirical distribution function \\(\\hat{F}_n(z)\\) gives the proportion of observations in a sample that are less than or equal to a constant \\(z\\): \\[ \\begin{aligned} \\hat{F}_n(z) =&amp; \\frac{1}{n} \\sum_{i = 1}^n I_{X_i \\leq z} \\end{aligned} \\] where \\(I\\) is an indicator variable that states whether observation \\(X_i\\) is less than or equal to \\(z\\). 8.0.1 Exercise set 8-1 Comparing an ECDF with a CDF for the normal distribution As the sample size increases, the ECDF approximates the CDF of the normal distribution. n &lt;- 500 x.vals &lt;- seq(-3, 3, length.out = 10000) Fx &lt;- pnorm(x.vals, 0, 1) plot(x.vals, Fx, xlab = &quot;z&quot;, ylab = &quot;F(z)&quot;, type = &quot;l&quot;) x &lt;- rnorm(n, 0, 1) lines(ecdf(x), verticals = TRUE, do.points = FALSE, lty = 2) Comparing an ECDF with a CDF for the Poisson distribution set.seed(22) n &lt;- 30 lambda &lt;- 10 x.vals &lt;- seq(0, 30, by = 1) Fx &lt;- ppois(x.vals, lambda = lambda) plot(x.vals, Fx, xlab = &quot;z&quot;, ylab = &quot;F(z)&quot;, type = &quot;p&quot;) x &lt;- rpois(n, lambda = lambda) lines(ecdf(x), verticals = TRUE, do.points = FALSE, lty = 2) See handwritten notes. 8.1 Semiparametric point estimation using the method of moments 8.1.1 Introduction to moments Methods of moments: a semiparametric approach to estimation Method of maximum likelihood: a parametric approach to estimation Suppose \\(X\\) is a random variable. The \\(j\\)th moment of the distribution is: \\[ \\begin{aligned} \\mu_j =&amp; E(X^j) \\end{aligned} \\] where \\(\\mu\\) is used as the symbol for a moment. For example \\(\\mu_1 = E(X)\\). It is easy to express the first moment of \\(X\\) as an expression using \\(X\\) - but what about \\(\\mu_2\\)? \\[ \\begin{aligned} \\mu_2 =&amp; E(X^2) \\end{aligned} \\] Now we have expressed the second moment, \\(\\mu_2\\), as the expectation of a new random variable, \\(X^2\\). At least this is how I interpret it. From my reading, I gather this is not so satisfying, because the second moment is then usually rearranged in the following way. First, we need to recall the definition of Var(\\(X\\)): \\[ \\begin{aligned} \\text{Var}(X) = \\text{E}(X^2) - [\\text{E}(X)]^2 \\\\ \\end{aligned} \\] So, if we rearrange we get: \\[ \\begin{aligned} \\text{E}(X^2) = \\text{Var}(X) + [\\text{E}(X)]^2 \\\\ \\end{aligned} \\] We can continue on with 3rd, 4th, 5th, etc, moments. But expressing these in terms of \\(X\\) is, I am guessing, a harder challenge. These additional moments describe further the probability distribution, but typically knowing the first and second moments is enough for practical purposes (our purposes). As an aside, the third moment gives us the skewness, and the fourth moment gives us the kurtosis, of the distribution. Finally, it is worth noting the notation for a moment - here I have used \\(\\mu\\), becuase it is often used in statistics notes (that I found online). This can be confusing, because we often use \\(\\mu\\) for the population mean. But we won’t do that here, for clarity. So, let us say that \\(X\\) follows a Normal(\\(\\theta\\), \\(\\sigma^2\\)) distribution. In this case, we know that E(\\(X\\)) = \\(\\theta\\), and that Var(\\(X\\)) = \\(\\sigma^2\\). So we can express the first and second moments as: \\[ \\begin{aligned} \\mu_1 =&amp; ~ \\text{E}(X) \\\\ =&amp; ~ \\theta \\\\ \\mu_2 =&amp; ~ \\text{E}(X^2) \\\\ =&amp; \\text{Var}(X) + [\\text{E}(X)]^2 \\\\ =&amp; ~ \\sigma^2 + \\theta^2 \\\\ \\end{aligned} \\] So we have expressed the moments in terms of the parameters of the distribution we are trying estimate. We can rearrange these, to express the parameters in terms of \\(X\\) - which is exactly what we want to do, since we are trying to estimate \\(\\theta\\) and \\(\\sigma^2\\)! \\[ \\begin{aligned} \\theta =&amp; ~ \\text{E}(X) \\\\ \\sigma^2 =&amp; ~ \\text{E}(X^2) - [\\text{E}(X)]^2 \\\\ \\end{aligned} \\] To paraphrase Edge, we can estimate the moments of arbitrary distributions (i.e., distributions that are not described by parameters) by following these steps: Write the equations that give the moments of a random variable in terms of the parameters being estimated Solve the equations from (1) for the desired parameters, giving expressions for the parameters in terms of the moments Estimate the moments and plug the estimates into the expressions for the parameters from (2) 8.1.2 Plug-in estimators Plug-in estimation: perhaps I’m oversimplifying here, but this sounds like awesome jargon for ‘calculate your estimator from data’. In other words, if you want the population mean - collect some data and calculate the mean. Then ‘plug that in’ to your population mean. We can express a plug-in estimator of the \\(k\\)th moment of \\(X\\), E(\\(X^j\\)) as the \\(k\\)th sample moment: \\[ \\begin{aligned} \\overline{X^k} = \\frac{1}{n} \\sum_{i = 1}^n X_i^k \\end{aligned} \\] So to recap: a moment describes the random variable \\(X\\). A sample moment is an estimate of the moment using independent samples (\\(X_1, X_2, X_3\\)) drawn from \\(X\\). 8.1.3 Exercise set 8-2 Supose \\(X_1, X_2, ..., X_n\\) are I.I.D observations. What is the plug-in estimator of the variance of \\(X\\)? \\[ \\begin{aligned} \\sigma^2 =&amp; ~ \\text{E}(X^2) - [\\text{E}(X)]^2 \\\\ \\hat\\sigma^2 =&amp; ~ \\frac{1}{n} \\sum_{i = 1}^n X_i^2 - [\\frac{1}{n} \\sum_{i = 1}^n X_i]^2 \\\\ \\end{aligned} \\] b. What is the plug-in estimator for the standard deviation of \\(X\\)? \\[ \\begin{aligned} \\hat\\sigma =&amp; ~ \\sqrt{\\frac{1}{n} \\sum_{i = 1}^n X_i^2 - [\\frac{1}{n} \\sum_{i = 1}^n X_i]^2} \\\\ \\end{aligned} \\] What is the plug-in estimator of the covariance of \\(X\\) and \\(Y\\)? Here is the formula for the covariance, as a reminder: \\[ \\begin{aligned} \\text{Cov}(X,Y) =&amp; ~ \\text{E}([X - \\text{E}(X)][Y - \\text{E}(Y)]) \\\\ =&amp; ~ \\text{E}(XY) - \\text{E}(X)\\text{E}(Y) \\end{aligned} \\] Let’s use the 2nd equation: \\[ \\begin{aligned} \\text{Cov}(X,Y) =&amp; ~ \\frac{1}{n} \\sum_{i = 1}^n X_iY_i - [\\frac{1}{n} \\sum_{i = 1}^n X_i][\\frac{1}{n} \\sum_{i = 1}^n Y_i] \\\\ \\end{aligned} \\] What is the plug-in estimator of the correlation of \\(X\\) and \\(Y\\)? Here is the formula for the correlation, as a reminder: \\[ \\begin{aligned} \\text{Cor}(X,Y) =&amp; ~ \\rho_{X,Y} \\\\ =&amp; ~ \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}} \\\\ =&amp; ~ \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\\\\ \\end{aligned} \\] Plugging in, we get: \\[ \\begin{aligned} \\rho_{X,Y} =&amp; ~ \\frac {\\frac{1}{n} \\sum_{i = 1}^n X_iY_i - [\\frac{1}{n} \\sum_{i = 1}^n X_i][\\frac{1}{n} \\sum_{i = 1}^n Y_i]} {\\sqrt{ \\frac{1}{n} \\sum_{i = 1}^n X_i^2 - [\\frac{1}{n} \\sum_{i = 1}^n X_i]^2}] [\\frac{1}{n} \\sum_{i = 1}^n Y_i^2] } \\\\ \\end{aligned} \\] Though the plug-in estimator of the variance is consistent, it is biased downward. Demonstrate the downward bias in R with simulations. This is what I did first: n_obs &lt;- 5 n_sims &lt;- 10000 set.seed(1010) x &lt;- mat.samps(n = n_obs, nsim = n_sims) x_var &lt;- apply(X = x, MARGIN = 1, FUN = var) mean(x_var) ## [1] 1.004699 The result is very close to 1. This surprised me, given that I was expecting something below 1. So I ran Edge’s code, and in it, he defines a function to calculate the variance himself. var.pi &lt;- function(vec){ return(sum((vec-mean(vec))^2)/length(vec)) } vars.pi &lt;- apply(x, 1, var.pi) mean(vars.pi) ## [1] 0.8037593 Why the difference? Turns out this is a great demonstration of something I had read some time ago, and then buried away because it frankly was not super relevant. Let’s inspect the help function for var: ?var And in the details you’ll see this note: The denominator n - 1 is used which gives an unbiased estimator of the (co)variance for i.i.d. observations. These functions return NA when there is only one observation (whereas S-PLUS has been returning NaN), and fail if x has length zero. Thanks to this book, this statement make sense. Let’s calculate the variance using n and n-1 as the denominator: vec &lt;- x[1,] vec ## [1] 0.4974519 1.1826813 1.0876947 -1.3540474 -0.9988671 # Default function to calculate the variance in R var(vec) ## [1] 1.406506 # Edge&#39;s function, with the denominator of n var.pi(vec) ## [1] 1.125205 # Beating a dead horse: sum((vec-mean(vec))^2) / (length(vec)) ## [1] 1.125205 # But now we change the denominator from n, to n-1 # Unbiased estimator for the variance # Gives the same result as `var` sum((vec-mean(vec))^2) / (length(vec) - 1) ## [1] 1.406506 I have not read ahead, but I suspect Edge will discuss this discrepancy soon.. Repeating the simulation, but with samples sizes from 2 to 10. n_sims &lt;- 10000 n_vec &lt;- 2:10 variance_vec &lt;- vector(length = length(n_vec)) for(i in 1:length(n_vec)){ n_obs &lt;- n_vec[i] x &lt;- mat.samps(n = n_obs, nsim = n_sims) vars.pi &lt;- apply(x, 1, var.pi) variance_vec[i] &lt;- mean(vars.pi) } variance_vec ## [1] 0.4879436 0.6578587 0.7378372 0.7852684 0.8369602 0.8502939 0.8715858 ## [8] 0.8900192 0.9012010 plot(n_vec, variance_vec, type = &quot;b&quot;, xlab = &quot;Number of samples&quot;, ylab = &quot;Mean variance&quot;, ylim = c(0, 1)) abline(h = 1, lty = 2 , col = &quot;red&quot;) The bias is largest with a sample size of 2, and the plug-in estimator gets closer to the true value with increasing sample size. I did not think to convert these decimal outputs to fractions as indicated in Edge’s solution, even having stumbled upon the unbiased estimator above. So this result is not yet intuitive: \\[ \\begin{aligned} \\text{E}(\\hat\\sigma^2_n) =&amp; ~ \\frac{n - 1}{n} \\sigma^2 \\\\ \\end{aligned} \\] This equation demonstrates the bias of the plug-in estimator for the variance. If we multiply it by \\(n/(n-1)\\), we get the sample variance (\\(s^2\\)), an unbiased estimator: \\[ \\begin{aligned} s^2 =&amp; ~ \\frac{\\sum_{i = 1}^n (X_i - \\overline X_i)^2}{n - 1} \\\\ \\end{aligned} \\] See Edge solution. It explains how we get the expectation of the plug-in estimator. Did not try. 8.1.4 The method of moments We apply the method of moments to the model for simple linear regresion: \\[ \\begin{aligned} Y = \\alpha + \\beta X + \\epsilon \\\\ \\end{aligned} \\] If we make the assumption that the expectation of the disturbance term is 0 for all values of \\(X\\): \\[ \\begin{aligned} \\text{E}(\\epsilon | X = x) =&amp; ~ 0 \\\\ \\end{aligned} \\] then the conditional expectation of \\(Y\\), given any \\(x\\), can be predicted using a line with a slope \\(\\beta\\) and intercept \\(\\alpha\\): \\[ \\begin{aligned} \\text{E}(Y) = \\alpha + \\beta_{\\mu_X} \\\\ \\end{aligned} \\] If this assumption is not true, then the relationship between X and Y is not linear! Now that we have made the linearity assumption (see Edge’s Figure 8-4), we can proceed with the MOM estimators for \\(\\alpha\\) and \\(\\beta\\). We know that the covariance of \\(X\\) and \\(Y\\) is: \\[ \\begin{aligned} \\text{Cov}(X,Y) = \\beta \\sigma_X^2 \\\\ \\end{aligned} \\] and that: \\[ \\begin{aligned} \\mu_X =&amp; ~ \\text{E}(X) \\\\ \\sigma_X^2 =&amp; ~ \\text{Var}(X) \\\\ \\text{Cov}(X,Y) =&amp; ~ \\text{E}(XY) - \\text{E}(X)\\text{E}(Y) \\\\ \\text{Var}(X) =&amp; ~ \\text{E}(X^2) - [\\text{E}(X)]^2 \\\\ \\end{aligned} \\] we can get expressions for the parameters \\(\\alpha\\) and \\(\\beta\\) in terms of the moments of \\(X\\) and \\(Y\\): \\[ \\begin{aligned} \\alpha =&amp; ~ \\text{E}(Y) - \\beta \\text{E}(X) \\\\ \\beta =&amp; ~ \\frac{\\text{Cov}(X,Y)}{\\sigma_X^2} \\\\ =&amp; \\frac{\\text{E}(XY) - \\text{E}(X)\\text{E}(Y)}{\\text{E}(X^2) - [\\text{E}(X)]^2} \\\\ \\end{aligned} \\] The expression for \\(\\beta\\) is entirely in terms of moments, as opposed to the expression for \\(\\alpha\\). So we’ll plug in sample moments for the moments in the expression for \\(\\beta\\), multiply the numerator and denominator by \\(\\frac{n}{n}\\), and get: \\[ \\begin{aligned} \\tilde{\\beta} =&amp; \\frac{\\sum X_i Y_i - \\frac{1}{n} (\\sum X_i) (\\sum Y_i)} {\\sum X_i^2 - \\frac{1}{n} (\\sum X_i)^2 }\\\\ \\end{aligned} \\] (try working through the algebra to get this expression yourself). Here’s the expression for \\(\\alpha\\): \\[ \\begin{aligned} \\tilde{\\alpha} =&amp; ~ \\frac{1}{n} \\sum Y_i - \\tilde{\\beta} \\frac{1}{n} \\sum X_i \\\\ =&amp; ~ \\frac{\\sum Y_i - \\tilde{\\beta} \\sum X_i} {n} \\end{aligned} \\] These expressions are essentially identical to the expressions for \\(\\alpha\\) and \\(\\beta\\) that we derived for the coefficients of the least squares line in chapter 3. 8.1.5 Exercise set 8-3 See solution on paper. See solution in Edge. 8.2 Semiparametric interval estimation using the bootstrap With enough high quality data, the empirical distribution function is a reasonable approximation to the true cumulative distribution function. We can leverage this fact to estimate uncertainty intervals to accompany our plug-in estimators. Bootstrap: a procedure that resamples a single dataset to create many simulated samples, which permits the calculation of standard errors and confidence intervals. Take B ‘bootstrap’ samples, with each sample being a set of n observations drawn with replacement from the original observations. Label these samples \\(D_1^*\\), \\(D_1^*\\), …, \\(D_B^*\\). For each of the simulated amples, calculate the statistic of interest \\(\\hat \\vartheta\\) for each simulated dataset. That is, calculate \\(\\hat \\vartheta(D_1^*)\\), \\(\\hat \\vartheta(D_2^*)\\), …, \\(\\hat \\vartheta(D_B^*)\\). These values form the bootstrap distribution of \\(\\hat \\vartheta(D)\\). We will apply this procedure to our dataset of paired x, y data to calculate the bootstrap distribution of \\(\\tilde \\beta\\), the method of moments estimator of \\(\\beta\\). We need to assume that the \\(x, y\\) pairs in the sample represent independent draws from a joint distribution. 8.2.0.1 Assumptions For all i and \\(j \\neq i\\), \\(X_i, Y_i\\) are independent of \\(X_j, Y_j\\). This is the independence of units assumption For all i, \\(X_i, Y_i\\) are drawn from the same joint distribution \\(F_{X,Y}(x,y)\\). This is the distribution assumption. 8.2.0.2 Bootstrapping the Anscombe dataset # Original fertilizer use and cereal yield data d &lt;- cbind(anscombe$x1, anscombe$y1) [order(anscombe$x1), ] # One bootstrap sample set.seed(8675309) db &lt;- cbind(anscombe$x1, anscombe$y1) [sample(1:11, replace = TRUE), ] d; db ## [,1] [,2] ## [1,] 4 4.26 ## [2,] 5 5.68 ## [3,] 6 7.24 ## [4,] 7 4.82 ## [5,] 8 6.95 ## [6,] 9 8.81 ## [7,] 10 8.04 ## [8,] 11 8.33 ## [9,] 12 10.84 ## [10,] 13 7.58 ## [11,] 14 9.96 ## [,1] [,2] ## [1,] 8 6.95 ## [2,] 14 9.96 ## [3,] 12 10.84 ## [4,] 12 10.84 ## [5,] 13 7.58 ## [6,] 4 4.26 ## [7,] 5 5.68 ## [8,] 7 4.82 ## [9,] 7 4.82 ## [10,] 11 8.33 ## [11,] 7 4.82 The function boot.samp returns a bootstrap sample of either a vector or the rows of a matrix. The function beta.mm calculates the methods of moments estimate of \\(\\beta\\). boot.samp ## function (x) ## { ## if (is.null(dim(x))) { ## x &lt;- matrix(x, ncol = 1) ## } ## n &lt;- nrow(x) ## boot.inds &lt;- sample(1:n, replace = TRUE) ## x[boot.inds, ] ## } ## &lt;bytecode: 0x7f8cb5b64ca8&gt; ## &lt;environment: namespace:stfspack&gt; beta.mm ## function (x, y) ## { ## n &lt;- length(x) ## (sum(x * y) - (1/n) * sum(x) * sum(y))/(sum(x^2) - (1/n) * ## sum(x)^2) ## } ## &lt;bytecode: 0x7f8cb5bbec00&gt; ## &lt;environment: namespace:stfspack&gt; We use a for loop to draw the bootstrap sample: set.seed(8675309) B &lt;- 10000 boot.dist &lt;- numeric(B) dat &lt;- cbind(anscombe$x1, anscombe$y2) for(i in 1:B){ samp &lt;- boot.samp(dat) boot.dist[i] &lt;- beta.mm(samp[,1], samp[,2]) } We can plot a histogram and calculate the standard error of the estimator, \\(\\tilde \\beta\\) (recalling that the standard error of an estimate is its standard deviation). # Plot histogram hist(boot.dist, xlim = c(-0.2, 1.2), breaks = 30, xlab = expression(paste(&quot;Bootstrapped &quot;, beta)), main = &quot;&quot;, col = &quot;gray&quot;, border = &quot;white&quot;) # Estimate standard error of boostrapped beta sd(boot.dist) ## [1] 0.165262 8.2.0.3 Approximating confidence intervals for \\(\\tilde \\beta\\) We consider three options: If we assume the sampling distribution is normal, then we can use the bootstrap estimate to identify a confidence interval (\\(1 - \\alpha\\) interval). # Normal bootstrap interval a &lt;- 0.05 b_mm &lt;- beta.mm(dat[,1], dat[,2]) b_sd &lt;- sd(boot.dist) z &lt;- qnorm(p = 1 - (a / 2), mean = 0, sd = 1) b_mm - b_sd*z; b_mm + b_sd*z ## [1] 0.1760924 ## [1] 0.8239076 Use the percentiles of the bootstrap distribution to bound a confidence interval (\\(1 - \\alpha ~ percentile\\) interval). # 95% percentile interval quantile(boot.dist, c(0.025, 0.975)) ## 2.5% 97.5% ## 0.1630270 0.8459399 Calculate bootstrap pivotal intervals (see Box 8-1; skipped for now). 8.2.1 Exercise set 8-4 Sensitivity of a bootstrapped mean to n, B. Try the boot.dist.1d function: n &lt;- 5 B &lt;- 10 sim &lt;- rnorm(n = n, mean = 0, sd = 1) boot.dist.1d(x = sim, B = B, FUN = mean) ## [1] -0.22323507 -0.01956961 0.04809411 -0.21668665 0.20288450 -0.01956961 ## [7] -0.13718622 -0.26937485 -0.33757747 -0.10288316 Use the wrapper function: n &lt;- 5 B &lt;- 10 wrap.bm(n = n, B = B) ## $`boot m` ## [1] -0.7622767 ## ## $`boot se` ## [1] 0.2741837 Changing n: # Adjust wrapper function wrap.bm2 &lt;- function (n, B, mu = 0, sigma = 1, FUN = mean, ...){ sim &lt;- rnorm(n, mu, sigma) boots &lt;- boot.dist.1d(sim, B, FUN = FUN, ...) hist(boots, main = paste(&quot;n = &quot;, n, &quot;; &quot;, &quot;B = &quot;, B, sep = &quot;&quot;), xlab = &quot;Bootstrap distribution&quot;, xlim = c(-1.5, 1.5), col = &quot;gray&quot;, border = &quot;white&quot;) } B &lt;- 10000 n &lt;- 5 par(mfrow = c(2, 2)) set.seed(3) wrap.bm2(n = n, B = B) n &lt;- 10 wrap.bm2(n = n, B = B) n &lt;- 50 wrap.bm2(n = n, B = B) n &lt;- 100 wrap.bm2(n = n, B = B) Changing B: n &lt;- 100 B &lt;- 10 par(mfrow = c(2, 2)) set.seed(101) wrap.bm2(n = n, B = B) B &lt;- 100 wrap.bm2(n = n, B = B) B &lt;- 1000 wrap.bm2(n = n, B = B) B &lt;- 5000 wrap.bm2(n = n, B = B) Sensitivity of a bootstrapped midrange to n, B. New functions: midrange &lt;- function(x){ (min(x) + max(x)) / 2 } wrap.bm3 &lt;- function (n, B, mu = 0, sigma = 1, FUN = mean, ...){ sim &lt;- rnorm(n, mu, sigma) boots &lt;- boot.dist.1d(sim, B, FUN = FUN, ...) sd(boots) } Changing n: B &lt;- 10000 my_seq &lt;- seq(1:10) n_vector &lt;- 2^my_seq se_vector &lt;- numeric(length = length(my_seq)) se_pred_vector &lt;- pi / (sqrt(24 * log(n_vector))) for(i in 1:length(my_seq)){ set.seed(101) se_vector[i] &lt;- wrap.bm3(n = n_vector[i], B = B, FUN = midrange) } par(mfrow = c(1,1)) plot(n_vector, se_pred_vector, type = &quot;l&quot;, main = paste(&quot;B = &quot;, B, sep = &quot;&quot;), ylim = c(0, 0.8), xlab = &quot;Sample size (n)&quot;, ylab = &quot;SE of midrange&quot;) points(n_vector, se_vector, col = &quot;red&quot;) Changing B: n &lt;- 100 my_seq &lt;- seq(1:10) B_vector &lt;- 2^my_seq se_vector &lt;- numeric(length = length(my_seq)) se_pred_vector &lt;- rep(pi / (sqrt(24 * log(n))), length(my_seq)) for(i in 1:length(my_seq)){ set.seed(101) se_vector[i] &lt;- wrap.bm3(n = n, B = B_vector[i], FUN = midrange) } par(mfrow = c(1,1)) plot(B_vector, se_pred_vector, type = &quot;l&quot;, main = paste(&quot;n = &quot;, n, sep = &quot;&quot;), ylim = c(0, 0.5), xlab = &quot;Bootstrap samples (B)&quot;, ylab = &quot;SE of midrange&quot;) points(n_vector, se_vector, col = &quot;red&quot;) Skipped. 8.3 Semiparametric hypothesis testing using permutation tests Here we will test the hypothesis that the estimated slope from the Anscombe dataset differs from a null hypothesis, where \\(\\beta = 0\\). This results in a simplified equation, where there is no assocation between \\(X\\) and \\(Y\\): \\[ \\begin{aligned} Y = \\alpha + \\epsilon \\end{aligned} \\] We will use a permutation test to test the null hypothesis. Let’s go over the assumptions of our test. 8.3.0.1 Assumptions Linearity: \\(\\text{E}(\\epsilon | X = x) = ~ 0\\) Independence of units: for all i and \\(j \\neq i\\), \\(X_i, Y_i\\) are independent of \\(X_j, Y_j\\) Distribution: for all i, \\(X_i, Y_i\\) are drawn from the same joint distribution \\(F_{X,Y}(x,y)\\) Independence of X and disturbances: for all \\(i\\), \\(X_i\\) and \\(\\epsilon_i\\) are independent Notice that we made no assumptions aboute the distribution of the disturbance term, \\(\\epsilon\\). The procedure for a permutation test is: Choose a test statistic and calculate it using the original data (\\(s_d\\)). Permute (i.e., shuffle) the data randomly and in such a way that if the null hypothesis were true, the hypothetical dataset is just as probable as the original data. Calculate the test statistic using the permuted data and save the value. Repeat steps (2) and (3) many times. The resulting test statistics are called a ‘permutation distribution’. Compare the original statistic \\(s_d\\) with the permutation distribution. Note that the permuted sample is somewhat like a sample that might be observed if the \\(X\\) and \\(Y\\) values were independent. Here’s the original dataset, and a permuted one: # Original fertilizer use and cereal yield data d &lt;- cbind(anscombe$x1, anscombe$y1) [order(anscombe$x1), ] # One permuted sample set.seed(8675309) db &lt;- cbind(anscombe$x1, anscombe$y1[sample(1:11, replace = FALSE)])[order(anscombe$x1), ] d; db ## [,1] [,2] ## [1,] 4 4.26 ## [2,] 5 5.68 ## [3,] 6 7.24 ## [4,] 7 4.82 ## [5,] 8 6.95 ## [6,] 9 8.81 ## [7,] 10 8.04 ## [8,] 11 8.33 ## [9,] 12 10.84 ## [10,] 13 7.58 ## [11,] 14 9.96 ## [,1] [,2] ## [1,] 4 8.81 ## [2,] 5 4.26 ## [3,] 6 9.96 ## [4,] 7 8.04 ## [5,] 8 8.33 ## [6,] 9 10.84 ## [7,] 10 6.95 ## [8,] 11 5.68 ## [9,] 12 7.58 ## [10,] 13 7.24 ## [11,] 14 4.82 If we do this a number of times, we can plot the histogram of permuted samples, along with a line to represent the original \\(\\tilde \\beta\\): set.seed(8675309) nperms &lt;- 10000 perm_dist &lt;- numeric(nperms) dat &lt;- cbind(anscombe$x1, anscombe$y1) for(i in 1:nperms){ samp &lt;- perm.samp(dat) perm_dist[i] &lt;- beta.mm(samp[,1], samp[,2]) } b_orig &lt;- beta.mm(anscombe$x1, anscombe$y1) b_orig ## [1] 0.5000909 # Plot histogram hist(perm_dist, xlim = c(-0.6, 0.6), breaks = 30, xlab = expression(paste(beta, &quot; from permuted samples&quot;)), main = &quot;&quot;, col = &quot;gray&quot;, border = &quot;white&quot;) abline(v = b_orig, col = &quot;red&quot;, lty = 2) We can compute a two-sided \\(p\\) value testing the null hypothesis that \\(\\beta = 0\\) by calculating the proportion of the permuted samples for which the absolute value of \\(\\tilde \\beta_p\\) is greater than the absolute value of \\(\\tilde \\beta\\) from the original data: mean(abs(perm_dist) &gt;= b_orig) ## [1] 0.0033 In summary we tested the hypothesis that: \\(\\beta = 0\\) AND the \\(X\\) and \\(Y\\) data are linear the permuted units are independent all observations are drawn from a common distribution the \\(X_i\\) are independent of the disturbance \\(\\epsilon_i\\) Therefore, in principle, the low \\(p\\) value indicates that one of the above are unlikely to be true. Usually, we focus on the null hypothesis, but it is important to remember the assumptions. 8.3.1 Exercise set 8-5 20 wheat fields, 10 received treatment application (substance Z). Design a permutation procedure to assess the claim that substance Z changes the wheat yield. The null hypothesis is that there is no association between yield and the treatment. Alternatively, the treatment may have altered the yield. To test this, we can randomly permute the control / treatment labels for the 20 fields. Then we can calculate the mean yield for each treatment, and compare the permutation distribution to the mean yield for each treatment for the original dataset. Edge has a thorough explanation, and in it he highlights that a low p-value may also result from a change in the variance due to substance Z. Thus, we are really testing the null hypothesis that substance Z does not affect the distribution of wheat yield. This might be worth simulating. # 10 pairs beta_vec &lt;- c(0, 0.1, 0.2) n_pairs &lt;- 10 n_datasets &lt;- 20 p_mat &lt;- matrix(nrow = 500, ncol = length(beta_vec)) for(i in 1:length(beta_vec)){ p_mat[,i] &lt;- sim.perm.B(n = n_pairs, nsim = n_datasets, a = 3, b = beta_vec[i]) } colnames(p_mat) &lt;- c(&quot;b_0&quot;, &quot;b_0.1&quot;, &quot;b_0.2&quot;) df10 &lt;- as_tibble(p_mat) # 50 pairs beta_vec &lt;- c(0, 0.1, 0.2) n_pairs &lt;- 50 n_datasets &lt;- 20 p_mat &lt;- matrix(nrow = 500, ncol = length(beta_vec)) for(i in 1:length(beta_vec)){ p_mat[,i] &lt;- sim.perm.B(n = n_pairs, nsim = n_datasets, a = 3, b = beta_vec[i]) } colnames(p_mat) &lt;- c(&quot;b_0&quot;, &quot;b_0.1&quot;, &quot;b_0.2&quot;) df50 &lt;- as_tibble(p_mat) # 100 pairs beta_vec &lt;- c(0, 0.1, 0.2) n_pairs &lt;- 100 n_datasets &lt;- 20 p_mat &lt;- matrix(nrow = 500, ncol = length(beta_vec)) for(i in 1:length(beta_vec)){ p_mat[,i] &lt;- sim.perm.B(n = n_pairs, nsim = n_datasets, a = 3, b = beta_vec[i]) } colnames(p_mat) &lt;- c(&quot;b_0&quot;, &quot;b_0.1&quot;, &quot;b_0.2&quot;) df100 &lt;- as_tibble(p_mat) # Get proportion of significant tests colMeans(df10 &lt; 0.05) ## b_0 b_0.1 b_0.2 ## 0.05 0.10 0.10 colMeans(df50 &lt; 0.05) ## b_0 b_0.1 b_0.2 ## 0.00 0.30 0.55 colMeans(df100 &lt; 0.05) ## b_0 b_0.1 b_0.2 ## 0.1 0.6 1.0 # Plot library(patchwork) p10 &lt;- df10 %&gt;% gather(key = beta, value = p) %&gt;% ggplot(aes(x = beta, p)) + geom_boxplot() + scale_y_continuous(limits = c(0, 1)) + ggtitle(&quot;n = 10 pairs&quot;) + geom_hline(yintercept = 0.05, color = &quot;red&quot;) p50 &lt;- df50 %&gt;% gather(key = beta, value = p) %&gt;% ggplot(aes(x = beta, p)) + geom_boxplot() + scale_y_continuous(limits = c(0, 1)) + ggtitle(&quot;n = 50 pairs&quot;) + geom_hline(yintercept = 0.05, color = &quot;red&quot;) p100 &lt;- df100 %&gt;% gather(key = beta, value = p) %&gt;% ggplot(aes(x = beta, p)) + geom_boxplot() + scale_y_continuous(limits = c(0, 1)) + ggtitle(&quot;n = 100 pairs&quot;) + geom_hline(yintercept = 0.05, color = &quot;red&quot;) p10 + p50 + p100 8.4 Chapter summary "],
["parametric.html", "Chapter 9 Parametric estimation and inference 9.1 Parametric estimation using maximum likelihood 9.2 Parametric interval estimation: the direct approach and Fisher information 9.3 Parametric hypothesis testing using the Wald test 9.4 Parametric hypothesis testing using the likelihood-ratio test", " Chapter 9 Parametric estimation and inference If all of the random variables in our model can be described by a finite set of parameters, we are using parametric estimation and inference. In paremetric estimation, the most important mathematical concept is the likelihood function, or likelihood. The likelihood allows us to compare values of a parameter in terms of their (the values’) ability to explain the observed data. In other words, if the true value of the parameter is y, how likely is the observed dataset? Suppose the observations, which we call \\(d\\), are instances drawn from a random variable \\(D\\), which is governed by a probability distribution function. We do not know what this probability distribution is! We have to think about the data - are they continuous or discrete; are they bounded at 0 or elsewhere; is the variance uniform or not? Based on the answers to these questions, we then assume a probability distribution function, \\(f_D\\). We can use \\(f_D\\) to evaluate the probability of the data, \\(d\\), at every potential value of the parameter \\(\\theta\\) we are trying to estimate, and we call this the likelihood \\(L(\\theta)\\): \\[ \\begin{aligned} L(\\theta) = f_D(d | \\theta) \\end{aligned} \\] It is tempting to think of the likelihood as a probability distribution. But it is not, because the function does not sum or integrate to 1 with respect to \\(\\theta\\) (which is a requirement for a probability distribution function). This is partly why we are defining it (the likelihood) as separate idea: the likelihood is a function of the parameters, and we use it to ask questions about the plausibility of the data (which are fixed), assuming different values of the parameters. If \\(L(\\theta_1|d) &gt; L(\\theta_2|d)\\), then the data we have observed are more likely to have occurred if \\(\\theta = \\theta_1\\) than \\(\\theta = \\theta_2\\). We interpret this result as: \\(\\theta_1\\) is a more plausible value for \\(\\theta\\) than \\(\\theta_2\\). Ok, so how do we actually calculate the likelihood? Suppose the data are \\(n\\) independent observations, \\(x_1, x_2, ..., x_n\\) each with the density function \\(f_X(x)\\), which depends on parameter \\(\\theta\\). This means that these observations are independent and identically distributed, and the joint density function (of the data and the likelihood) is given by: \\[ \\begin{aligned} f_{X_1, X_2, ..., X_n}(x_1, x_2, ..., x_n) =&amp; ~ f_X(x_1) * f_X(x_2) * ... * f_X(x_n) \\\\ =&amp; ~ \\prod_{i = 1}^n f_X(x_i) \\end{aligned} \\] where the symbol \\(\\prod\\) denotes multiplication in the same way that the symbol \\(\\sum\\) denotes addition. Instead of multiplying, we’d prefer to work with sums for numerical reasons. Because the log of a product is the sum of the logarithms of the terms being multiplied, \\[ \\begin{aligned} \\text{ln}(yz) = \\text{ln}(y) + \\text{ln}(z) \\end{aligned} \\] we can express the joint density function as a sum instead: \\[ \\begin{aligned} \\text{ln}[f_X(x_1) * f_X(x_2) * ... * f_X(x_n)] =&amp; ~ \\sum_{i = 1}^n \\text{ln}[f_X(x_i)] \\end{aligned} \\] and define it as follows: \\[ \\begin{aligned} l(\\theta) = \\text{ln}[L(\\theta)] \\end{aligned} \\] The likelihood provides a framework for estimation and inference. 9.0.1 Exercise set 9-1 Is this statement true / false? The value of \\(\\theta\\) that maximizes \\(L(\\theta)\\) is the most probable value of \\(\\theta\\) given the observed data. False - frequentists consider \\(\\theta\\) to be fixed, and thus has no probability distribution. See Edge explanation. Change to: The value of \\(\\theta\\) that maximizes \\(L(\\theta)\\) is the one that maximizes the probability of obtaining the observed data. Write all of the following in the simplest form: The pdf of \\(X\\), a Normal(\\(\\mu, \\sigma^2\\)) random variable: \\[ \\begin{aligned} f_X(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}} \\end{aligned} \\] The log-likelihood of \\(\\mu\\) given an observation of \\(x\\), which is assumed to be an instance of \\(X\\): \\[ \\begin{aligned} l(\\mu) =&amp; ~ \\text{ln}(\\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}}) \\\\ =&amp; ~ \\text{ln}(\\frac{1}{\\sigma \\sqrt{2 \\pi}}) + \\text{ln}(e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}}) \\\\ =&amp; ~ \\text{ln}(\\frac{1}{\\sigma \\sqrt{2 \\pi}}) - \\frac{(x - \\mu)^2}{2 \\sigma^2} \\\\ \\end{aligned} \\] The joint density of \\(X_1\\) and \\(X_2\\), two independent Normal(\\(\\mu, \\sigma^2\\)) random variables. \\[ \\begin{aligned} f_{X_1, X_2}(x_1, x_2) =&amp; ~ \\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{(x_1 - \\mu)^2}{2 \\sigma^2}} * \\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{(x_2 - \\mu)^2}{2 \\sigma^2}} \\\\ &amp; \\text{consider each of these terms as a, b, and c:} \\\\ =&amp; ~ (ab)*(ac) \\\\ =&amp; ~ a^2(bc) \\\\ &amp; \\text{therefore we can write} \\\\ =&amp; ~ \\frac{1}{\\sigma^2 2 \\pi} e^{-\\frac{(x_1 - \\mu)^2}{2 \\sigma^2}} * e^{-\\frac{(x_2 - \\mu)^2}{2 \\sigma^2}} \\\\ &amp; \\text{we know that } a^m * a^n = a^{m + n} \\text{, so} \\\\ =&amp; ~ \\frac{1}{\\sigma^2 2 \\pi} e^{-\\frac{(x_1 - \\mu)^2} {2 \\sigma^2} - \\frac{(x_2 - \\mu)^2}{2 \\sigma^2}} \\\\ &amp; \\text{we can factor out a -1} \\\\ =&amp; ~ \\frac{1}{\\sigma^2 2 \\pi} e^{- \\left ( \\frac{(x_1 - \\mu)^2} {2 \\sigma^2} + \\frac{(x_2 - \\mu)^2}{2 \\sigma^2} \\right )} \\\\ &amp; \\text{and we are left with} \\\\ =&amp; ~ \\frac{1}{\\sigma^2 2 \\pi} e^{-\\frac{(x_1 - \\mu)^2 + (x_2 - \\mu)^2}{2 \\sigma^2}} \\\\ \\end{aligned} \\] The log-likelihood of \\(\\mu\\) given an observation of \\(x_1\\) and \\(x_2\\), which are assumed to be instances of \\(X_1\\) and \\(X_2\\): \\[ \\begin{aligned} l(\\mu) =&amp; ~ \\text{ln} \\left (\\frac{1}{\\sigma^2 2 \\pi} e^{-\\frac{(x_1 - \\mu)^2 + (x_2 - \\mu)^2}{2 \\sigma^2}} \\right) \\\\ =&amp; ~ \\text{ln}(\\frac{1}{\\sigma^2 2 \\pi}) + \\text{ln}( e^{-\\frac{(x_1 - \\mu)^2 + (x_2 - \\mu)^2}{2 \\sigma^2}}) \\\\ =&amp; ~ \\text{ln}(\\frac{1}{\\sigma^2 2 \\pi}) -\\frac{(x_1 - \\mu)^2}{2 \\sigma^2} -\\frac{(x_2 - \\mu)^2}{2 \\sigma^2}\\\\ &amp; \\text{though I don&#39;t know how to get to Edge&#39;s answer, apart from starting with our answer in 1b:} \\\\ =&amp; ~ 2~\\text{ln}(\\frac{1}{\\sigma \\sqrt {2 \\pi}}) -\\frac{(x_1 - \\mu)^2}{2 \\sigma^2} -\\frac{(x_2 - \\mu)^2}{2 \\sigma^2}\\\\ \\end{aligned} \\] The log-likelihood of \\(\\mu\\) given observations of \\(x_1, x_2, ..., x_n\\), which are assumed to be instances of \\(n\\) independent random variables with a Normal(\\(\\mu, \\sigma^2\\)) distribution: \\[ \\begin{aligned} l(\\mu) =&amp; ~ n~\\text{ln}(\\frac{1}{\\sigma \\sqrt {2 \\pi}}) - \\sum_{i = 1}^n \\frac{(x_i - \\mu)^2}{2 \\sigma^2} \\\\ \\end{aligned} \\] 9.1 Parametric estimation using maximum likelihood The maximum-likelihood estimate of a parameter is the value of the parameter that maximizes the probability of observing the data The maximum-likelihood estimate of the parameter \\(\\theta\\) is \\(\\hat \\theta\\): \\[ \\begin{aligned} \\hat \\theta =&amp; ~ \\text{argmax} ~ L(\\theta) \\\\ &amp; \\text{or} \\\\ \\hat \\theta =&amp; ~ \\text{argmax} ~ l(\\theta) \\end{aligned} \\] ‘argmax’ means ‘argument of the maximum’ in this case, it is the value that maximizes the likelihood \\(L(\\theta)\\) or log-likelihood \\(l(\\theta)\\) usually, we use the log-likelihood to find \\(\\hat \\theta\\) To identify \\(\\hat \\theta\\): Write down the likelihood function \\(L(\\theta)\\) Take the log of likelihood function to get \\(l(\\theta)\\), and simplify Maximize \\(l(\\theta)\\) in terms of \\(\\theta\\). The value of \\(\\theta\\) that maximizes \\(l(\\theta)\\) is the maximum-likelihood estimator \\(\\hat \\theta\\) Edge goes through each of these steps to find the MLE of the parameter \\(\\lambda\\) of the exponential distribution, but I won’t repeat it here - mostly because I will probably never complete these steps in practice (if I tried to do this at all, I’d probably try to solve for the MLE numerically). The single most important assumption is that the data are actually being generated by the likelihood function we specified. If we are right, then the MLE has some desirable properties: consistency: \\(\\hat \\theta\\) converges to the true value of \\(\\theta\\) as the sample size increases asymptotically normally distributed: as the \\(n\\) approaches infinity, the distribution of the MLE approaches a normal distribution asymptotic efficiency: in the large samples, there are no consistent estimators with lower mean squared error than MLE functional invariance: this means you can estimate a value (using a defined function) based on \\(\\hat \\theta\\), and it too will be a maximum-likelihood estimator for that derived value Three (repeated) caveats, for emphasis: The MLE is NOT the value of the parameter that is most probable given the data. It is the parameter value that makes the data most probable The appeal of the MLE is efficiency, but this efficiency is defined with respect to the mean squared error, which may not be the appropriate loss function to the problem at hand The MLE is only meaningful if the model plausibly generated the data 9.1.1 Exercise set 9-2 The likelihood function for a Bernoulli distributed dataset is: \\[ \\begin{aligned} L(p) =&amp; ~ \\prod_{i = 1}^n p^{x_i} (1-p)^{1 - x_i} \\end{aligned} \\] The log-likelihood is: \\[ \\begin{aligned} l(p) =&amp; ~ \\text{ln}[L(p)] \\\\ =&amp; ~ \\text{ln} \\left [\\prod_{i = 1}^n p^{x_i} (1-p)^{1 - x_i} \\right] \\\\ =&amp; ~ \\sum_{i = 1}^n \\text{ln}[p^{x_i} (1-p)^{1 - x_i}] \\\\ =&amp; ~ \\sum_{i = 1}^n \\text{ln}[p^{x_i}] + \\text{ln}[(1-p)^{1 - x_i}] \\\\ =&amp; ~ \\sum_{i = 1}^n x_i \\text{ln}(p) + (1 - x_i)\\text{ln}(1-p) \\\\ \\end{aligned} \\] c. First, we will draw ten observations from a Bernoulli(0.6) distribution. We’ll use a binomial distribution with \\(n = k\\) and \\(p = 0.6\\), recognizing that the binomial is basically the sum of n independent Bernoulli trials with \\(p = 0.6\\). n &lt;- 10 trials &lt;- 1 p &lt;- 0.6 set.seed(121) x &lt;- rbinom(n = n, size = trials, prob = p) x ## [1] 1 0 1 0 1 1 1 0 1 1 Because we prefer to use log-likelihoods for mathematical reasons, let’s translate this statement into R code: \\[ \\begin{aligned} l(p) =&amp; ~ \\sum_{i = 1}^n x_i \\text{ln}(p) + (1 - x_i)\\text{ln}(1-p) \\\\ \\end{aligned} \\] ln_vector &lt;- numeric(length(x)) for(i in 1:length(x)){ ln_vector[i] &lt;- x[i]*log(p) + (1 - x[i]) * log(1 - p)} ln_vector ## [1] -0.5108256 -0.9162907 -0.5108256 -0.9162907 -0.5108256 -0.5108256 ## [7] -0.5108256 -0.9162907 -0.5108256 -0.5108256 exp(ln_vector) ## [1] 0.6 0.4 0.6 0.4 0.6 0.6 0.6 0.4 0.6 0.6 To recap, we’ve calculated the log-likelihood of each observation (0 or 1), given a probability of 0.6. Above, I also took the exponent of our vector, to return the likelihood of each observation. Assuming that these observations are IID, we can add up the individual log-likelihoods and get the total log-likelihood of the dataset (n = 10), given \\(p = 0.6\\). We can also take the exponent of the total log-likelihood to get the likelihood: sum(ln_vector) ## [1] -6.324652 exp(sum(ln_vector)) ## [1] 0.00179159 That is just for a single value of p. Now we’ll repeat this process for a vector of p from 0 to 1. First we create function that will calculate the log-likelihood of a dataset given one value of \\(p\\), based on the for loop above: # Function to calculate the log-likelihood for a vector of observations ln_bern &lt;- function(p, x){ ln_vector &lt;- numeric(length(x)) for(i in 1:length(x)){ ln_vector[i] &lt;- x[i]*log(p) + (1 - x[i]) * log(1 - p)} ln &lt;- sum(ln_vector) return(ln) } Now we create a vector for \\(p\\), loop through all the values, then plot: # Sequence of p p_vec &lt;- seq(0.001, 0.999, by = 0.001) ln_vec &lt;- numeric(length(p_vec)) # Log-likelihood of the data for(i in 1:length(p_vec)){ ln_vec[i] &lt;- ln_bern(p = p_vec[i], x = x) } # Get the value of p that maximizes the log-likelihood max_p &lt;- p_vec[which.max(ln_vec)] max_ln &lt;- ln_vec[which.max(ln_vec)] # Likelihood of the data Ln_vec &lt;- exp(ln_vec) max_Ln &lt;- Ln_vec[which.max(Ln_vec)] # Plot par(mfrow = c(1,3)) plot(table(x), xlab = &quot;&quot;, ylab = &quot;Frequency&quot;, main = &quot;10 observations, p = 0.6&quot;) plot(p_vec, Ln_vec, type = &quot;l&quot;, xlab = &quot;p&quot;, ylab = &quot;Likelihood&quot;) abline(v = 0.6, col = &quot;red&quot;, lty = &quot;dashed&quot;) points(x = max_p, y = max_Ln, pch = 19) plot(p_vec, ln_vec, type = &quot;l&quot;, xlab = &quot;p&quot;, ylab = &quot;Log likelihood&quot;) abline(v = 0.6, col = &quot;red&quot;, lty = &quot;dashed&quot;) points(x = max_p, y = max_ln, pch = 19) Figure 9.1: This is awesome. We just found the maximum likelihood estimate of p (the black points), going from the mathematical expression we derived, and translating it to R code. The red dashed line is the true value of p. Savor this moment. Below is Edge’s answer, but there is a disconnect between the answer in part b and translating the mathematical expression into R code. Specifically, his function takes a shortcut by using the binomial distribution instead of the Bernoulli. But it is a good check on my approach above: n &lt;- 10 trials &lt;- 1 p &lt;- 0.6 set.seed(121) x &lt;- rbinom(n = n, size = trials, prob = p) # Function to calculate likelihood for a vector of observations Ln_bern &lt;- function(p, x){ k &lt;- sum(x) n &lt;- length(x) Ln &lt;- numeric(length(p)) for(i in 1:length(p)){ Ln[i] &lt;- prod(p[i]^k * (1 - p[i])^(n - k)) } return(Ln) } # Sequence of p p_vec &lt;- seq(0.001, 0.999, by = 0.001) # Likelihood of the data Ln &lt;- Ln_bern(p = p_vec, x = x) max_p &lt;- p_vec[which.max(Ln)] max_Ln &lt;- Ln[which.max(Ln)] # Log-likelihood of the data ln &lt;- log(Ln) max_ln &lt;- ln[which.max(ln)] # Plot par(mfrow = c(1,3)) plot(table(x), xlab = &quot;&quot;, ylab = &quot;Frequency&quot;, main = &quot;10 observations, p = 0.6&quot;) plot(p_vec, Ln, type = &quot;l&quot;, xlab = &quot;p&quot;, ylab = &quot;Likelihood&quot;) abline(v = 0.6, col = &quot;red&quot;, lty = &quot;dashed&quot;) points(x = max_p, y = max_Ln, pch = 19) plot(p_vec, ln, type = &quot;l&quot;, xlab = &quot;p&quot;, ylab = &quot;Log likelihood&quot;) abline(v = 0.6, col = &quot;red&quot;, lty = &quot;dashed&quot;) points(x = max_p, y = max_ln, pch = 19) Figure 9.2: Same results as above. Ponder the differences in the code for calculating the likelihood. max_p ## [1] 0.7 summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00 0.25 1.00 0.70 1.00 1.00 The MLE is 0.7, which is the same as the proportion of observations equal to 1. Suppose that \\(X_1, X_2, ..., X_n\\) are distributed as Normal(\\(\\theta, \\sigma^2\\)). What is the MLE of \\(\\theta\\)? First, we get the expression for the log-likelihood of \\(\\theta\\): \\[ \\begin{aligned} l(\\theta) =&amp; ~ n~\\text{ln}(\\frac{1}{\\sigma \\sqrt {2 \\pi}}) - \\sum_{i = 1}^n \\frac{(x_i - \\theta)^2}{2 \\sigma^2} \\\\ =&amp; ~ n~\\text{ln}(\\frac{1}{\\sigma \\sqrt {2 \\pi}}) - \\frac{1}{2 \\sigma^2} \\sum_{i = 1}^n (x_i - \\theta)^2 \\\\ =&amp; ~ n~\\text{ln}(\\frac{1}{\\sigma \\sqrt {2 \\pi}}) - \\frac{1}{2 \\sigma^2} \\sum_{i = 1}^n (x_i^2 - 2x_i \\theta + \\theta^2) \\\\ \\end{aligned} \\] Then we take the derivative and show that the MLE is the sample mean, \\(\\bar x\\); see Edge solution for the remaining details. See Edge solution. 9.1.2 Maximum-likelihood estimation for simple linear regression \\[ \\begin{aligned} Y_i =&amp; \\alpha + \\beta x_i + \\epsilon \\end{aligned} \\] \\(\\alpha\\) and \\(\\beta\\) are fixed constants the observations \\(x_i\\) are no longer considered random, but are fixed constants (this is different from the method of moments estimator) the only random variable in the model is the disturbance term, \\(\\epsilon\\), and it is assumed to have a parametric distribution (~ Normal(\\(0, \\sigma^2\\))) the dependent variable is considered to be random (despite the fact that it is observed?) 9.1.2.1 Assumptions Linearity: \\(\\text{E}(\\epsilon | X = x) = ~ 0\\) Homoscedasticity: \\(\\text{Var}(\\epsilon | X = x) = 0\\) for all \\(x\\) Independence of units: for all i and \\(j \\neq i\\), \\(\\epsilon_i\\) is independent of \\(\\epsilon_j\\) Distribution: for all i, \\(\\epsilon_i\\) is drawn from a normal distribution Edge goes through the steps necessary to derive the maximum likelihood estimator for \\(\\alpha\\) and \\(\\beta\\), but I won’t repeat it here. It turns out that the MLE for these terms is equivalent to the expressions we derived using least-squares in chapter 3. Thus we can interpret the least-squares line as the maximum likelihood estimator of the ‘true’ line under the assumption of normally distributed, independent, homoscedastic disturbances It also turns out the the ML estimates are the same as the MOM estimates, but the assumptions used to justify each are different (MOM estimates do not invoke normality and constant variance of the disturbances), and thus provide different guarantees (e.g., ML estimates are asymptotically efficient and functionally invariant) 9.1.3 Exercise set 9-3 See attempted solution on paper. I was able to get close to Edge’s answer, but there is a pesky \\(n\\) term that I cannot recreate.. Skipped Skipped Skipped 9.2 Parametric interval estimation: the direct approach and Fisher information Three approaches for estimating the variance of a maximum-likelihood estimator: Use a resampling method, like the bootstrap Use the direct approach: analyze mathematically the specific model of interest; e.g., problem 2 of exercise set 9-3 carried out this direct approach for the variance of \\(\\hat \\beta\\) Use the Fisher information, a general result for maximum-likelihood estimation. This entails taking the second derivative of the log-likelihood function to understand the shape (steepness) of the function around the maximum-likelihood estimate; a steeper function indicates less uncertainty and vice versa. See Edge’s Figure 9-3. 9.2.1 Exercise set 9-4 Skipped (using the direct approach and Fisher information to get the variance of the Poisson distribution) 9.3 Parametric hypothesis testing using the Wald test As with interval estimation above, we have three options for hypothesis testing in a parametric framework: Resampling; e.g., a permutation test Direct approach, where we design a test specific to the model in question Wald test or the likelihood ratio test, both of which apply to a wide range of problems in maximum-likelihood estimation 9.3.1 Wald test Suppose we want to test the hypothesis that a parameter \\(\\theta\\) is equal to a hypothesized value \\(\\theta_0\\). Consider \\(\\theta_0\\) the null hypothesis. For many models, the asymptotic distribution of the maximum-likelihood estimator \\(\\hat \\theta\\) is normal, with an expectation equal to \\(\\theta\\). We define the test statistic, \\(W\\), as: \\[ \\begin{aligned} W =&amp; ~ \\frac{\\hat \\theta - \\theta_0}{\\sqrt{\\text{Var}(\\hat \\theta)}} \\end{aligned} \\] If the null hypothesis is true, the quantity in the numerator is 0. We can further say (though I don’t fully understand why) that for large sample sizes, this quantity is normally distributed with expectation 0 and variance 1. (why the variance is 1, is unclear). But let’s move on. We do not know \\(\\text{Var}(\\hat \\theta)\\), but we can estimate it as \\(\\widehat{\\text{Var}}(\\hat \\theta)\\) using direct methods or the observed Fisher information, giving a test statistic \\(W^*\\): \\[ \\begin{aligned} W \\approx W^* =&amp; ~ \\frac{\\hat \\theta - \\theta_0} {\\sqrt{\\widehat{\\text{Var}}(\\hat \\theta)}} \\end{aligned} \\] For large samples, if the null hypothesis is true, \\(W^*\\) will have an approximate Normal(0,1) distribution. That means we can compare the calculated \\(W^*\\) with a Normal(0,1) distribution. The approximate two-sided \\(p\\) arising from an observed value of \\(W^*\\) is: \\[ \\begin{aligned} p_W = 2 \\Phi (-|W^*|) \\end{aligned} \\] where \\(\\Phi\\) is the cumulative distribution function of the Normal(0,1) distribution. The ‘2’ is there because the test is two-sided. 9.3.2 Exercise set 9-5 Compute the \\(p\\) value for the Wald test of the hypothesis that \\(\\beta = 0\\) using the Anscombe data. # Original fertilizer use and cereal yield data d &lt;- tibble(x = anscombe$x1, y = anscombe$y1) Use the least-squares slope to find \\(\\hat\\beta\\): \\[ \\begin{aligned} &amp; \\text{(eq. 3.9)} \\\\ \\tilde\\beta &amp; = \\frac{\\sum_{i=1}^n (x_i - \\bar x) (y_i - \\bar y)} {\\sum_{i=1}^n [(x_i - \\bar x)^2]} \\end{aligned} \\] Evaluate in R: x &lt;- anscombe$x1 y &lt;- anscombe$y1 n &lt;- length(x) xbar &lt;- mean(x) ybar &lt;- mean(y) b_hat &lt;- sum((x - xbar)*(y - ybar)) / sum((x - xbar)^2) Use the least-squares intercept to find \\(\\hat\\alpha\\): \\[ \\begin{aligned} &amp; \\text{(eq. 3.8)} \\\\ \\tilde\\alpha &amp; = \\bar y - \\tilde\\beta \\bar x \\end{aligned} \\] Evaluate in R: a_hat &lt;- ybar - b_hat*xbar Use equation 9.14b to estimate the variance of the disturbance term (I don’t know how to create an upside down hat symbol..): \\[ \\begin{aligned} \\frac{n}{n-2} \\widehat{\\sigma^2} =&amp; ~ \\frac{1}{n-2} \\sum_{i = 1}^n (Y_i - \\hat\\alpha - \\hat\\beta x_i)^2 \\end{aligned} \\] v_hat_dist &lt;- sum((y - a_hat - b_hat * x)^2) / (n - 2) Now estimate the variance of the slope using equation 9.12: \\[ \\begin{aligned} &amp; \\text{(eq. 9.12)} \\\\ \\text{Var}(\\hat\\beta) &amp; = \\frac{\\sigma^2} {\\sum_{i=1}^n [(x_i - \\bar x)^2]} \\end{aligned} \\] v_hat_beta &lt;- v_hat_dist/sum((x - xbar)^2) Now we plug these estimates into our equation for \\(W^*\\): B0 &lt;- 0 wald &lt;- (b_hat - B0) / sqrt(v_hat_beta) Calculate \\(p\\): \\[ \\begin{aligned} p_W = 2 \\Phi (-|W^*|) \\end{aligned} \\] # Right tail 1 - pnorm(q = wald, mean = 0, sd = 1) ## [1] 1.110376e-05 # Left tail pnorm(q = -wald, mean = 0, sd = 1) ## [1] 1.110376e-05 # Two-sided test, using the left tail 2 * (pnorm(q = -wald, mean = 0, sd = 1)) ## [1] 2.220751e-05 Calculate \\(T\\): # Left tail of t-distribution # df = 11 observations minus two parameters that we estimated (alpha, beta) 2 * pt(q = -wald, df = 11-2) ## [1] 0.002169629 Compare with lm: fit &lt;- lm(y ~ x, data = d) summary(fit) ## ## Call: ## lm(formula = y ~ x, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.92127 -0.45577 -0.04136 0.70941 1.83882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0001 1.1247 2.667 0.02573 * ## x 0.5001 0.1179 4.241 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 9 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.6295 ## F-statistic: 17.99 on 1 and 9 DF, p-value: 0.00217 Notice that the reported t-value is equal to our calculated \\(W^*\\), and that the reported \\(p\\) uses the \\(T\\) distribution. A random variable with a \\(\\chi^2\\) distribution has the same distribution as the sum of the squares of \\(k\\) independent, Normal(0,1) random variables. We know that the asymptotic distribution of a maximum-likelihood estimator \\(\\hat\\theta\\) is normal, with the expectation equal to the true parameter value \\(\\theta\\). If \\(H_0\\) is true, then \\(W\\) is Normal(0,1). If we evaluate \\(W^2\\), then its asymptotic distribution under \\(H_0\\) is \\(\\chi^2(1)\\). (need to chew on this some more) Examining Type 1 error and power of the Wald test when the significance level is \\(\\alpha = 0.05\\). sim.Wald.B(n = 10, nsim = 100, a = 3, b = 0.1) # 10 pairs beta_vec &lt;- c(0, 0.1, 0.2) n_datasets &lt;- 1000 p_mat &lt;- matrix(nrow = n_datasets, ncol = length(beta_vec)) n_pairs &lt;- 10 for(i in 1:length(beta_vec)){ p_mat[,i] &lt;- sim.Wald.B(n = n_pairs, nsim = n_datasets, a = 3, b = beta_vec[i]) } # Create summary table df_wald &lt;- tibble(beta = c(0, 0.1, 0.2), n_10 = colMeans(p_mat &lt; 0.05)) # 50 pairs n_pairs &lt;- 50 for(i in 1:length(beta_vec)){ p_mat[,i] &lt;- sim.Wald.B(n = n_pairs, nsim = n_datasets, a = 3, b = beta_vec[i]) } # Add to summary table df_wald &lt;- df_wald %&gt;% mutate(n_50 = colMeans(p_mat &lt; 0.05)) # 100 pairs n_pairs &lt;- 100 for(i in 1:length(beta_vec)){ p_mat[,i] &lt;- sim.Wald.B(n = n_pairs, nsim = n_datasets, a = 3, b = beta_vec[i]) } # Add to summary table df_wald &lt;- df_wald %&gt;% mutate(n_100 = colMeans(p_mat &lt; 0.05)) library(knitr) kable(df_wald, caption = &quot;Proportion of significance tests for the estimated slope that were significant, where the true beta = 0, 0.1, 0.2 and the number of observations was 10, 50, 100.&quot;) Table 9.1: Proportion of significance tests for the estimated slope that were significant, where the true beta = 0, 0.1, 0.2 and the number of observations was 10, 50, 100. beta n_10 n_50 n_100 0.0 0.085 0.062 0.061 0.1 0.130 0.312 0.513 0.2 0.270 0.781 0.971 9.4 Parametric hypothesis testing using the likelihood-ratio test Likelihood-ratio tests can be used to make joint inferences about several parameters at once. They accomplish this by comparing nested models. For example, consider the full model: \\[ \\begin{aligned} Y_i &amp; = \\alpha + \\beta_1 x_i + \\beta_2 w_i + \\beta_3 z_i \\end{aligned} \\] In this full model, the free parameters are: \\(\\alpha, \\beta_1, \\beta_2, \\beta_3\\). If we give any of these parameters a fixed value - then the resulting model is considered to be nested within the full model. Often, we compare models where some of the coefficients are set to 0, indicating no effect. So, all of the following models can be considered to be nested within the full model: \\[ \\begin{aligned} Y_i &amp; = \\alpha + \\beta_3 z_i \\\\ Y_i &amp; = \\alpha + \\beta_1 x_i + \\beta_2 w_i \\\\ Y_i &amp; = \\alpha \\end{aligned} \\] The likelihood-ratio test compares the maximum likelihood of two nested models, and keeps track of the difference in the number of free parameters (\\(k\\)) in the models. The test statistic \\(\\Lambda\\) is calculated as: \\[ \\begin{aligned} \\Lambda &amp; = 2 \\text{ln} \\frac{L(\\hat\\theta)}{L(\\hat\\theta_0)} \\\\ &amp; = 2(l(\\hat\\theta) - l(\\hat\\theta_0)) \\end{aligned} \\] where \\(\\hat\\theta\\) and \\(\\hat\\theta_0\\) represent the full (or more complex) model and the nested model with a hypothesized value, respectively. In other words, \\(\\hat\\theta_0)\\) represents a null hypothesis. According to Wilks’ theorem, if the null hypothesis is true, the statistic \\(\\Lambda\\) is asymptotically distributed as \\(\\chi^2 (k)\\), where \\(k\\) is the number of parameters that are constrained in the nested model (representing the null hypothesis) but free in the full (comparison) model. The \\(p\\) value is the probability of obtaining a value of \\(\\Lambda\\) as large or larger than the one observed given that the null hypothesis is true: \\[ \\begin{aligned} p_{LRT} = 1 - F_{\\chi^2(k)}(\\Lambda) \\end{aligned} \\] where \\(F_{\\chi^2(k)}\\) is the cumulative distribution of the \\(\\chi^2\\) distribution. 9.4.1 Exercise set 9-6 Answer on paper. We wish to calculate the LRT, where the null hypothesis is that the slope is zero. \\[ \\begin{aligned} \\Lambda &amp; = 2(l(\\hat\\theta) - l(\\hat\\theta_0)) \\end{aligned} \\] Here is the formula for the log-likelihood using the least-square estimates. \\[ \\begin{aligned} l(\\mu) =&amp; ~ n~\\text{ln}(\\frac{1}{\\sigma \\sqrt {2 \\pi}}) - \\frac{1}{2\\sigma^2} \\sum_{i = 1}^n (y_i - \\alpha - \\beta x_i)^2 \\\\ \\end{aligned} \\] We can simplify this equation where \\(\\beta = 0\\) and consequently \\(\\hat\\alpha = \\bar y\\) (from the answer to 1a). Here is a function written by Edge to calculate the test statistic \\(\\Lambda\\): lr.stat.slr &lt;- function(x, y){ n &lt;- length(x) #compute MLEs of beta and alpha B.hat &lt;- (sum(x*y)-sum(x)*sum(y)/n)/( sum(x^2) - sum(x)^2/n) A.hat &lt;- (sum(y) - B.hat*sum(x))/n #Compute estimated variance of MLE of beta vhat &lt;- sum((y - A.hat - B.hat*x)^2)/(n-2) #likelihood-ratio statistic lr &lt;- (sum((y - mean(y))^2) - sum((y - A.hat - B.hat*x)^2))/vhat return(lr) } Lambda &lt;- lr.stat.slr(x, y) And we’ll calculate \\(p\\) using \\(k = 1\\) because we have fixed one parameter: 1 - pchisq(q = Lambda, df = 1) ## [1] 2.220751e-05 Compare with the Wald test: 2 * (pnorm(q = -wald, mean = 0, sd = 1)) ## [1] 2.220751e-05 This give the same answer In addition, here I use the anova function in R to do the likelihood ratio test on two linear models, one with and the other without slope term. lm1 &lt;- lm(y ~ x) summary(lm1) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.92127 -0.45577 -0.04136 0.70941 1.83882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0001 1.1247 2.667 0.02573 * ## x 0.5001 0.1179 4.241 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 9 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.6295 ## F-statistic: 17.99 on 1 and 9 DF, p-value: 0.00217 lm2 &lt;- lm(y ~ 1) summary(lm2) ## ## Call: ## lm(formula = y ~ 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.2409 -1.1859 0.0791 1.0691 3.3391 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.5009 0.6125 12.25 2.41e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.032 on 10 degrees of freedom anova(lm1, lm2) ## Analysis of Variance Table ## ## Model 1: y ~ x ## Model 2: y ~ 1 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 9 13.763 ## 2 10 41.273 -1 -27.51 17.99 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notice that the test statistic is the same, but the \\(p\\) value differs, because it appears to use the F distribution instead of the \\(\\chi^2\\) distribution for the hypothesis test. "],
["bayesian.html", "Chapter 10 Bayesian estimation and inference 10.1 How to choose a prior distribution? 10.2 The unscaled posterior, conjugacy, and sampling from the posterior 10.3 Bayesian point estimation using Bayes estimators 10.4 Bayesian interval estimation using credible intervals 10.5 Bayesian ‘hypothesis testing’ using Bayes factors 10.6 Conclusion: Bayesian vs. frequentist methods 10.7 Chapter summary", " Chapter 10 Bayesian estimation and inference Parameters Frequentist view: parameters are fixed Bayesian view: parameter are random Posterior distribution: the probability distribution of the parameter(s) being estimated, given the data observed Prior distribution: the probability distribution function used to ‘describe’ the parameter(s) before any data are observed If we have a prior, we can calculate the posterior \\(f(\\theta | D = d)\\) as follows: \\[ \\begin{aligned} f(\\theta | D = d) =&amp; ~ \\frac{f_D(d|\\theta) ~ f_{\\theta}(\\theta)} {f_D(d)} \\\\ \\end{aligned} \\] where the likelihood \\(f_D(d|\\theta)\\) is multiplied by the prior \\(f_{\\theta}(\\theta)\\) in the numerator, which is divided by the probability of the data \\(f_D(d)\\). The probability of the data is the probability of observing the data given the parameter, averaged across all parameter values weighted by the probability (or density) of each parameter value: \\[ \\begin{aligned} f_D(d) =&amp; ~ \\int_{-\\infty}^{\\infty} f_D(d|\\theta) f_{\\theta}(\\theta) d\\theta\\\\ \\end{aligned} \\] 10.1 How to choose a prior distribution? 10.2 The unscaled posterior, conjugacy, and sampling from the posterior 10.2.1 Rejection sampling algorithm Sample an observation \\(\\theta^*\\) from the prior distribution. Compute \\(m\\), the quotient of the unscaled posterior divided by the prior at \\(\\theta^*\\): \\[ \\begin{aligned} m =&amp; ~ \\frac{f_D(d|\\theta^*) ~ f_{\\theta}(\\theta^*)} {f_{\\theta}(\\theta^*)} \\\\ =&amp; ~ f_D(d|\\theta^*) \\\\ =&amp; ~ L(\\theta^* | d) \\end{aligned} \\] Independently, sample a random variable X from a continuous distribution. If X m, then “accept” \\(\\theta^*\\) as an observation from the posterior distribution and save its value. Otherwise “reject” \\(\\theta^*\\) and discard it. Repeat steps 1–4 until the desired number of simulated observations from the posterior distribution are gathered. 10.2.2 Exercise set 10-1 n &lt;- 20 true.mean &lt;- 2 known.sd &lt;- 1 prior.mean &lt;- 0 prior.sd &lt;- 1 set.seed(8675309) z &lt;- rnorm(n,true.mean,known.sd) mean(z) ## [1] 2.149314 hist(z) Edge’s function uses equations 10.4 and 10.5 to calculate the mean and variance of the posterior distribution, which we know is normal due to conjugacy (Normal prior \\(\\times\\) Normal likelihood = Normal posterior): post.conj.norm.norm &lt;- function(z, known.sd, prior.mean, prior.sd){ xbar &lt;- mean(z) post.expec &lt;- (prior.mean / prior.sd^2 + xbar*length(z) / known.sd^2)/(1 / prior.sd^2 + length(z) / known.sd^2) post.var &lt;- 1 / (1 / prior.sd^2 + length(z) / known.sd^2) list(&quot;posterior.expectation&quot; = post.expec, &quot;posterior.variance&quot; = post.var) } post_conjugate &lt;- post.conj.norm.norm(z, known.sd, prior.mean, prior.sd) Using MCMC library(MCMCpack) mn.mod &lt;- MCnormalnormal(z, sigma2 = 1, mu0 = prior.mean, tau20 = prior.sd^2, mc = 10000) summary(mn.mod) ## ## Iterations = 1:10000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## 2.046645 0.219418 0.002194 0.002308 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## 1.614 1.897 2.045 2.194 2.476 Let’s plot the MCMC results in a histogram, and overlaid with a normal distribution from the conjugate expression. hist(mn.mod, freq = FALSE, border = &quot;white&quot;) x &lt;- seq(0, 4, length=100) hx &lt;- dnorm(x, mean = post_conjugate$posterior.expectation, sd = sqrt(post_conjugate$posterior.variance)) lines(x, hx, lwd = 2) The ratio of the unscaled posterior to the prior is the likelihood. See Edge. We can adjust step 2 in the above algorithm by dividing the likelihood of \\(\\theta^*\\) by the maximum likelihood: Compute \\(m\\), the quotient of the unscaled posterior divided by the prior at \\(\\theta^*\\): \\[ \\begin{aligned} m =&amp; ~ \\frac{L(\\theta^* | d)} {\\text{argmax} ~ L(\\theta | d)} \\\\ \\end{aligned} \\] d. Edge’s R function for rejection sampling #Get 1 sample under rejection sampling from normal with known sd. #z is a vector of data. get.1.samp.norm &lt;- function(z, known.sd = 1, prior.mean = 0, prior.sd = 1){ accepted &lt;- FALSE max.like &lt;- exp(sum(log(dnorm(z, mean = mean(z), sd = known.sd)))) while(accepted == FALSE){ cand &lt;- rnorm(1, prior.mean, prior.sd) like &lt;- exp(sum(log(dnorm(z, mean = cand, sd = known.sd)))) crit &lt;- like / max.like xunif &lt;- runif(1,0,1) if(xunif &lt;= crit){accepted &lt;- TRUE} } cand } # Wrapper for get.1.samp.norm() that gets rejection sample from posterior of desired size. reject.samp.norm &lt;- function(z, known.sd = 1, prior.mean = 0, prior.sd = 1, nsamps = 10000){ samps &lt;- numeric(nsamps) for(i in seq_along(samps)){ samps[i] &lt;- get.1.samp.norm(z, known.sd, prior.mean, prior.sd) } samps } #Get 1 sample under rejection sampling from normal with known sd, using unscaled likelihood #z is a vector of data. get.1.samp.norm.unscaled &lt;- function(z, known.sd = 1, prior.mean = 0, prior.sd = 1){ accepted &lt;- FALSE max.like &lt;- exp(sum(log(dnorm(z, mean = mean(z), sd = known.sd)))) while(accepted == FALSE){ cand &lt;- rnorm(1, prior.mean, prior.sd) like &lt;- exp(sum(log(dnorm(z, mean = cand, sd = known.sd)))) crit &lt;- like #/ max.like xunif &lt;- runif(1,0,1) if(xunif &lt;= crit){accepted &lt;- TRUE} } cand } # Wrapper for get.1.samp.norm() that gets rejection sample from posterior of desired size. reject.samp.norm.unscaled &lt;- function(z, known.sd = 1, prior.mean = 0, prior.sd = 1, nsamps = 10000){ samps &lt;- numeric(nsamps) for(i in seq_along(samps)){ samps[i] &lt;- get.1.samp.norm.unscaled(z, known.sd, prior.mean, prior.sd) } samps } post_rej &lt;- reject.samp.norm(z = z, known.sd = known.sd, prior.mean = prior.mean, prior.sd = prior.sd, nsamps = 10000) # post_rej_unscaled &lt;- reject.samp.norm.unscaled(z = z, known.sd = known.sd, # prior.mean = prior.mean, prior.sd = prior.sd, nsamps = 10000) # takes much longer par(mfrow = c(1,2)) hist(mn.mod, freq = FALSE, border = &quot;white&quot;) x &lt;- seq(0, 4, length=100) pd_conjugate &lt;- dnorm(x, mean = post_conjugate$posterior.expectation, sd = sqrt(post_conjugate$posterior.variance)) lines(x, pd_conjugate, lwd = 2) hist(post_rej, freq = FALSE, border = &quot;white&quot;) lines(x, pd_conjugate, lwd = 2) 10.3 Bayesian point estimation using Bayes estimators The Bayesian approach to point estimation is to report a standard indicator of the posterior distribution’s central tendency: the posterior mean, median, or mode. These are Bayes estimators with appealing loss functions: Mean: minimizes squared error loss Median: minimizes absolute error loss Mode: minimizes zero-one loss See text for a more thorough explanation. 10.3.1 Exercise set 10-2 Fit model using maximum likelihood: library(arm) x &lt;- anscombe$x1 y &lt;- anscombe$y1 lm1 &lt;- lm(y ~ x) summary(lm1) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.92127 -0.45577 -0.04136 0.70941 1.83882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0001 1.1247 2.667 0.02573 * ## x 0.5001 0.1179 4.241 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 9 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.6295 ## F-statistic: 17.99 on 1 and 9 DF, p-value: 0.00217 Fit model using MCMCregress: b0: prior mean of Beta B0: prior precision of Beta b0 &lt;- c(0, 0) reg1 &lt;- MCMCregress(y ~ x, b0 = b0, B0 = 0.0001) res1 &lt;- summary(reg1)$statistics[,1:2] %&gt;% as_tibble(rownames = &quot;parameter&quot;) %&gt;% mutate(precision = 0.0001) reg2 &lt;- MCMCregress(y ~ x, b0 = b0, B0 = 1) res2 &lt;- summary(reg2)$statistics[,1:2] %&gt;% as_tibble(rownames = &quot;parameter&quot;) %&gt;% mutate(precision = 1) reg3 &lt;- MCMCregress(y ~ x, b0 = b0, B0 = 100) res3 &lt;- summary(reg3)$statistics[,1:2] %&gt;% as_tibble(rownames = &quot;parameter&quot;) %&gt;% mutate(precision = 100) df &lt;- rbind(res1, res2, res3) df %&gt;% ggplot(aes(x = precision, y = Mean)) + geom_point() + geom_errorbar(aes(ymin = Mean - SD, ymax = Mean + SD), width = 0) + facet_wrap(~ parameter, scales = &quot;free&quot;) + scale_x_log10() + coord_flip() b0 &lt;- c(3, 0) reg1 &lt;- MCMCregress(y ~ x, b0 = b0, B0 = 0.0001) res1 &lt;- summary(reg1)$statistics[,1:2] %&gt;% as_tibble(rownames = &quot;parameter&quot;) %&gt;% mutate(precision = 0.0001) reg2 &lt;- MCMCregress(y ~ x, b0 = b0, B0 = 1) res2 &lt;- summary(reg2)$statistics[,1:2] %&gt;% as_tibble(rownames = &quot;parameter&quot;) %&gt;% mutate(precision = 1) reg3 &lt;- MCMCregress(y ~ x, b0 = b0, B0 = 100) res3 &lt;- summary(reg3)$statistics[,1:2] %&gt;% as_tibble(rownames = &quot;parameter&quot;) %&gt;% mutate(precision = 100) df &lt;- rbind(res1, res2, res3) df %&gt;% ggplot(aes(x = precision, y = Mean)) + geom_point() + geom_errorbar(aes(ymin = Mean - SD, ymax = Mean + SD), width = 0) + facet_wrap(~ parameter, scales = &quot;free&quot;) + scale_x_log10() + coord_flip() b0 &lt;- c(10, -5) reg1 &lt;- MCMCregress(y ~ x, b0 = b0, B0 = 0.0001) res1 &lt;- summary(reg1)$statistics[,1:2] %&gt;% as_tibble(rownames = &quot;parameter&quot;) %&gt;% mutate(precision = 0.0001) reg2 &lt;- MCMCregress(y ~ x, b0 = b0, B0 = 1) res2 &lt;- summary(reg2)$statistics[,1:2] %&gt;% as_tibble(rownames = &quot;parameter&quot;) %&gt;% mutate(precision = 1) reg3 &lt;- MCMCregress(y ~ x, b0 = b0, B0 = 100) res3 &lt;- summary(reg3)$statistics[,1:2] %&gt;% as_tibble(rownames = &quot;parameter&quot;) %&gt;% mutate(precision = 100) df &lt;- rbind(res1, res2, res3) df %&gt;% ggplot(aes(x = precision, y = Mean)) + geom_point() + geom_errorbar(aes(ymin = Mean - SD, ymax = Mean + SD), width = 0) + facet_wrap(~ parameter, scales = &quot;free&quot;) + scale_x_log10() + coord_flip() When the precision on the prior estimates of the intercept and slope is very high, the posterior point estimates reflect the choice of priors more than the data. It is interesting to note that the variance of the disturbances is very large when the precision is very high. This is likely because the Bayesian machine has to fight between the observed data and the strong prior, and the consequence of this fight is large uncertainty. Proving claims about Bayes estimators; skipped 10.4 Bayesian interval estimation using credible intervals 10.4.1 Exercise set 10-3 10.5 Bayesian ‘hypothesis testing’ using Bayes factors 10.5.1 Exercise set 10-4 10.6 Conclusion: Bayesian vs. frequentist methods 10.7 Chapter summary "]
]
