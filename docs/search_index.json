[
["index.html", "Notes on Statistical Thinking from Scratch Preface", " Notes on Statistical Thinking from Scratch Robin Elahi 2020-04-22 Preface This project is a set of notes for Statistical Thinking from Scratch, by M.D. Edge. The goal is to become more comfortable with the nitty gritty underlying the statistical tools I commonly use - essentially, regression. Edge takes a unique approach in that he takes a small dataset, and dissects regression ‘from scratch’. Each of these bookdown chapters corresponds to each of Edge’s 10 book chapters. The impetus for this bookdown project was the cancellation of my spring 2020 course Experimental Design and Probability due to COVID-19. So, I’ll be filling in some gaps in my stats toolkit and learning how to use bookdown. The stfs package can be installed from Github: install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;mdedge/stfspack&quot;) library(stfspack) As Edge points out in his Prelude, the typical introductory course in biological statistics (including my own) revolves around learning a series of tests: t-tests, regression, ANOVA . In STFS, the focus instead is on one procedure - simple linear regression - and takes ‘little for granted’ (translation: we’ll be working through the math, lightly). The primary dataset consists of 11 observations. To heck with big data. So in this project, you will find mostly code - I’ll leave the exposition to Edge, who does a nice job explaining the concepts in his textbook. I’ll chime in whenever it seems useful. The chapters will work through the following: 1. Probability 2. Estimation (using data to guess the values that describe a data generating process) 3. Inference (testing hypotheses about the processes that might have generated an observed dataset) How to publish a bookdown project on github https://community.rstudio.com/t/hosting-bookdown-in-github/20427 http://seankross.com/2016/11/17/How-to-Start-a-Bookdown-Book.html "],
["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction library(stfspack) library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) Figure 1.1 shows data on the amount of fertilizer applied to cropland (x-axis), and the cereal yields (y-axis), for each of 11 countries in Africa. There is a positive relationship, but is it strong? Is it weak? How are we to reason about these data? anscombe %&gt;% ggplot(aes(x1, y1)) + geom_point() + labs(x = &quot;Fertilizer consumption (kg/hectare)&quot;, y = &quot;Cereal yield (100kg/hectare)&quot;) Figure 1.1: Fertilizer consumption and cereal yield in 11 sub-Saharan African countries Statistics, that’s how! Statistics allows us to reason from data, and rests on a mathematical framework. It is worth understanding, even minimally, this framework. That’s why we are reading this book. Chapter 1 provides an overview of simple linear regression, which allows us to identify a line that ‘best’ fits the data. We can use the lm() function in R to fit a simple linear regression: mod.fit &lt;- lm(y1 ~ x1, data = anscombe) anscombe %&gt;% ggplot(aes(x1, y1)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(x = &quot;Fertilizer consumption (kg/hectare)&quot;, y = &quot;Cereal yield (100kg/hectare)&quot;) Figure 1.2: The agriculture data with a line of best fit from the simple linear regression summary(mod.fit) ## ## Call: ## lm(formula = y1 ~ x1, data = anscombe) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.92127 -0.45577 -0.04136 0.70941 1.83882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0001 1.1247 2.667 0.02573 * ## x1 0.5001 0.1179 4.241 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 9 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.6295 ## F-statistic: 17.99 on 1 and 9 DF, p-value: 0.00217 What do the following columns in the ‘Coefficients’ table refer to? the Estimates: (Intercept) and x1 Std. Error Pr(&gt;|t|) How do each of these relate to the ideas of estimation and inference? Next, a cautionary tale. Here we fit another regression model to a different set of (fake) data, that gives the exact same regression results. mod.fit2 &lt;- lm(y3 ~ x3, data = anscombe) anscombe %&gt;% ggplot(aes(x3, y3)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(x = &quot;Fertilizer consumption (kg/hectare)&quot;, y = &quot;Cereal yield (100kg/hectare)&quot;) Figure 1.3: The data underlying the analysis of the variables y3 and x3 in the anscombe data set You should treat the results of the table with suspicion, given the figure above. summary(mod.fit2) ## ## Call: ## lm(formula = y3 ~ x3, data = anscombe) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.1586 -0.6146 -0.2303 0.1540 3.2411 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0025 1.1245 2.670 0.02562 * ## x3 0.4997 0.1179 4.239 0.00218 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.236 on 9 degrees of freedom ## Multiple R-squared: 0.6663, Adjusted R-squared: 0.6292 ## F-statistic: 17.97 on 1 and 9 DF, p-value: 0.002176 The rest of the book will give us the foundation to interpret all of the values in the regression table, and the underlying assumptions of the linear model. "],
["r-eda.html", "Chapter 2 R and exploratory data analysis 2.1 Inspecting the dataframe 2.2 Histograms 2.3 Summarising data 2.4 Loops 2.5 Functions 2.6 Boxplots 2.7 Scatterplots 2.8 Exercise set 2-2", " Chapter 2 R and exploratory data analysis library(stfspack) library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) This chapter is about getting comfortable with R. Edge uses base R for data wrangling and plotting, but here I’ll also recreate the chapter exercises using the tidyverse. For a deeper dive into R and the tidyverse: Data carpentry’s ecology lesson R for data science 2.1 Inspecting the dataframe head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa glimpse(iris) ## Observations: 150 ## Variables: 5 ## $ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4… ## $ Sepal.Width &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3… ## $ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1… ## $ Petal.Width &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0… ## $ Species &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, … summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## 2.2 Histograms # Base hist(iris$Sepal.Length, xlab = &quot;Sepal Length&quot;, main = &quot;&quot;) # Tidyverse iris %&gt;% ggplot(aes(Sepal.Length)) + geom_histogram(bins = 8, col = &quot;white&quot;) + labs(x = &quot;Sepal Length&quot;) 2.3 Summarising data # Base tapply(iris$Sepal.Length, iris$Species, mean) ## setosa versicolor virginica ## 5.006 5.936 6.588 # Tidyverse iris %&gt;% group_by(Species) %&gt;% summarise(mean = mean(Sepal.Length)) ## # A tibble: 3 x 2 ## Species mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 5.01 ## 2 versicolor 5.94 ## 3 virginica 6.59 Note that the tidyverse output is a tibble (essentially a dataframe), which is a consistent feature of the tidy approach to data wrangling. Not that you can’t do this in base R with the aggregate function - which is how I used to do it BT (before tidyverse): # Base option 2 aggregate(iris$Sepal.Length, list(iris$Species), mean) ## Group.1 x ## 1 setosa 5.006 ## 2 versicolor 5.936 ## 3 virginica 6.588 # You can check the output using str() aggregate(iris$Sepal.Length, list(iris$Species), mean) %&gt;% str() ## &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ Group.1: Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 2 3 ## $ x : num 5.01 5.94 6.59 2.4 Loops for(i in 1:3){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 i &lt;- 1 print(i) ## [1] 1 unique(iris$Species) ## [1] setosa versicolor virginica ## Levels: setosa versicolor virginica for(i in unique(iris$Species)){ print(mean(iris$Sepal.Length[iris$Species == i])) } ## [1] 5.006 ## [1] 5.936 ## [1] 6.588 2.5 Functions conditional.mean &lt;- function(x, y){ for(i in unique(y)){ print(mean(x[y == i])) } } conditional.mean(x = iris$Sepal.Length, y = iris$Species) ## [1] 5.006 ## [1] 5.936 ## [1] 6.588 conditional.mean(x = iris$Sepal.Width, y = iris$Species) ## [1] 3.428 ## [1] 2.77 ## [1] 2.974 2.6 Boxplots # Base boxplot(iris$Sepal.Length ~ iris$Species, xlab = &quot;Species&quot;, ylab = &quot;Sepal length&quot;) # Tidyverse iris %&gt;% ggplot(aes(Species, Sepal.Length)) + geom_boxplot() + labs(x = &quot;Species&quot;, y = &quot;Sepal length&quot;) 2.7 Scatterplots # Base plot(iris$Sepal.Length, iris$Sepal.Width, xlab = &quot;Sepal length&quot;, ylab = &quot;Sepal width&quot;) # Tidyverse iris %&gt;% ggplot(aes(Sepal.Length, Sepal.Width)) + geom_point() + labs(x = &quot;Sepal length&quot;, y = &quot;Sepal width&quot;) With unique symbols for species: # Base plot(iris$Sepal.Length, iris$Sepal.Width, xlab = &quot;Sepal length&quot;, ylab = &quot;Sepal width&quot;, pch = as.numeric(iris$Species)) legend(&quot;topright&quot;, pch = c(1,2,3), legend = c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;)) # Tidyverse iris %&gt;% ggplot(aes(Sepal.Length, Sepal.Width, shape = Species)) + geom_point() + labs(x = &quot;Sepal length&quot;, y = &quot;Sepal width&quot;) 2.8 Exercise set 2-2 Repeat the above analyses (histogram, summarising data, boxplots) for Petal.Width. Install and load a new package gpairs. Run the following line of code. What do you see? gpairs(iris, scatter.pars = list(col = as.numeric(iris$Species))) Install and load the package stfspack if you have not already done so. install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;mdedge/stfspack&quot;) library(stfspack) "],
["best-fit-line.html", "Chapter 3 Line of best fit 3.1 Exercise set 3-1 3.2 Exercise set 3-2", " Chapter 3 Line of best fit library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) 3.1 Exercise set 3-1 Calculate least-squares intercept a and least-squares slope b for the anscombe data. I will use equations 3.8 and 3.9. x &lt;- anscombe$x1 y &lt;- anscombe$y1 xbar &lt;- mean(x) ybar &lt;- mean(y) b &lt;- sum((x - xbar)*(y - ybar)) / sum((x - xbar)^2) b ## [1] 0.5000909 a &lt;- ybar - b*xbar a ## [1] 3.000091 # Check using lm m1 &lt;- lm(y ~ x) summary(m1) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.92127 -0.45577 -0.04136 0.70941 1.83882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0001 1.1247 2.667 0.02573 * ## x 0.5001 0.1179 4.241 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 9 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.6295 ## F-statistic: 17.99 on 1 and 9 DF, p-value: 0.00217 3.2 Exercise set 3-2 On paper On paper On paper Comparing L1 and L2 lines. library(quantreg) x &lt;- anscombe$x1 y &lt;- anscombe$y1 mL1 &lt;- rq(y ~ x, tau = 0.5) mL2 &lt;- lm(y ~ x) plot(x, y) abline(mL2, lty = 1) abline(mL1, lty = 2) legend(&quot;topleft&quot;, legend = c(&quot;L2&quot;, &quot;L1&quot;), lty = c(1, 2)) Dataset with outlier. mL1 &lt;- rq(y3 ~ x3, tau = 0.5, data = anscombe) mL2 &lt;- lm(y3 ~ x3, data = anscombe) plot(y3 ~ x3, data = anscombe) abline(mL2, lty = 1) abline(mL1, lty = 2) legend(&quot;topleft&quot;, legend = c(&quot;L2&quot;, &quot;L1&quot;), lty = c(1, 2)) "],
["probability.html", "Chapter 4 Probability and random variables 4.1 Kolmogorov’s three axioms of probability 4.2 Conditional probability and independence 4.3 Bayes’ Theorem 4.4 Discrete random variables and distributions 4.5 Continuous random variables and distributions 4.6 Probability density functions 4.7 Families of distributions", " Chapter 4 Probability and random variables library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) 4.0.1 Probability vs estimation Here are two quotes from STFS that encapsulate the distinction between probability and estimation: In probability theory, we think about processes that generate data, and we ask \"what can we say about the data generated by such a process? In statistical estimation and inference, we work in the opposite direction. We start with data, and we ask \"what can we say about the process that generated these data? So, we can define probability as the study of data generated by specified processes - and that’s the focus of this chapter. Later on, when we get to data - we’ll be assuming probabilistic models - but we have to accept the fact that the data we observed were not likely to be generated by the assumed model! But we do the best we can. 4.0.2 What is a probability? Frequency view the probability of a given event is simply the proportion of times it occurs over many trials difficult to apply to statements like “what is the probability that it will rain tomorrow” and “what is the probability that a meteor will hit the earth tomorrow” (i.e., one-shot events) Degrees-of-belief view what is the degree of belief in the occurrence of an event that a rational person would have? An aside: is the universe fundamentally probabilistic or deterministic? 4.0.3 Set notation A set (\\(\\Omega\\), in STFS) is an unordered collection of objects (elements). Intersection (\\(\\cap\\)) of two sets is the set of elements that appear in both sets. Union (\\(\\cup\\)) of two sets is the set that has every element that appears in either of the two original sets. The complement of a set (\\(S^C\\)) includes all of the elements not in the original set but present in the sample space (\\(\\Omega\\)) (which contains all possible elements). Subsets are referred to as S, and have the following properties: \\[ \\begin{aligned} S \\cup S^C = \\Omega \\\\ S \\cap S^C = \\emptyset \\end{aligned} \\] where \\(\\emptyset\\) is used to denote the empty set, or the set with no elements. A Venn diagram is useful here: The notation for the probability of an event is typically denoted using either \\(P()\\) or \\([]\\): \\[ \\begin{aligned} P(event) = [event] \\\\ \\end{aligned} \\] 4.1 Kolmogorov’s three axioms of probability Probabilities of any event i (\\(E_i\\)) cannot be negative: \\[ \\begin{aligned} \\ [E_i] \\geq 0 \\\\ \\end{aligned} \\] The probability of the event that includes every outcome is 1: \\[ \\begin{aligned} \\ [\\Omega] = 1 \\\\ \\end{aligned} \\] The probability of observing either of two mutually exclusive events is the sum of their individual probabilities: \\[ \\begin{aligned} \\text{If } E_1 \\cap E_2 = \\emptyset, \\\\ \\text{then } [E_1 \\cup E_2] = [E_1] + [E_2] \\end{aligned} \\] 4.1.1 Exercise set 4-1 Given P(\\(A\\)), we would like to know P(\\(A^C\\)). We already learned the following properties of complements: \\[ \\begin{aligned} A \\cup A^C = \\Omega \\\\ A \\cap A^C = \\emptyset \\end{aligned} \\] The intersection between \\(A\\) and \\(A^C\\) is zero, meaning they don’t share any elements. The union of \\(A\\) and \\(A^C\\) is \\([\\Omega]\\). We also learned the 2nd axiom of probability, \\([\\Omega] = 1\\). Therefore: \\[ \\begin{aligned} \\ [A] + [A^C] = 1 \\\\ \\ [A^C] = 1 - [A] \\end{aligned} \\] On paper. Below I’ve pasted an image of my handwritten version of Edge’s solution - I found it easier to think about once I visualized it using a Venn diagram. 4.2 Conditional probability and independence The law of conditional probability states that: \\[ \\begin{aligned} \\ [A \\mid B] = \\frac{[A \\cap B]}{[B]} \\\\ \\end{aligned} \\] If A and B are independent, then: \\[ \\begin{aligned} \\ [A \\mid B] = [A] \\\\ \\end{aligned} \\] 4.2.1 Exercise set 4-2 If we rearrange the law of probability, we get: \\[ \\begin{aligned} \\ [A \\mid B] [B] = [A \\cap B] \\\\ \\end{aligned} \\] By definition, if A and B are independent, then we can replace \\([A \\mid B]\\) with \\([A]\\) and get: \\[ \\begin{aligned} \\ [A \\cap B] = [A] [B] \\\\ \\end{aligned} \\] If we divide both sides by \\([A]\\), we get: \\[ \\begin{aligned} \\ [B] = \\frac{[A \\cap B]}{[A]} \\\\ \\end{aligned} \\] The right-hand side of the above equation is, by the law of conditional probability, equal to \\([B \\mid A]\\) and thus: \\[ \\begin{aligned} \\ [B] = [B \\mid A]\\\\ \\end{aligned} \\] Suppose you know \\([A \\mid B]\\), \\([A]\\), and \\([B]\\). Calculate \\([B \\mid A]\\). \\[ \\begin{aligned} \\ [A \\mid B] = \\frac{[A \\cap B]}{[B]} \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\ [A \\mid B] [B] = [A \\cap B] \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\ [B \\mid A] = \\frac{[A \\cap B]}{[A]} \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\ [B \\mid A] = \\frac{[A \\mid B] [B]}{[A]} \\\\ \\end{aligned} \\] 4.3 Bayes’ Theorem 4.4 Discrete random variables and distributions Random variable: e.g., the process of rolling a die, \\(X\\) \\(X\\) is random, can take integer values 1-6 \\(X\\) is not a number yet - it is the unrealized outcome of a random process The realization of that process is an instance - which is a number Capital \\(X\\): random variable Lower case \\(x\\): instance All the probability information is contained in its distribution. In our case, \\(X\\) is a discrete random variable, meaning that the number of outcomes is countable, in principle. Two ways to represent the distribution: Probability mass function (pmf), \\(f_X(x) = P(X = x)\\) Cumulative distribution function (cdf), \\(F_X(x) = P(X \\leq x)\\) The cdf is a series of partial sums of the pmf, \\[ \\begin{aligned} F_X(x) = P(X \\leq x) = \\sum_{x_i \\leq x} f_X(x_i) \\end{aligned} \\] and increases monotonically in \\(x\\). 4.4.1 Exercise set 4-3 4.5 Continuous random variables and distributions What about values that are not countable - anything with a decimal? Impossible to get a specific number, or instance, and thus we cannot use the pmf. But we can still use the cdf! 4.5.1 Exercise set 4-4 \\(X\\) is a random variable that takes values in the interval [0, 1], and the probability distribution is uniform. Draw the cumulative distribution function \\(F_X(x)\\) for \\(x \\in [-1, 2]\\). x &lt;- c(0,1) Fx &lt;- x plot(x, Fx, type = &quot;l&quot;, xlim = c(-1,2)) lines(c(-1, 0), c(0, 0)) lines(c(1,2), c(1, 1)) If \\(X\\) were more likely to land in [0.4, 0.6] than in any other region of length 0.2, the line between 0.4 and 0.6 would be steeper. x1 &lt;- c(0,0.4) x2 &lt;- c(0.4, 0.6) x3 &lt;- c(0.6, 1) Fx1 &lt;- c(0,0.3) Fx2 &lt;- c(0.3, 0.7) Fx3 &lt;- c(0.7, 1) plot(x1, Fx1, type = &quot;l&quot;, xlim = c(-1,2), ylim = c(0,1), xlab = &quot;x&quot;, ylab = &quot;Fx&quot;) lines(x2, Fx2) lines(x3, Fx3) lines(c(-1, 0), c(0, 0)) lines(c(1,2), c(1, 1)) 4.6 Probability density functions For a continuous random variable, the pdf is the derivative of the cdf Below I have re-created Fig 4-4, with a pdf in the upper panel and a cdf in the lower panel - for an exponential random variable with rate 1. 4.6.1 Exercise set 4-6 On paper On paper 4.7 Families of distributions Two requirements for mass or density functions: Must be non-negative Sum of all values is 1 (mass), or area under curve is 1 (density) Distribution family similarly shaped distributions summaries of their behavior can be computed from the same functions but parameter values differ 4.7.1 Exercise set 4-6 On paper On paper Consider a Poisson distribution with parameter \\(\\lambda\\) = 5. If we want to know the value of the probability mass function for x = 2, \\(f_X(2)\\), we use the dpois() function: dpois(2, lambda = 5) ## [1] 0.08422434 To get the value of the cumulative distribution function \\(F_X(2)\\), we use ppois(): ppois(2, lambda = 5) ## [1] 0.124652 If we want to know the inverse of the cumulative distribution function. That is, we want to know the value of \\(q\\) that solves the equation \\(F_X(q) = p\\). What is the number \\(q\\) such that the probability tha the random variable is less than or equal to \\(q\\) is \\(p\\); \\(q\\) is the \\(pth\\) percentile of the distribution. We can get this using qpois(): qpois(0.124652, lambda = 5) ## [1] 2 The inverse of the cdf is also called the quantile function. Using the standard normal distribution (mean = 0, sd = 1): Plot the probability density function for x \\(\\in\\) [-3, 3]. x &lt;- seq(-3, 3, by = 0.1) x ## [1] -3.0 -2.9 -2.8 -2.7 -2.6 -2.5 -2.4 -2.3 -2.2 -2.1 -2.0 -1.9 -1.8 -1.7 -1.6 ## [16] -1.5 -1.4 -1.3 -1.2 -1.1 -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 ## [31] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 ## [46] 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 ## [61] 3.0 fx &lt;- dnorm(x = x) plot(x, fx, type = &quot;l&quot;, main = &quot;PDF&quot;, col = &quot;red&quot;) Plot the cdf for x \\(\\in\\) [-3, 3]. x &lt;- seq(-3, 3, by = 0.1) x ## [1] -3.0 -2.9 -2.8 -2.7 -2.6 -2.5 -2.4 -2.3 -2.2 -2.1 -2.0 -1.9 -1.8 -1.7 -1.6 ## [16] -1.5 -1.4 -1.3 -1.2 -1.1 -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 ## [31] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 ## [46] 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 ## [61] 3.0 fx &lt;- pnorm(q = x) plot(x, fx, type = &quot;l&quot;, main = &quot;CDF&quot;, col = &quot;red&quot;) What value of \\(x\\) is at the 97.5th percentile of the standard normal? qnorm(p = 0.975) ## [1] 1.959964 Simulating from a normal distribution. n &lt;- 1000 x &lt;- rnorm(n) hist(x) Simulating from a uniform distribution. n &lt;- 1000 x &lt;- runif(n, min = 0, max = 1) hist(x) Now take those values between 0 and 1, and feed them into the qnorm function to get the values at which we see those quantiles: y &lt;- qnorm(p = x, mean = 0, sd = 1) hist(y) These values are normally distributed around 0. This plot is saying that most of the probability (in fact ~95%) lies between -2 &lt; y &lt; 2 (2 standard deviations). r &lt;- seq(-3, 3, length.out = 1000) cdf &lt;- pnorm(r) #Draw the normal cumulative distribution function. plot(r, cdf, type = &quot;l&quot;, xaxs = &quot;i&quot;, yaxs = &quot;i&quot;, xlim = c(-3, 3), xlab = expression(italic(x)), ylab = expression(paste(italic(F[X]), &quot;(&quot;, italic(x), &quot;)&quot;, sep = &quot;&quot;)), lwd = 2) #Draw light grey lines representing random samples from the #standard normal distribution. x &lt;- rnorm(100) for(i in x){ lines(c(i,i), c(min(x), pnorm(i)), col = rgb(190, 190, 190, alpha = 60, max = 255)) lines(c(min(x)-1,i), c(pnorm(i), pnorm(i)), col = rgb(190, 190, 190, alpha = 60, max = 255)) } "],
["randomvars.html", "Chapter 5 Properties of random variables 5.1 Expected values and the law of large numbers 5.2 Variance and standard deviation 5.3 Joint distributions, covariance, and correlation 5.4 Conditional distribution, expectation, variance 5.5 The central limit theorem 5.6 A probabilistic model for simple linear regression", " Chapter 5 Properties of random variables library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) 5.1 Expected values and the law of large numbers 5.1.1 Exercise set 5-1 On paper samp.size &lt;- 1 n.samps &lt;- 1000 samps &lt;- rnorm(samp.size * n.samps, mean = 0, sd = 1) samp.mat &lt;- matrix(samps, ncol = n.samps) samp.means &lt;- colMeans(samp.mat) hist(samp.means) samp.size &lt;- 100 n.samps &lt;- 1000 samps &lt;-matrix(rexp(samp.size*n.samps, rate = 1), ncol = n.samps) samp.means &lt;- colMeans(samps) hist(samp.means) 5.2 Variance and standard deviation 5.3 Joint distributions, covariance, and correlation 5.4 Conditional distribution, expectation, variance 5.5 The central limit theorem 5.5.1 Exercise set 5-4 Bean machine in action! library(animation) nball &lt;- 500 #change the number of balls nlayer &lt;- 10 #change the number of rows of pegs on the board rate &lt;- 10 #change the speed at which the balls fall ani.options(nmax = nball + nlayer - 2, interval = 1/rate) quincunx(balls = nball, layers = nlayer) Exploring the beta distribution To see what the beta distribution looks like for a given set of shape parameters, set the sample size to 1. For example: library(stfspack) # dosm.beta.hist(n = 1, nsim = 10000, shape1 = 1, shape2 = 1) will give you a histogram of 10,000 observations from a beta distribution with parameters 1 and 1. If you increase the sample size, then the distribution of the sample mean gets closer to normality. Try this — starting with samples of size 1 and increasing the sample size — with the following sets of parameter values: (1, 1), (0.2, 0.2), (2, 0.5), (0.5, 2), (3, 3). Feel free to try other parameter sets — it’s fun. What do you notice? sims &lt;- 1000 s1 &lt;- 0.2 # change this s2 &lt;- 0.2 # change this par(mfrow = c(2,3)) dosm.beta.hist(n = 1, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.4899736 0.4209460 0.1771955 dosm.beta.hist(n = 4, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.50144237 0.20766420 0.04312442 dosm.beta.hist(n = 8, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.48819651 0.14883897 0.02215304 dosm.beta.hist(n = 16, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.50294463 0.10765186 0.01158892 dosm.beta.hist(n = 32, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.500572992 0.073641756 0.005423108 dosm.beta.hist(n = 64, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.501692807 0.053155822 0.002825541 Let’s deconstruct what is going on with this function, where n = 1 (we simulate 10000 observations from a single set of parameter values). dosm.beta.hist ## function (n, nsim, shape1 = 1, shape2 = 1, ...) ## { ## samps &lt;- rbeta(n * nsim, shape1, shape2) ## sim.mat &lt;- matrix(samps, nrow = nsim) ## dosm &lt;- rowMeans(sim.mat) ## hist(dosm, freq = FALSE, ...) ## x &lt;- seq(0, 1, length.out = 1000) ## lines(x, dnorm(x, mean = mean(dosm), sd = sd(dosm))) ## c(`mean of DOSM` = mean(dosm), `SD of DOSM` = sd(dosm), `var of DOSM` = var(dosm)) ## } ## &lt;bytecode: 0x7fc2364a8cf0&gt; ## &lt;environment: namespace:stfspack&gt; nsim &lt;- 10000 n &lt;- 1 s1 &lt;- 0.2 # change this s2 &lt;- 0.2 # change this samps &lt;- rbeta(n * nsim, shape1 = s1, shape2 = s2) str(samps) # here are 10,000 ## num [1:10000] 0.9997 0.3062 0.8319 0.961 0.0797 ... # We are converting the vector into a matrix # So that we can easily calculate the mean of each row sim.mat &lt;- matrix(samps, nrow = nsim) dim(sim.mat) ## [1] 10000 1 head(sim.mat) ## [,1] ## [1,] 0.99973974 ## [2,] 0.30620613 ## [3,] 0.83193250 ## [4,] 0.96099348 ## [5,] 0.07970673 ## [6,] 0.98667533 # Calculate rowmeans - with n=1, this doesn&#39;t change anything # But change n to anything bigger and inspect the dimensions of the objects dosm &lt;- rowMeans(sim.mat) str(dosm) ## num [1:10000] 0.9997 0.3062 0.8319 0.961 0.0797 ... head(dosm) # compare these values to sim.mat ## [1] 0.99973974 0.30620613 0.83193250 0.96099348 0.07970673 0.98667533 par(mfrow = c(1,1)) hist(dosm, freq = FALSE) # plotting the simulated values # Set up a vector that goes from 0 to 1 to overlay a normal distribution on the histogram x &lt;- seq(0, 1, length.out = 1000) # Now plot a normal distribution, using the mean and sd of the simulated values lines(x, dnorm(x, mean = mean(dosm), sd = sd(dosm)), col = &quot;red&quot;) The Pareto distribution is a skewed, heavy-tailed, power-law distribution used in description of social, scientific, geophysical, actuarial, and many other types of observable phenomena. It was applied originally to the distribution of wealth in a society, fitting the observation that a large portion of wealth is held by a small fraction of the population. Named after the Italian civil engineer, economist, and sociologist Vilfredo Pareto. Parameters of the rpareto function: a: shape (on the web as \\(\\alpha\\)) b: scale (on the web as \\(x_m\\)) If the shape parameter is \\(\\leq\\) 1, \\(E(X)\\) is \\(\\infty\\). If the shape parameter is \\(\\leq\\) 2, \\(Var(X)\\) is \\(\\infty\\). First we simulate many sampes of size 1000 from a Pareto distribution with shape = 4. # experiment with n and the parameters a and b n &lt;- 100 n_sims &lt;- 10000 a &lt;- 1 b &lt;- 4 x &lt;- rpareto(n = n, a = a, b = b) summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.011 5.518 8.593 14.564 14.669 174.217 # Calculate mean and sd mu &lt;- mean(x) stdev &lt;- sd(x) hist(x, freq = FALSE) # Set up a vector that goes from 0 to 1 to overlay a normal distribution on the histogram x_vals &lt;- seq(min(x), max(x), length.out = 1000) # Now plot a normal distribution, using the mean and sd of the simulated values lines(x_vals, dnorm(x_vals, mean = mu, sd = stdev), col = &quot;red&quot;) # Compare tail to normal compare.tail.to.normal ## function (x, k, mu, sigma) ## { ## mean(x &lt; (mu - k * sigma) | x &gt; (mu + k * sigma))/(1 - (pnorm(k) - ## pnorm(-k))) ## } ## &lt;bytecode: 0x7fc22fd28138&gt; ## &lt;environment: namespace:stfspack&gt; k &lt;- 2 # sds compare.tail.to.normal(x = x, k = k, mu = mu, sigma = stdev) ## [1] 0.6593368 summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.011 5.518 8.593 14.564 14.669 174.217 mu ## [1] 14.56421 stdev ## [1] 21.4814 # This gives the value of the mean, minus the value k*stdev # (i.e., an extreme negative value) # Below I will use my object stdev in place of sigma (the parameter from Edge&#39;s function) (mu - k * stdev) ## [1] -28.39859 # Extreme positive value (mu + k * stdev) ## [1] 57.52701 # This statement asks whether the value in x is an extreme value # The operator &#39;|&#39; is &#39;OR&#39; # Is x extreme negative OR extreme positive? x &lt; (mu - k * stdev) | x &gt; (mu + k * stdev) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [73] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [97] FALSE FALSE FALSE FALSE # We can get the frequencies of this logical vector using table table(x &lt; (mu - k * stdev) | x &gt; (mu + k * stdev)) ## ## FALSE TRUE ## 97 3 # Or, as Edge, does, calculate the average of TRUEs - which is simply the proportion of TRUEs mean(x &lt; (mu - k * stdev) | x &gt; (mu + k * stdev)) ## [1] 0.03 # What proportion/probability of TRUEs would we expect under a normal probability distribution? pnorm(k) # probability of observing a value less than k standard deviations above the mean ## [1] 0.9772499 pnorm(-k) # probability of observing a value less than k standard deviations below the mean ## [1] 0.02275013 (1 - (pnorm(k) - pnorm(-k))) # probability of observing an extreme value ## [1] 0.04550026 # So putting it all together, we have the ratio of: # the probability of observing an extreme value in the data, over the # the probability of observing an extreme value in a normal distribution: mean(x &lt; (mu - k * stdev) | x &gt; (mu + k * stdev))/(1 - (pnorm(k) - pnorm(-k))) ## [1] 0.6593368 compare.tail.to.normal(x = x, k = k, mu = mu, sigma = stdev) ## [1] 0.6593368 # If this ratio is &lt; 1, then the data have fewer extreme values than suggested by a normal # If this ratio is &gt; 1, then the data have more extreme values than suggested by a normal Above, I haven’t computed the means of many simulations - which is the crux of the question! So here I just paste Edge’s solution. In it, he calculates \\(E(X)\\) and \\(Var(X)\\) using the Pareto probability distribution. I have changed n and n.sim to match my values above. #Sample size per simulation (n) and number of simulations. n &lt;- 100 n.sim &lt;- 10000 #Pareto parameters. Variance is finite, and so #CLT applies, if a &gt; 2. For large a, convergence to #normal is better. With small a, convergence is slow, #especially in the tails. a &lt;- 4 b &lt;- 1 #Compute the expectation and variance of the distribution #of the sample mean. a must be above 2 for these expressions #to hold. expec.par &lt;- a*b/(a-1) var.par &lt;- a*b^2 / ((a-1)^2 * (a-2)) sd.mean &lt;- sqrt(var.par / n) #Simulate data sim &lt;- matrix(rpareto(n*n.sim, a, b), nrow = n.sim) # Each column represents ith sample taken per simulation # Each row represents a different simulation sim[1:3, 1:10] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 1.045810 1.115826 1.285527 1.007430 1.337020 2.118968 1.008379 1.577746 ## [2,] 1.132998 1.268665 1.084002 1.244079 1.216772 1.385577 1.268523 1.481353 ## [3,] 1.588239 1.189361 1.017061 1.145074 1.053308 1.079629 1.014789 1.008207 ## [,9] [,10] ## [1,] 1.623480 1.538905 ## [2,] 1.164438 1.104984 ## [3,] 1.448589 1.017469 # Compute sample means. means.sim &lt;- rowMeans(sim) str(means.sim) ## num [1:10000] 1.4 1.34 1.29 1.34 1.32 ... #Draw a histogram of the sample means along with the approximate #normal pdf that follows from the CLT. hist(means.sim, prob = TRUE) curve(dnorm(x, expec.par, sd.mean), add = TRUE, col = &#39;red&#39;) compare.tail.to.normal(means.sim, 1/2, expec.par, sd.mean) ## [1] 0.9865898 compare.tail.to.normal(means.sim, 1, expec.par, sd.mean) ## [1] 0.9255918 compare.tail.to.normal(means.sim, 2, expec.par, sd.mean) ## [1] 0.8813136 compare.tail.to.normal(means.sim, 3, expec.par, sd.mean) ## [1] 2.370549 compare.tail.to.normal(means.sim, 4, expec.par, sd.mean) ## [1] 25.25951 compare.tail.to.normal(means.sim, 5, expec.par, sd.mean) ## [1] 872.1389 compare.tail.to.normal(means.sim, 6, expec.par, sd.mean) ## [1] 101359.5 5.6 A probabilistic model for simple linear regression 5.6.1 Exercise set 5-5 Write the square of the correlation coefficient (eq. 5.30) in terms of the variance of Y (eq. 5.32) and the conditional variance of Y given X (eq. 5.31). \\[ \\begin{aligned} \\text{eq. 5.30: } \\rho_{X,Y} = \\beta \\frac{\\sigma_X}{\\sigma_Y} \\\\ \\text{eq. 5.31: } Var(Y) = \\beta^2 \\sigma_X^2 + \\sigma_{\\epsilon}^2 \\\\ \\text{eq. 5.32: } Var(Y \\mid X = x) = \\sigma_{\\epsilon}^2 \\\\ \\end{aligned} \\] Squaring \\(\\rho_{X,Y}\\), and expressing \\(Var(Y)\\) using the definition from above: \\[ \\begin{aligned} \\rho_{X,Y}^2 = \\beta^2 \\frac{\\sigma_X^2}{\\sigma_Y^2} = \\beta^2 \\frac{\\sigma_X^2}{Var(Y)} \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\rho_{X,Y}^2 = \\beta^2 \\frac{\\sigma_X^2}{\\beta^2 \\sigma_X^2 + \\sigma_{\\epsilon}^2} \\\\ \\end{aligned} \\] Some algebra… \\[ \\begin{aligned} \\rho_{X,Y}^2 = 1 - \\frac{\\sigma_{\\epsilon}^2}{\\beta^2 \\sigma_X^2 + \\sigma_{\\epsilon}^2} \\\\ \\end{aligned} \\] And we use the formulas from above again to restate as: \\[ \\begin{aligned} \\rho_{X,Y}^2 = 1 - \\frac{Var(Y \\mid X = x)}{Var(Y)} \\\\ \\end{aligned} \\] which gives us the ‘proportion of variance explained’. So if there isn’t much variance left in \\(Y\\) after conditioning on \\(X\\) (i.e., the numerator is small relative to the denominator), if we subtract it from 1, we get a high \\(r^2\\). And vice versa. Simulating a regression. library(stfspack) sim.lm ## function (n, a, b, sigma.disturb = 1, mu.x = 8, sigma.x = 2, ## rdisturb = rnorm, rx = rnorm, het.coef = 0) ## { ## x &lt;- sort(rx(n, mu.x, sigma.x)) ## disturbs &lt;- rdisturb(n, 0, sapply(sigma.disturb + scale(x) * ## het.coef, max, 0)) ## y &lt;- a + b * x + disturbs ## cbind(x, y) ## } ## &lt;bytecode: 0x7fc235353730&gt; ## &lt;environment: namespace:stfspack&gt; sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1) head(sim_0_1) ## x y ## [1,] 4.737662 5.901336 ## [2,] 4.751068 4.712086 ## [3,] 4.786227 4.857548 ## [4,] 5.045494 5.083184 ## [5,] 5.427010 5.690239 ## [6,] 5.721143 2.353452 plot(sim_0_1[,1], sim_0_1[,2]) Still using all the default values for parameters: sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1, sigma.disturb = 1, mu.x = 8, sigma.x = 2, rdisturb = rnorm, rx = rnorm, het.coef = 0) plot(sim_0_1[,1], sim_0_1[,2]) Now I’ll change one at a time: sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1, sigma.disturb = 2, mu.x = 8, sigma.x = 2, rdisturb = rnorm, rx = rnorm, het.coef = 0) plot(sim_0_1[,1], sim_0_1[,2]) sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1, sigma.disturb = 1, mu.x = 16, sigma.x = 2, rdisturb = rnorm, rx = rnorm, het.coef = 0) plot(sim_0_1[,1], sim_0_1[,2]) sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1, sigma.disturb = 1, mu.x = 8, sigma.x = 4, rdisturb = rnorm, rx = rnorm, het.coef = 0) plot(sim_0_1[,1], sim_0_1[,2]) sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1, sigma.disturb = 1, mu.x = 8, sigma.x = 2, rdisturb = rlaplace, rx = rnorm, het.coef = 0) plot(sim_0_1[,1], sim_0_1[,2]) "]
]
