[
["randomvars.html", "Chapter 5 Properties of random variables 5.1 Expected values and the law of large numbers 5.2 Variance and standard deviation 5.3 Joint distributions, covariance, and correlation 5.4 Conditional distribution, expectation, variance 5.5 The central limit theorem 5.6 A probabilistic model for simple linear regression", " Chapter 5 Properties of random variables library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) 5.1 Expected values and the law of large numbers When summarizing a probability distribution, it is useful to have a measure of: Location (Expectation; E(\\(X\\))) Dispersal (Variance; Var(\\(X\\))) The expectation of a discrete random variable is the average: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\sum_{i = 1}^{k}x_i P(X = x_i) \\\\ =&amp; \\sum_{i = 1}^{k}x_i f_X(x_i) \\\\ \\end{aligned} \\] If \\(Y\\) represents a six-sided die, then: \\[ \\begin{aligned} \\text{E}(Y) =&amp; \\sum_{i = 1}^{k}y_i f_Y(y_i) \\\\ =&amp; 1(1/6) + 2(1/6) + 3(1/6) + 4(1/6) + 5(1/6) + 6(1/6) \\\\ =&amp; 21/6 \\\\ =&amp; 7/2 \\\\ \\end{aligned} \\] If \\(X\\) is continuous: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\int_{- \\infty}^{\\infty} x f_X(x) dx \\\\ \\end{aligned} \\] Here we are integrating over the probability density function, rather than summing over the mass density function. 5.1.1 Weak law of large numbers The expectation is more like a long-term average, rather than an actual instance (7/2 is not a possible instance of a dice roll). \\(X_i\\) are i.i.d. Assume E(\\(X_1\\)) = E(\\(X_2\\)) = … = E(\\(X_n\\)) = \\(\\mu\\) Define \\(\\overline{X}_n\\) as the mean of the observations: \\[ \\begin{aligned} \\overline{X}_n = \\frac{1}{n} (X_1 + X_2 + X_3 + ... + X_n) \\end{aligned} \\] As \\(n \\rightarrow \\infty\\), \\(\\overline{X}_n\\) “converges in probability” to \\(\\mu\\). This means for any positive constant \\(\\delta\\), \\[ \\begin{aligned} lim_{n \\rightarrow \\infty} \\text{P}(|\\overline{X}_n - \\mu| &gt; \\delta) = 0 \\end{aligned} \\] 5.1.2 Handy facts about expectations The expectation of a constant times a random variable is the constant times the expecation of the random variable: \\[ \\begin{aligned} \\text{E}(aX) = a \\text{E}(X) \\end{aligned} \\] The expectation of a constant is the constant: \\[ \\begin{aligned} \\text{E}(c) = c \\end{aligned} \\] The expectation of a sum of random variables is the sum of the expectations of those random variables: \\[ \\begin{aligned} \\text{E}(X + Y) = \\text{E}(X) + \\text{E}(Y) \\end{aligned} \\] Putting all these facts together, we can calculate the expectation of two random variables \\(X\\) and \\(Y\\) as: \\[ \\begin{aligned} \\text{E}(aX + bY + c) = a \\text{E}(X) + b \\text{E}(Y) + c \\end{aligned} \\] This is called the linearity of expectation, which we will use freqeuently in the exercises. Linearity does not hold for other measures of location (e.g., median, mode). This fact accounts, in part, for the privileged status of the mean in statistics. To calculate the expectation of a function: \\[ \\begin{aligned} \\text{E}[g(X)] = \\sum_{i = 1}^{k} g(x_i)f_X(x_i) \\end{aligned} \\] \\[ \\begin{aligned} \\text{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x)f_X(x)dx \\end{aligned} \\] 5.1.3 Exercise set 5-1 1a. Expected value of a Bernoulli random variable with parameter p? \\[ \\begin{aligned} f_X(x) = \\text{P}(X = x) = p^x(1 - p)^{1-x} \\text{ for } x \\in \\text{{0, 1}} \\\\ \\end{aligned} \\] Because there are only two outcomes (0 or 1), we can compute the expectation directly: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\sum_0^1 x f_X(x) \\\\ =&amp; \\sum_0^1 x p^x(1 - p)^{1-x} \\\\ =&amp; 0 p^0(1 - p)^{1-0} + 1 p^1(1 - p)^{1-1} \\\\ =&amp; 0 p^0(1 - p)^{1} + 1 p^1(1 - p)^{0} \\\\ =&amp; 0 (1) (1-p) + p(1) \\\\ =&amp; 0 + p \\\\ =&amp; p \\end{aligned} \\] 1b. samp.size &lt;- 1 n.samps &lt;- 1000 samps &lt;- rnorm(samp.size * n.samps, mean = 0, sd = 1) samp.mat &lt;- matrix(samps, ncol = n.samps) samp.means &lt;- colMeans(samp.mat) hist(samp.means) samp.size &lt;- 100 n.samps &lt;- 1000 samps &lt;-matrix(rexp(samp.size*n.samps, rate = 1), ncol = n.samps) samp.means &lt;- colMeans(samps) hist(samp.means) 5.2 Variance and standard deviation 5.3 Joint distributions, covariance, and correlation 5.4 Conditional distribution, expectation, variance 5.5 The central limit theorem 5.5.1 Exercise set 5-4 Bean machine in action! library(animation) nball &lt;- 500 #change the number of balls nlayer &lt;- 10 #change the number of rows of pegs on the board rate &lt;- 10 #change the speed at which the balls fall ani.options(nmax = nball + nlayer - 2, interval = 1/rate) quincunx(balls = nball, layers = nlayer) Exploring the beta distribution To see what the beta distribution looks like for a given set of shape parameters, set the sample size to 1. For example: library(stfspack) # dosm.beta.hist(n = 1, nsim = 10000, shape1 = 1, shape2 = 1) will give you a histogram of 10,000 observations from a beta distribution with parameters 1 and 1. If you increase the sample size, then the distribution of the sample mean gets closer to normality. Try this — starting with samples of size 1 and increasing the sample size — with the following sets of parameter values: (1, 1), (0.2, 0.2), (2, 0.5), (0.5, 2), (3, 3). Feel free to try other parameter sets — it’s fun. What do you notice? sims &lt;- 1000 s1 &lt;- 0.2 # change this s2 &lt;- 0.2 # change this par(mfrow = c(2,3)) dosm.beta.hist(n = 1, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.4947472 0.4225966 0.1785878 dosm.beta.hist(n = 4, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.50657492 0.21206281 0.04497064 dosm.beta.hist(n = 8, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.51226131 0.14659183 0.02148917 dosm.beta.hist(n = 16, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.49665753 0.10533252 0.01109494 dosm.beta.hist(n = 32, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.506001053 0.074539023 0.005556066 dosm.beta.hist(n = 64, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.499587363 0.052092720 0.002713652 Let’s deconstruct what is going on with this function, where n = 1 (we simulate 10000 observations from a single set of parameter values). dosm.beta.hist ## function (n, nsim, shape1 = 1, shape2 = 1, ...) ## { ## samps &lt;- rbeta(n * nsim, shape1, shape2) ## sim.mat &lt;- matrix(samps, nrow = nsim) ## dosm &lt;- rowMeans(sim.mat) ## hist(dosm, freq = FALSE, ...) ## x &lt;- seq(0, 1, length.out = 1000) ## lines(x, dnorm(x, mean = mean(dosm), sd = sd(dosm))) ## c(`mean of DOSM` = mean(dosm), `SD of DOSM` = sd(dosm), `var of DOSM` = var(dosm)) ## } ## &lt;bytecode: 0x7ff09fe398f8&gt; ## &lt;environment: namespace:stfspack&gt; nsim &lt;- 10000 n &lt;- 1 s1 &lt;- 0.2 # change this s2 &lt;- 0.2 # change this samps &lt;- rbeta(n * nsim, shape1 = s1, shape2 = s2) str(samps) # here are 10,000 ## num [1:10000] 0.849 0.995 0.258 1 0.917 ... # We are converting the vector into a matrix # So that we can easily calculate the mean of each row sim.mat &lt;- matrix(samps, nrow = nsim) dim(sim.mat) ## [1] 10000 1 head(sim.mat) ## [,1] ## [1,] 0.8485321 ## [2,] 0.9948809 ## [3,] 0.2582725 ## [4,] 0.9996426 ## [5,] 0.9166422 ## [6,] 0.1991812 # Calculate rowmeans - with n=1, this doesn&#39;t change anything # But change n to anything bigger and inspect the dimensions of the objects dosm &lt;- rowMeans(sim.mat) str(dosm) ## num [1:10000] 0.849 0.995 0.258 1 0.917 ... head(dosm) # compare these values to sim.mat ## [1] 0.8485321 0.9948809 0.2582725 0.9996426 0.9166422 0.1991812 par(mfrow = c(1,1)) hist(dosm, freq = FALSE) # plotting the simulated values # Set up a vector that goes from 0 to 1 to overlay a normal distribution on the histogram x &lt;- seq(0, 1, length.out = 1000) # Now plot a normal distribution, using the mean and sd of the simulated values lines(x, dnorm(x, mean = mean(dosm), sd = sd(dosm)), col = &quot;red&quot;) The Pareto distribution is a skewed, heavy-tailed, power-law distribution used in description of social, scientific, geophysical, actuarial, and many other types of observable phenomena. It was applied originally to the distribution of wealth in a society, fitting the observation that a large portion of wealth is held by a small fraction of the population. Named after the Italian civil engineer, economist, and sociologist Vilfredo Pareto. Parameters of the rpareto function: a: shape (on the web as \\(\\alpha\\)) b: scale (on the web as \\(x_m\\)) If the shape parameter is \\(\\leq\\) 1, \\(E(X)\\) is \\(\\infty\\). If the shape parameter is \\(\\leq\\) 2, \\(Var(X)\\) is \\(\\infty\\). First we simulate many sampes of size 1000 from a Pareto distribution with shape = 4. # experiment with n and the parameters a and b n &lt;- 100 n_sims &lt;- 10000 a &lt;- 1 b &lt;- 4 x &lt;- rpareto(n = n, a = a, b = b) summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.001 5.537 7.556 25.398 14.362 532.678 # Calculate mean and sd mu &lt;- mean(x) stdev &lt;- sd(x) hist(x, freq = FALSE) # Set up a vector that goes from 0 to 1 to overlay a normal distribution on the histogram x_vals &lt;- seq(min(x), max(x), length.out = 1000) # Now plot a normal distribution, using the mean and sd of the simulated values lines(x_vals, dnorm(x_vals, mean = mu, sd = stdev), col = &quot;red&quot;) # Compare tail to normal compare.tail.to.normal ## function (x, k, mu, sigma) ## { ## mean(x &lt; (mu - k * sigma) | x &gt; (mu + k * sigma))/(1 - (pnorm(k) - ## pnorm(-k))) ## } ## &lt;bytecode: 0x7ff09f931578&gt; ## &lt;environment: namespace:stfspack&gt; k &lt;- 2 # sds compare.tail.to.normal(x = x, k = k, mu = mu, sigma = stdev) ## [1] 0.8791158 summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.001 5.537 7.556 25.398 14.362 532.678 mu ## [1] 25.39793 stdev ## [1] 66.30965 # This gives the value of the mean, minus the value k*stdev # (i.e., an extreme negative value) # Below I will use my object stdev in place of sigma (the parameter from Edge&#39;s function) (mu - k * stdev) ## [1] -107.2214 # Extreme positive value (mu + k * stdev) ## [1] 158.0172 # This statement asks whether the value in x is an extreme value # The operator &#39;|&#39; is &#39;OR&#39; # Is x extreme negative OR extreme positive? x &lt; (mu - k * stdev) | x &gt; (mu + k * stdev) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [49] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [73] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [85] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [97] FALSE FALSE FALSE FALSE # We can get the frequencies of this logical vector using table table(x &lt; (mu - k * stdev) | x &gt; (mu + k * stdev)) ## ## FALSE TRUE ## 96 4 # Or, as Edge, does, calculate the average of TRUEs - which is simply the proportion of TRUEs mean(x &lt; (mu - k * stdev) | x &gt; (mu + k * stdev)) ## [1] 0.04 # What proportion/probability of TRUEs would we expect under a normal probability distribution? pnorm(k) # probability of observing a value less than k standard deviations above the mean ## [1] 0.9772499 pnorm(-k) # probability of observing a value less than k standard deviations below the mean ## [1] 0.02275013 (1 - (pnorm(k) - pnorm(-k))) # probability of observing an extreme value ## [1] 0.04550026 # So putting it all together, we have the ratio of: # the probability of observing an extreme value in the data, over the # the probability of observing an extreme value in a normal distribution: mean(x &lt; (mu - k * stdev) | x &gt; (mu + k * stdev))/(1 - (pnorm(k) - pnorm(-k))) ## [1] 0.8791158 compare.tail.to.normal(x = x, k = k, mu = mu, sigma = stdev) ## [1] 0.8791158 # If this ratio is &lt; 1, then the data have fewer extreme values than suggested by a normal # If this ratio is &gt; 1, then the data have more extreme values than suggested by a normal Above, I haven’t computed the means of many simulations - which is the crux of the question! So here I just paste Edge’s solution. In it, he calculates \\(E(X)\\) and \\(Var(X)\\) using the Pareto probability distribution. I have changed n and n.sim to match my values above. #Sample size per simulation (n) and number of simulations. n &lt;- 100 n.sim &lt;- 10000 #Pareto parameters. Variance is finite, and so #CLT applies, if a &gt; 2. For large a, convergence to #normal is better. With small a, convergence is slow, #especially in the tails. a &lt;- 4 b &lt;- 1 #Compute the expectation and variance of the distribution #of the sample mean. a must be above 2 for these expressions #to hold. expec.par &lt;- a*b/(a-1) var.par &lt;- a*b^2 / ((a-1)^2 * (a-2)) sd.mean &lt;- sqrt(var.par / n) #Simulate data sim &lt;- matrix(rpareto(n*n.sim, a, b), nrow = n.sim) # Each column represents ith sample taken per simulation # Each row represents a different simulation sim[1:3, 1:10] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 1.439617 1.536618 1.005086 1.068854 1.579818 1.570121 1.184395 1.018211 ## [2,] 1.438014 1.361763 1.213814 1.401818 1.872971 1.695931 1.144809 2.902138 ## [3,] 2.347026 1.071645 1.190611 1.160193 1.089018 1.182749 1.099244 1.557061 ## [,9] [,10] ## [1,] 1.162403 1.395817 ## [2,] 1.228103 1.326563 ## [3,] 1.222384 1.037988 # Compute sample means. means.sim &lt;- rowMeans(sim) str(means.sim) ## num [1:10000] 1.28 1.33 1.35 1.35 1.35 ... #Draw a histogram of the sample means along with the approximate #normal pdf that follows from the CLT. hist(means.sim, prob = TRUE) curve(dnorm(x, expec.par, sd.mean), add = TRUE, col = &#39;red&#39;) compare.tail.to.normal(means.sim, 1/2, expec.par, sd.mean) ## [1] 0.9796215 compare.tail.to.normal(means.sim, 1, expec.par, sd.mean) ## [1] 0.9445007 compare.tail.to.normal(means.sim, 2, expec.par, sd.mean) ## [1] 0.9340605 compare.tail.to.normal(means.sim, 3, expec.par, sd.mean) ## [1] 2.074231 compare.tail.to.normal(means.sim, 4, expec.par, sd.mean) ## [1] 15.78719 compare.tail.to.normal(means.sim, 5, expec.par, sd.mean) ## [1] 697.7112 compare.tail.to.normal(means.sim, 6, expec.par, sd.mean) ## [1] 50679.73 5.6 A probabilistic model for simple linear regression 5.6.1 Exercise set 5-5 Write the square of the correlation coefficient (eq. 5.30) in terms of the variance of Y (eq. 5.32) and the conditional variance of Y given X (eq. 5.31). \\[ \\begin{aligned} \\text{eq. 5.30: } \\rho_{X,Y} = \\beta \\frac{\\sigma_X}{\\sigma_Y} \\\\ \\text{eq. 5.31: } Var(Y) = \\beta^2 \\sigma_X^2 + \\sigma_{\\epsilon}^2 \\\\ \\text{eq. 5.32: } Var(Y \\mid X = x) = \\sigma_{\\epsilon}^2 \\\\ \\end{aligned} \\] Squaring \\(\\rho_{X,Y}\\), and expressing \\(Var(Y)\\) using the definition from above: \\[ \\begin{aligned} \\rho_{X,Y}^2 = \\beta^2 \\frac{\\sigma_X^2}{\\sigma_Y^2} = \\beta^2 \\frac{\\sigma_X^2}{Var(Y)} \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\rho_{X,Y}^2 = \\beta^2 \\frac{\\sigma_X^2}{\\beta^2 \\sigma_X^2 + \\sigma_{\\epsilon}^2} \\\\ \\end{aligned} \\] Some algebra… \\[ \\begin{aligned} \\rho_{X,Y}^2 = 1 - \\frac{\\sigma_{\\epsilon}^2}{\\beta^2 \\sigma_X^2 + \\sigma_{\\epsilon}^2} \\\\ \\end{aligned} \\] And we use the formulas from above again to restate as: \\[ \\begin{aligned} \\rho_{X,Y}^2 = 1 - \\frac{Var(Y \\mid X = x)}{Var(Y)} \\\\ \\end{aligned} \\] which gives us the ‘proportion of variance explained’. So if there isn’t much variance left in \\(Y\\) after conditioning on \\(X\\) (i.e., the numerator is small relative to the denominator), if we subtract it from 1, we get a high \\(r^2\\). And vice versa. Simulating a regression. library(stfspack) sim.lm ## function (n, a, b, sigma.disturb = 1, mu.x = 8, sigma.x = 2, ## rdisturb = rnorm, rx = rnorm, het.coef = 0) ## { ## x &lt;- sort(rx(n, mu.x, sigma.x)) ## disturbs &lt;- rdisturb(n, 0, sapply(sigma.disturb + scale(x) * ## het.coef, max, 0)) ## y &lt;- a + b * x + disturbs ## cbind(x, y) ## } ## &lt;bytecode: 0x7ff09e037270&gt; ## &lt;environment: namespace:stfspack&gt; sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1) head(sim_0_1) ## x y ## [1,] 3.872151 4.665735 ## [2,] 4.552216 4.218760 ## [3,] 4.619204 3.729991 ## [4,] 5.241229 5.111264 ## [5,] 5.268996 5.984025 ## [6,] 5.412779 5.464499 plot(sim_0_1[,1], sim_0_1[,2]) Still using all the default values for parameters: sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1, sigma.disturb = 1, mu.x = 8, sigma.x = 2, rdisturb = rnorm, rx = rnorm, het.coef = 0) plot(sim_0_1[,1], sim_0_1[,2]) Now I’ll change one at a time: sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1, sigma.disturb = 2, mu.x = 8, sigma.x = 2, rdisturb = rnorm, rx = rnorm, het.coef = 0) plot(sim_0_1[,1], sim_0_1[,2]) sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1, sigma.disturb = 1, mu.x = 16, sigma.x = 2, rdisturb = rnorm, rx = rnorm, het.coef = 0) plot(sim_0_1[,1], sim_0_1[,2]) sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1, sigma.disturb = 1, mu.x = 8, sigma.x = 4, rdisturb = rnorm, rx = rnorm, het.coef = 0) plot(sim_0_1[,1], sim_0_1[,2]) sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1, sigma.disturb = 1, mu.x = 8, sigma.x = 2, rdisturb = rlaplace, rx = rnorm, het.coef = 0) plot(sim_0_1[,1], sim_0_1[,2]) "]
]
