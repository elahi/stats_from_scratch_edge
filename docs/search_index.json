[
["bayesian.html", "Chapter 10 Bayesian estimation and inference 10.1 How to choose a prior distribution? 10.2 The unscaled posterior, conjugacy, and sampling from the posterior 10.3 Bayesian point estimation using Bayes estimators 10.4 Bayesian interval estimation using credible intervals 10.5 Bayesian ‘hypothesis testing’ using Bayes factors 10.6 Conclusion: Bayesian vs. frequentist methods 10.7 Chapter summary", " Chapter 10 Bayesian estimation and inference Parameters Frequentist view: parameters are fixed Bayesian view: parameter are random Posterior distribution: the probability distribution of the parameter(s) being estimated, given the data observed Prior distribution: the probability distribution function used to ‘describe’ the parameter(s) before any data are observed If we have a prior, we can calculate the posterior \\(f(\\theta | D = d)\\) as follows: \\[ \\begin{aligned} f(\\theta | D = d) =&amp; ~ \\frac{f_D(d|\\theta) ~ f_{\\theta}(\\theta)} {f_D(d)} \\\\ \\end{aligned} \\] where the likelihood \\(f_D(d|\\theta)\\) is multiplied by the prior \\(f_{\\theta}(\\theta)\\) in the numerator, which is divided by the probability of the data \\(f_D(d)\\). The probability of the data is the probability of observing the data given the parameter, averaged across all parameter values weighted by the probability (or density) of each parameter value: \\[ \\begin{aligned} f_D(d) =&amp; ~ \\int_{-\\infty}^{\\infty} f_D(d|\\theta) f_{\\theta}(\\theta) d\\theta\\\\ \\end{aligned} \\] 10.1 How to choose a prior distribution? 10.2 The unscaled posterior, conjugacy, and sampling from the posterior 10.2.1 Rejection sampling algorithm Sample an observation \\(\\theta^*\\) from the prior distribution. Compute \\(m\\), the quotient of the unscaled posterior divided by the prior at \\(\\theta^*\\): \\[ \\begin{aligned} m =&amp; ~ \\frac{f_D(d|\\theta^*) ~ f_{\\theta}(\\theta^*)} {f_{\\theta}(\\theta^*)} \\\\ =&amp; ~ f_D(d|\\theta^*) \\\\ =&amp; ~ L(\\theta^* | d) \\end{aligned} \\] Independently, sample a random variable X from a continuous distribution. If X m, then “accept” \\(\\theta^*\\) as an observation from the posterior distribution and save its value. Otherwise “reject” \\(\\theta^*\\) and discard it. Repeat steps 1–4 until the desired number of simulated observations from the posterior distribution are gathered. 10.2.2 Exercise set 10-1 n &lt;- 20 true.mean &lt;- 2 known.sd &lt;- 1 prior.mean &lt;- 0 prior.sd &lt;- 1 set.seed(8675309) z &lt;- rnorm(n,true.mean,known.sd) mean(z) ## [1] 2.149314 hist(z) Edge’s function uses equations 10.4 and 10.5 to calculate the mean and variance of the posterior distribution, which we know is normal due to conjugacy (Normal prior \\(\\times\\) Normal likelihood = Normal posterior): post.conj.norm.norm &lt;- function(z, known.sd, prior.mean, prior.sd){ xbar &lt;- mean(z) post.expec &lt;- (prior.mean / prior.sd^2 + xbar*length(z) / known.sd^2)/(1 / prior.sd^2 + length(z) / known.sd^2) post.var &lt;- 1 / (1 / prior.sd^2 + length(z) / known.sd^2) list(&quot;posterior.expectation&quot; = post.expec, &quot;posterior.variance&quot; = post.var) } post_conjugate &lt;- post.conj.norm.norm(z, known.sd, prior.mean, prior.sd) Using MCMC library(MCMCpack) mn.mod &lt;- MCnormalnormal(z, sigma2 = 1, mu0 = prior.mean, tau20 = prior.sd^2, mc = 10000) summary(mn.mod) ## ## Iterations = 1:10000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## 2.046645 0.219418 0.002194 0.002308 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## 1.614 1.897 2.045 2.194 2.476 Let’s plot the MCMC results in a histogram, and overlaid with a normal distribution from the conjugate expression. hist(mn.mod, freq = FALSE, border = &quot;white&quot;) x &lt;- seq(0, 4, length=100) hx &lt;- dnorm(x, mean = post_conjugate$posterior.expectation, sd = sqrt(post_conjugate$posterior.variance)) lines(x, hx, lwd = 2) The ratio of the unscaled posterior to the prior is the likelihood. See Edge. We can adjust step 2 in the above algorithm by dividing the likelihood of \\(\\theta^*\\) by the maximum likelihood: Compute \\(m\\), the quotient of the unscaled posterior divided by the prior at \\(\\theta^*\\): \\[ \\begin{aligned} m =&amp; ~ \\frac{L(\\theta^* | d)} {\\text{argmax} ~ L(\\theta | d)} \\\\ \\end{aligned} \\] d. Edge’s R function for rejection sampling #Get 1 sample under rejection sampling from normal with known sd. #z is a vector of data. get.1.samp.norm &lt;- function(z, known.sd = 1, prior.mean = 0, prior.sd = 1){ accepted &lt;- FALSE max.like &lt;- exp(sum(log(dnorm(z, mean = mean(z), sd = known.sd)))) while(accepted == FALSE){ cand &lt;- rnorm(1, prior.mean, prior.sd) like &lt;- exp(sum(log(dnorm(z, mean = cand, sd = known.sd)))) crit &lt;- like / max.like xunif &lt;- runif(1,0,1) if(xunif &lt;= crit){accepted &lt;- TRUE} } cand } # Wrapper for get.1.samp.norm() that gets rejection sample from posterior of desired size. reject.samp.norm &lt;- function(z, known.sd = 1, prior.mean = 0, prior.sd = 1, nsamps = 10000){ samps &lt;- numeric(nsamps) for(i in seq_along(samps)){ samps[i] &lt;- get.1.samp.norm(z, known.sd, prior.mean, prior.sd) } samps } #Get 1 sample under rejection sampling from normal with known sd, using unscaled likelihood #z is a vector of data. get.1.samp.norm.unscaled &lt;- function(z, known.sd = 1, prior.mean = 0, prior.sd = 1){ accepted &lt;- FALSE max.like &lt;- exp(sum(log(dnorm(z, mean = mean(z), sd = known.sd)))) while(accepted == FALSE){ cand &lt;- rnorm(1, prior.mean, prior.sd) like &lt;- exp(sum(log(dnorm(z, mean = cand, sd = known.sd)))) crit &lt;- like #/ max.like xunif &lt;- runif(1,0,1) if(xunif &lt;= crit){accepted &lt;- TRUE} } cand } # Wrapper for get.1.samp.norm() that gets rejection sample from posterior of desired size. reject.samp.norm.unscaled &lt;- function(z, known.sd = 1, prior.mean = 0, prior.sd = 1, nsamps = 10000){ samps &lt;- numeric(nsamps) for(i in seq_along(samps)){ samps[i] &lt;- get.1.samp.norm.unscaled(z, known.sd, prior.mean, prior.sd) } samps } post_rej &lt;- reject.samp.norm(z = z, known.sd = known.sd, prior.mean = prior.mean, prior.sd = prior.sd, nsamps = 10000) # post_rej_unscaled &lt;- reject.samp.norm.unscaled(z = z, known.sd = known.sd, # prior.mean = prior.mean, prior.sd = prior.sd, nsamps = 10000) # takes much longer par(mfrow = c(1,2)) hist(mn.mod, freq = FALSE, border = &quot;white&quot;) x &lt;- seq(0, 4, length=100) pd_conjugate &lt;- dnorm(x, mean = post_conjugate$posterior.expectation, sd = sqrt(post_conjugate$posterior.variance)) lines(x, pd_conjugate, lwd = 2) hist(post_rej, freq = FALSE, border = &quot;white&quot;) lines(x, pd_conjugate, lwd = 2) 10.3 Bayesian point estimation using Bayes estimators 10.3.1 Exercise set 10-2 10.4 Bayesian interval estimation using credible intervals 10.4.1 Exercise set 10-3 10.5 Bayesian ‘hypothesis testing’ using Bayes factors 10.5.1 Exercise set 10-4 10.6 Conclusion: Bayesian vs. frequentist methods 10.7 Chapter summary "]
]
