[
["parametric.html", "Chapter 9 Parametric estimation and inference 9.1 Parametric estimation using maximum likelihood 9.2 Parametric interval estimation: the direct approach and Fisher information 9.3 Parametric hypothesis testing using the Wald test 9.4 Parametric hypothesis testing using the likelihood-ratio test", " Chapter 9 Parametric estimation and inference If all of the random variables in our model can be described by a finite set of parameters, we are using parametric estimation and inference. In paremetric estimation, the most important mathematical concept is the likelihood function, or likelihood. The likelihood allows us to compare values of a parameter in terms of their (the values’) ability to explain the observed data. In other words, if the true value of the parameter is y, how likely is the observed dataset? Suppose the observations, which we call \\(d\\), are instances drawn from a random variable \\(D\\), which is governed by a probability distribution function. We do not know what this probability distribution is! We have to think about the data - are they continuous or discrete; are they bounded at 0 or elsewhere; is the variance uniform or not? Based on the answers to these questions, we then assume a probability distribution function, \\(f_D\\). We can use \\(f_D\\) to evaluate the probability of the data, \\(d\\), at every potential value of the parameter \\(\\theta\\) we are trying to estimate, and we call this the likelihood \\(L(\\theta)\\): \\[ \\begin{aligned} L(\\theta) = f_D(d | \\theta) \\end{aligned} \\] It is tempting to think of the likelihood as a probability distribution. But it is not, because the function does not sum or integrate to 1 with respect to \\(\\theta\\) (which is a requirement for a probability distribution function). This is partly why we are defining it (the likelihood) as separate idea: the likelihood is a function of the parameters, and we use it to ask questions about the plausibility of the data (which are fixed), assuming different values of the parameters. If \\(L(\\theta_1|d) &gt; L(\\theta_2|d)\\), then the data we have observed are more likely to have occurred if \\(\\theta = \\theta_1\\) than \\(\\theta = \\theta_2\\). We interpret this result as: \\(\\theta_1\\) is a more plausible value for \\(\\theta\\) than \\(\\theta_2\\). Ok, so how do we actually calculate the likelihood? Suppose the data are \\(n\\) independent observations, \\(x_1, x_2, ..., x_n\\) each with the density function \\(f_X(x)\\), which depends on parameter \\(\\theta\\). This means that these observations are independent and identically distributed, and the joint density function (of the data and the likelihood) is given by: \\[ \\begin{aligned} f_{X_1, X_2, ..., X_n}(x_1, x_2, ..., x_n) =&amp; ~ f_X(x_1) * f_X(x_2) * ... * f_X(x_n) \\\\ =&amp; ~ \\prod_{i = 1}^n f_X(x_i) \\end{aligned} \\] where the symbol \\(\\prod\\) denotes multiplication in the same way that the symbol \\(\\sum\\) denotes addition. Instead of multiplying, we’d prefer to work with sums for numerical reasons. Because the log of a product is the sum of the logarithms of the terms being multiplied, \\[ \\begin{aligned} \\text{ln}(yz) = \\text{ln}(y) + \\text{ln}(z) \\end{aligned} \\] we can express the joint density function as a sum instead: \\[ \\begin{aligned} \\text{ln}[f_X(x_1) * f_X(x_2) * ... * f_X(x_n)] =&amp; ~ \\sum_{i = 1}^n \\text{ln}[f_X(x_i)] \\end{aligned} \\] and define it as follows: \\[ \\begin{aligned} l(\\theta) = \\text{ln}[L(\\theta)] \\end{aligned} \\] The likelihood provides a framework for estimation and inference. 9.0.1 Exercise set 9-1 Is this statement true / false? The value of \\(\\theta\\) that maximizes \\(L(\\theta)\\) is the most probable value of \\(\\theta\\) given the observed data. False - frequentists consider \\(\\theta\\) to be fixed, and thus has no probability distribution. See Edge explanation. Change to: The value of \\(\\theta\\) that maximizes \\(L(\\theta)\\) is the one that maximizes the probability of obtaining the observed data. Write all of the following in the simplest form: The pdf of \\(X\\), a Normal(\\(\\mu, \\sigma^2\\)) random variable: \\[ \\begin{aligned} f_X(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}} \\end{aligned} \\] The log-likelihood of \\(\\mu\\) given an observation of \\(x\\), which is assumed to be an instance of \\(X\\): \\[ \\begin{aligned} l(\\mu) =&amp; ~ \\text{ln}(\\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}}) \\\\ =&amp; ~ \\text{ln}(\\frac{1}{\\sigma \\sqrt{2 \\pi}}) + \\text{ln}(e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}}) \\\\ =&amp; ~ \\text{ln}(\\frac{1}{\\sigma \\sqrt{2 \\pi}}) - \\frac{(x - \\mu)^2}{2 \\sigma^2} \\\\ \\end{aligned} \\] The joint density of \\(X_1\\) and \\(X_2\\), two independent Normal(\\(\\mu, \\sigma^2\\)) random variables. \\[ \\begin{aligned} f_{X_1, X_2}(x_1, x_2) =&amp; ~ \\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{(x_1 - \\mu)^2}{2 \\sigma^2}} * \\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{(x_2 - \\mu)^2}{2 \\sigma^2}} \\\\ &amp; \\text{consider each of these terms as a, b, and c:} \\\\ =&amp; ~ (ab)*(ac) \\\\ =&amp; ~ a^2(bc) \\\\ &amp; \\text{therefore we can write} \\\\ =&amp; ~ \\frac{1}{\\sigma^2 2 \\pi} e^{-\\frac{(x_1 - \\mu)^2}{2 \\sigma^2}} * e^{-\\frac{(x_2 - \\mu)^2}{2 \\sigma^2}} \\\\ &amp; \\text{we know that } a^m * a^n = a^{m + n} \\text{, so} \\\\ =&amp; ~ \\frac{1}{\\sigma^2 2 \\pi} e^{-\\frac{(x_1 - \\mu)^2} {2 \\sigma^2} - \\frac{(x_2 - \\mu)^2}{2 \\sigma^2}} \\\\ &amp; \\text{we can factor out a -1} \\\\ =&amp; ~ \\frac{1}{\\sigma^2 2 \\pi} e^{- \\left ( \\frac{(x_1 - \\mu)^2} {2 \\sigma^2} + \\frac{(x_2 - \\mu)^2}{2 \\sigma^2} \\right )} \\\\ &amp; \\text{and we are left with} \\\\ =&amp; ~ \\frac{1}{\\sigma^2 2 \\pi} e^{-\\frac{(x_1 - \\mu)^2 + (x_2 - \\mu)^2}{2 \\sigma^2}} \\\\ \\end{aligned} \\] The log-likelihood of \\(\\mu\\) given an observation of \\(x_1\\) and \\(x_2\\), which are assumed to be instances of \\(X_1\\) and \\(X_2\\): \\[ \\begin{aligned} l(\\mu) =&amp; ~ \\text{ln} \\left (\\frac{1}{\\sigma^2 2 \\pi} e^{-\\frac{(x_1 - \\mu)^2 + (x_2 - \\mu)^2}{2 \\sigma^2}} \\right) \\\\ =&amp; ~ \\text{ln}(\\frac{1}{\\sigma^2 2 \\pi}) + \\text{ln}( e^{-\\frac{(x_1 - \\mu)^2 + (x_2 - \\mu)^2}{2 \\sigma^2}}) \\\\ =&amp; ~ \\text{ln}(\\frac{1}{\\sigma^2 2 \\pi}) -\\frac{(x_1 - \\mu)^2}{2 \\sigma^2} -\\frac{(x_2 - \\mu)^2}{2 \\sigma^2}\\\\ &amp; \\text{though I don&#39;t know how to get to Edge&#39;s answer, apart from starting with our answer in 1b:} \\\\ =&amp; ~ 2~\\text{ln}(\\frac{1}{\\sigma \\sqrt {2 \\pi}}) -\\frac{(x_1 - \\mu)^2}{2 \\sigma^2} -\\frac{(x_2 - \\mu)^2}{2 \\sigma^2}\\\\ \\end{aligned} \\] The log-likelihood of \\(\\mu\\) given observations of \\(x_1, x_2, ..., x_n\\), which are assumed to be instances of \\(n\\) independent random variables with a Normal(\\(\\mu, \\sigma^2\\)) distribution: \\[ \\begin{aligned} l(\\mu) =&amp; ~ n~\\text{ln}(\\frac{1}{\\sigma \\sqrt {2 \\pi}}) - \\sum_{i = 1}^n \\frac{(x_i - \\mu)^2}{2 \\sigma^2} \\\\ \\end{aligned} \\] 9.1 Parametric estimation using maximum likelihood The maximum-likelihood estimate of a parameter is the value of the parameter that maximizes the probability of observing the data The maximum-likelihood estimate of the parameter \\(\\theta\\) is \\(\\hat \\theta\\): \\[ \\begin{aligned} \\hat \\theta =&amp; ~ \\text{argmax} ~ L(\\theta) \\\\ &amp; \\text{or} \\\\ \\hat \\theta =&amp; ~ \\text{argmax} ~ l(\\theta) \\end{aligned} \\] ‘argmax’ means ‘argument of the maximum’ in this case, it is the value that maximizes the likelihood \\(L(\\theta)\\) or log-likelihood \\(l(\\theta)\\) usually, we use the log-likelihood to find \\(\\hat \\theta\\) To identify \\(\\hat \\theta\\): Write down the likelihood function \\(L(\\theta)\\) Take the log of likelihood function to get \\(l(\\theta)\\), and simplify Maximize \\(l(\\theta)\\) in terms of \\(\\theta\\). The value of \\(\\theta\\) that maximizes \\(l(\\theta)\\) is the maximum-likelihood estimator \\(\\hat \\theta\\) Edge goes through each of these steps to find the MLE of the parameter \\(\\lambda\\) of the exponential distribution, but I won’t repeat it here - mostly because I will probably never complete these steps in practice (if I tried to do this at all, I’d probably try to solve for the MLE numerically). The single most important assumption is that the data are actually being generated by the likelihood function we specified. If we are right, then the MLE has some desirable properties: consistency: \\(\\hat \\theta\\) converges to the true value of \\(\\theta\\) as the sample size increases asymptotically normally distributed: as the \\(n\\) approaches infinity, the distribution of the MLE approaches a normal distribution asymptotic efficiency: in the large samples, there are no consistent estimators with lower mean squared error than MLE functional invariance: this means you can estimate a value (using a defined function) based on \\(\\hat \\theta\\), and it too will be a maximum-likelihood estimator for that derived value Three (repeated) caveats, for emphasis: The MLE is NOT the value of the parameter that is most probable given the data. It is the parameter value that makes the data most probable The appeal of the MLE is efficiency, but this efficiency is defined with respect to the mean squared error, which may not be the appropriate loss function to the problem at hand The MLE is only meaningful if the model plausibly generated the data 9.1.1 Exercise set 9-2 The likelihood function for a Bernoulli distributed dataset is: \\[ \\begin{aligned} L(p) =&amp; ~ \\prod_{i = 1}^n p^{x_i} (1-p)^{1 - x_i} \\end{aligned} \\] The log-likelihood is: \\[ \\begin{aligned} l(p) =&amp; ~ \\text{ln}[L(p)] \\\\ =&amp; ~ \\text{ln} \\left [\\prod_{i = 1}^n p^{x_i} (1-p)^{1 - x_i} \\right] \\\\ =&amp; ~ \\sum_{i = 1}^n \\text{ln}[p^{x_i} (1-p)^{1 - x_i}] \\\\ =&amp; ~ \\sum_{i = 1}^n \\text{ln}[p^{x_i}] + \\text{ln}[(1-p)^{1 - x_i}] \\\\ =&amp; ~ \\sum_{i = 1}^n x_i \\text{ln}(p) + (1 - x_i)\\text{ln}(1-p) \\\\ \\end{aligned} \\] c. First, we will draw ten observations from a Bernoulli(0.6) distribution. We’ll use a binomial distribution with \\(n = k\\) and \\(p = 0.6\\), recognizing that the binomial is basically the sum of n independent Bernoulli trials with \\(p = 0.6\\). n &lt;- 10 trials &lt;- 1 p &lt;- 0.6 set.seed(121) x &lt;- rbinom(n = n, size = trials, prob = p) x ## [1] 1 0 1 0 1 1 1 0 1 1 Because we prefer to use log-likelihoods for mathematical reasons, let’s translate this statement into R code: \\[ \\begin{aligned} l(p) =&amp; ~ \\sum_{i = 1}^n x_i \\text{ln}(p) + (1 - x_i)\\text{ln}(1-p) \\\\ \\end{aligned} \\] ln_vector &lt;- numeric(length(x)) for(i in 1:length(x)){ ln_vector[i] &lt;- x[i]*log(p) + (1 - x[i]) * log(1 - p)} ln_vector ## [1] -0.5108256 -0.9162907 -0.5108256 -0.9162907 -0.5108256 -0.5108256 ## [7] -0.5108256 -0.9162907 -0.5108256 -0.5108256 exp(ln_vector) ## [1] 0.6 0.4 0.6 0.4 0.6 0.6 0.6 0.4 0.6 0.6 To recap, we’ve calculated the log-likelihood of each observation (0 or 1), given a probability of 0.6. Above, I also took the exponent of our vector, to return the likelihood of each observation. Assuming that these observations are IID, we can add up the individual log-likelihoods and get the total log-likelihood of the dataset (n = 10), given \\(p = 0.6\\). We can also take the exponent of the total log-likelihood to get the likelihood: sum(ln_vector) ## [1] -6.324652 exp(sum(ln_vector)) ## [1] 0.00179159 That is just for a single value of p. Now we’ll repeat this process for a vector of p from 0 to 1. First we create function that will calculate the log-likelihood of a dataset given one value of \\(p\\), based on the for loop above: # Function to calculate the log-likelihood for a vector of observations ln_bern &lt;- function(p, x){ ln_vector &lt;- numeric(length(x)) for(i in 1:length(x)){ ln_vector[i] &lt;- x[i]*log(p) + (1 - x[i]) * log(1 - p)} ln &lt;- sum(ln_vector) return(ln) } Now we create a vector for \\(p\\), loop through all the values, then plot: # Sequence of p p_vec &lt;- seq(0.001, 0.999, by = 0.001) ln_vec &lt;- numeric(length(p_vec)) # Log-likelihood of the data for(i in 1:length(p_vec)){ ln_vec[i] &lt;- ln_bern(p = p_vec[i], x = x) } # Get the value of p that maximizes the log-likelihood max_p &lt;- p_vec[which.max(ln_vec)] max_ln &lt;- ln_vec[which.max(ln_vec)] # Likelihood of the data Ln_vec &lt;- exp(ln_vec) max_Ln &lt;- Ln_vec[which.max(Ln_vec)] # Plot par(mfrow = c(1,3)) plot(table(x), xlab = &quot;&quot;, ylab = &quot;Frequency&quot;, main = &quot;10 observations, p = 0.6&quot;) plot(p_vec, Ln_vec, type = &quot;l&quot;, xlab = &quot;p&quot;, ylab = &quot;Likelihood&quot;) abline(v = 0.6, col = &quot;red&quot;, lty = &quot;dashed&quot;) points(x = max_p, y = max_Ln, pch = 19) plot(p_vec, ln_vec, type = &quot;l&quot;, xlab = &quot;p&quot;, ylab = &quot;Log likelihood&quot;) abline(v = 0.6, col = &quot;red&quot;, lty = &quot;dashed&quot;) points(x = max_p, y = max_ln, pch = 19) Figure 9.1: This is awesome. We just found the maximum likelihood estimate of p (the black points), going from the mathematical expression we derived, and translating it to R code. The red dashed line is the true value of p. Savor this moment. Below is Edge’s answer, but there is a disconnect between the answer in part b and translating the mathematical expression into R code. Specifically, his function takes a shortcut by using the binomial distribution instead of the Bernoulli. But it is a good check on my approach above: n &lt;- 10 trials &lt;- 1 p &lt;- 0.6 set.seed(121) x &lt;- rbinom(n = n, size = trials, prob = p) # Function to calculate likelihood for a vector of observations Ln_bern &lt;- function(p, x){ k &lt;- sum(x) n &lt;- length(x) Ln &lt;- numeric(length(p)) for(i in 1:length(p)){ Ln[i] &lt;- prod(p[i]^k * (1 - p[i])^(n - k)) } return(Ln) } # Sequence of p p_vec &lt;- seq(0.001, 0.999, by = 0.001) # Likelihood of the data Ln &lt;- Ln_bern(p = p_vec, x = x) max_p &lt;- p_vec[which.max(Ln)] max_Ln &lt;- Ln[which.max(Ln)] # Log-likelihood of the data ln &lt;- log(Ln) max_ln &lt;- ln[which.max(ln)] # Plot par(mfrow = c(1,3)) plot(table(x), xlab = &quot;&quot;, ylab = &quot;Frequency&quot;, main = &quot;10 observations, p = 0.6&quot;) plot(p_vec, Ln, type = &quot;l&quot;, xlab = &quot;p&quot;, ylab = &quot;Likelihood&quot;) abline(v = 0.6, col = &quot;red&quot;, lty = &quot;dashed&quot;) points(x = max_p, y = max_Ln, pch = 19) plot(p_vec, ln, type = &quot;l&quot;, xlab = &quot;p&quot;, ylab = &quot;Log likelihood&quot;) abline(v = 0.6, col = &quot;red&quot;, lty = &quot;dashed&quot;) points(x = max_p, y = max_ln, pch = 19) Figure 9.2: Same results as above. Ponder the differences in the code for calculating the likelihood. max_p ## [1] 0.7 summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00 0.25 1.00 0.70 1.00 1.00 The MLE is 0.7, which is the same as the proportion of observations equal to 1. Suppose that \\(X_1, X_2, ..., X_n\\) are distributed as Normal(\\(\\theta, \\sigma^2\\)). What is the MLE of \\(\\theta\\)? First, we get the expression for the log-likelihood of \\(\\theta\\): \\[ \\begin{aligned} l(\\theta) =&amp; ~ n~\\text{ln}(\\frac{1}{\\sigma \\sqrt {2 \\pi}}) - \\sum_{i = 1}^n \\frac{(x_i - \\theta)^2}{2 \\sigma^2} \\\\ =&amp; ~ n~\\text{ln}(\\frac{1}{\\sigma \\sqrt {2 \\pi}}) - \\frac{1}{2 \\sigma^2} \\sum_{i = 1}^n (x_i - \\theta)^2 \\\\ =&amp; ~ n~\\text{ln}(\\frac{1}{\\sigma \\sqrt {2 \\pi}}) - \\frac{1}{2 \\sigma^2} \\sum_{i = 1}^n (x_i^2 - 2x_i \\theta + \\theta^2) \\\\ \\end{aligned} \\] Then we take the derivative and show that the MLE is the sample mean, \\(\\bar x\\); see Edge solution for the remaining details. See Edge solution. 9.1.2 Maximum-likelihood estimation for simple linear regression \\[ \\begin{aligned} Y_i =&amp; \\alpha + \\beta x_i + \\epsilon \\end{aligned} \\] \\(\\alpha\\) and \\(\\beta\\) are fixed constants the observations \\(x_i\\) are no longer considered random, but are fixed constants (this is different from the method of moments estimator) the only random variable in the model is the disturbance term, \\(\\epsilon\\), and it is assumed to have a parametric distribution (~ Normal(\\(0, \\sigma^2\\))) the dependent variable is considered to be random (despite the fact that it is observed?) 9.1.2.1 Assumptions Linearity: \\(\\text{E}(\\epsilon | X = x) = ~ 0\\) Homoscedasticity: \\(\\text{Var}(\\epsilon | X = x) = 0\\) for all \\(x\\) Independence of units: for all i and \\(j \\neq i\\), \\(\\epsilon_i\\) is independent of \\(\\epsilon_j\\) Distribution: for all i, \\(\\epsilon_i\\) is drawn from a normal distribution Edge goes through the steps necessary to derive the maximum likelihood estimator for \\(\\alpha\\) and \\(\\beta\\), but I won’t repeat it here. It turns out that the MLE for these terms is equivalent to the expressions we derived using least-squares in chapter 3. Thus we can interpret the least-squares line as the maximum likelihood estimator of the ‘true’ line under the assumption of normally distributed, independent, homoscedastic disturbances It also turns out the the ML estimates are the same as the MOM estimates, but the assumptions used to justify each are different (MOM estimates do not invoke normality and constant variance of the disturbances), and thus provide different guarantees (e.g., ML estimates are asymptotically efficient and functionally invariant) 9.1.3 Exercise set 9-3 See attempted solution on paper. I was able to get close to Edge’s answer, but there is a pesky \\(n\\) term that I cannot recreate.. Skipped Skipped Skipped 9.2 Parametric interval estimation: the direct approach and Fisher information Three approaches for estimating the variance of a maximum-likelihood estimator: Use a resampling method, like the bootstrap Use the direct approach: analyze mathematically the specific model of interest; e.g., problem 2 of exercise set 9-3 carried out this direct approach for the variance of \\(\\hat \\beta\\) Use the Fisher information, a general result for maximum-likelihood estimation. This entails taking the second derivative of the log-likelihood function to understand the shape (steepness) of the function around the maximum-likelihood estimate; a steeper function indicates less uncertainty and vice versa. See Edge’s Figure 9-3. 9.2.1 Exercise set 9-4 Skipped (using the direct approach and Fisher information to get the variance of the Poisson distribution) 9.3 Parametric hypothesis testing using the Wald test As with interval estimation above, we have three options for hypothesis testing in a parametric framework: Resampling; e.g., a permutation test Direct approach, where we design a test specific to the model in question Wald test or the likelihood ratio test, both of which apply to a wide range of problems in maximum-likelihood estimation 9.3.1 Wald test Suppose we want to test the hypothesis that a parameter \\(\\theta\\) is equal to a hypothesized value \\(\\theta_0\\). Consider \\(\\theta_0\\) the null hypothesis. For many models, the asymptotic distribution of the maximum-likelihood estimator \\(\\hat \\theta\\) is normal, with an expectation equal to \\(\\theta\\). We define the test statistic, \\(W\\), as: \\[ \\begin{aligned} W =&amp; ~ \\frac{\\hat \\theta - \\theta_0}{\\sqrt{\\text{Var}(\\hat \\theta)}} \\end{aligned} \\] If the null hypothesis is true, the quantity in the numerator is 0. We can further say (though I don’t fully understand why) that for large sample sizes, this quantity is normally distributed with expectation 0 and variance 1. (why the variance is 1, is unclear). But let’s move on. We do not know \\(\\text{Var}(\\hat \\theta)\\), but we can estimate it as \\(\\widehat{\\text{Var}}(\\hat \\theta)\\) using direct methods or the observed Fisher information, giving a test statistic \\(W^*\\): \\[ \\begin{aligned} W \\approx W^* =&amp; ~ \\frac{\\hat \\theta - \\theta_0} {\\sqrt{\\widehat{\\text{Var}}(\\hat \\theta)}} \\end{aligned} \\] For large samples, if the null hypothesis is true, \\(W^*\\) will have an approximate Normal(0,1) distribution. That means we can compare the calculated \\(W^*\\) with a Normal(0,1) distribution. The approximate two-sided \\(p\\) arising from an observed value of \\(W^*\\) is: \\[ \\begin{aligned} p_W = 2 \\Phi (-|W^*|) \\end{aligned} \\] where \\(\\Phi\\) is the cumulative distribution function of the Normal(0,1) distribution. The ‘2’ is there because the test is two-sided. 9.3.2 Exercise set 9-5 Compute the \\(p\\) value for the Wald test of the hypothesis that \\(\\beta = 0\\) using the Anscombe data. # Original fertilizer use and cereal yield data d &lt;- tibble(x = anscombe$x1, y = anscombe$y1) Use the least-squares slope to find \\(\\hat\\beta\\): \\[ \\begin{aligned} &amp; \\text{(eq. 3.9)} \\\\ \\tilde\\beta &amp; = \\frac{\\sum_{i=1}^n (x_i - \\bar x) (y_i - \\bar y)} {\\sum_{i=1}^n [(x_i - \\bar x)^2]} \\end{aligned} \\] Evaluate in R: x &lt;- anscombe$x1 y &lt;- anscombe$y1 n &lt;- length(x) xbar &lt;- mean(x) ybar &lt;- mean(y) b_hat &lt;- sum((x - xbar)*(y - ybar)) / sum((x - xbar)^2) Use the least-squares intercept to find \\(\\hat\\alpha\\): \\[ \\begin{aligned} &amp; \\text{(eq. 3.8)} \\\\ \\tilde\\alpha &amp; = \\bar y - \\tilde\\beta \\bar x \\end{aligned} \\] Evaluate in R: a_hat &lt;- ybar - b_hat*xbar Use equation 9.14b to estimate the variance of the disturbance term (I don’t know how to create an upside down hat symbol..): \\[ \\begin{aligned} \\frac{n}{n-2} \\widehat{\\sigma^2} =&amp; ~ \\frac{1}{n-2} \\sum_{i = 1}^n (Y_i - \\hat\\alpha - \\hat\\beta x_i)^2 \\end{aligned} \\] v_hat_dist &lt;- sum((y - a_hat - b_hat * x)^2) / (n - 2) Now estimate the variance of the slope using equation 9.12: \\[ \\begin{aligned} &amp; \\text{(eq. 9.12)} \\\\ \\text{Var}(\\hat\\beta) &amp; = \\frac{\\sigma^2} {\\sum_{i=1}^n [(x_i - \\bar x)^2]} \\end{aligned} \\] v_hat_beta &lt;- v_hat_dist/sum((x - xbar)^2) Now we plug these estimates into our equation for \\(W^*\\): B0 &lt;- 0 wald &lt;- (b_hat - B0) / sqrt(v_hat_beta) Calculate \\(p\\): \\[ \\begin{aligned} p_W = 2 \\Phi (-|W^*|) \\end{aligned} \\] # Right tail 1 - pnorm(q = wald, mean = 0, sd = 1) ## [1] 1.110376e-05 # Left tail pnorm(q = -wald, mean = 0, sd = 1) ## [1] 1.110376e-05 # Two-sided test, using the left tail 2 * (pnorm(q = -wald, mean = 0, sd = 1)) ## [1] 2.220751e-05 Calculate \\(T\\): # Left tail of t-distribution # df = 11 observations minus two parameters that we estimated (alpha, beta) 2 * pt(q = -wald, df = 11-2) ## [1] 0.002169629 Compare with lm: fit &lt;- lm(y ~ x, data = d) summary(fit) ## ## Call: ## lm(formula = y ~ x, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.92127 -0.45577 -0.04136 0.70941 1.83882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0001 1.1247 2.667 0.02573 * ## x 0.5001 0.1179 4.241 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 9 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.6295 ## F-statistic: 17.99 on 1 and 9 DF, p-value: 0.00217 Notice that the reported t-value is equal to our calculated \\(W^*\\), and that the reported \\(p\\) uses the \\(T\\) distribution. A random variable with a \\(\\chi^2\\) distribution has the same distribution as the sum of the squares of \\(k\\) independent, Normal(0,1) random variables. We know that the asymptotic distribution of a maximum-likelihood estimator \\(\\hat\\theta\\) is normal, with the expectation equal to the true parameter value \\(\\theta\\). If \\(H_0\\) is true, then \\(W\\) is Normal(0,1). If we evaluate \\(W^2\\), then its asymptotic distribution under \\(H_0\\) is \\(\\chi^2(1)\\). (need to chew on this some more) Examining Type 1 error and power of the Wald test when the significance level is \\(\\alpha = 0.05\\). sim.Wald.B(n = 10, nsim = 100, a = 3, b = 0.1) # 10 pairs beta_vec &lt;- c(0, 0.1, 0.2) n_datasets &lt;- 1000 p_mat &lt;- matrix(nrow = n_datasets, ncol = length(beta_vec)) n_pairs &lt;- 10 for(i in 1:length(beta_vec)){ p_mat[,i] &lt;- sim.Wald.B(n = n_pairs, nsim = n_datasets, a = 3, b = beta_vec[i]) } # Create summary table df_wald &lt;- tibble(beta = c(0, 0.1, 0.2), n_10 = colMeans(p_mat &lt; 0.05)) # 50 pairs n_pairs &lt;- 50 for(i in 1:length(beta_vec)){ p_mat[,i] &lt;- sim.Wald.B(n = n_pairs, nsim = n_datasets, a = 3, b = beta_vec[i]) } # Add to summary table df_wald &lt;- df_wald %&gt;% mutate(n_50 = colMeans(p_mat &lt; 0.05)) # 100 pairs n_pairs &lt;- 100 for(i in 1:length(beta_vec)){ p_mat[,i] &lt;- sim.Wald.B(n = n_pairs, nsim = n_datasets, a = 3, b = beta_vec[i]) } # Add to summary table df_wald &lt;- df_wald %&gt;% mutate(n_100 = colMeans(p_mat &lt; 0.05)) library(knitr) kable(df_wald, caption = &quot;Proportion of significance tests for the estimated slope that were significant, where the true beta = 0, 0.1, 0.2 and the number of observations was 10, 50, 100.&quot;) Table 9.1: Proportion of significance tests for the estimated slope that were significant, where the true beta = 0, 0.1, 0.2 and the number of observations was 10, 50, 100. beta n_10 n_50 n_100 0.0 0.085 0.062 0.061 0.1 0.130 0.312 0.513 0.2 0.270 0.781 0.971 9.4 Parametric hypothesis testing using the likelihood-ratio test Likelihood-ratio tests can be used to make joint inferences about several parameters at once. They accomplish this by comparing nested models. For example, consider the full model: \\[ \\begin{aligned} Y_i &amp; = \\alpha + \\beta_1 x_i + \\beta_2 w_i + \\beta_3 z_i \\end{aligned} \\] In this full model, the free parameters are: \\(\\alpha, \\beta_1, \\beta_2, \\beta_3\\). If we give any of these parameters a fixed value - then the resulting model is considered to be nested within the full model. Often, we compare models where some of the coefficients are set to 0, indicating no effect. So, all of the following models can be considered to be nested within the full model: \\[ \\begin{aligned} Y_i &amp; = \\alpha + \\beta_3 z_i \\\\ Y_i &amp; = \\alpha + \\beta_1 x_i + \\beta_2 w_i \\\\ Y_i &amp; = \\alpha \\end{aligned} \\] The likelihood-ratio test compares the maximum likelihood of two nested models, and keeps track of the difference in the number of free parameters (\\(k\\)) in the models. The test statistic \\(\\Lambda\\) is calculated as: \\[ \\begin{aligned} \\Lambda &amp; = 2 \\text{ln} \\frac{L(\\hat\\theta)}{L(\\hat\\theta_0)} \\\\ &amp; = 2(l(\\hat\\theta) - l(\\hat\\theta_0)) \\end{aligned} \\] where \\(\\hat\\theta\\) and \\(\\hat\\theta_0\\) represent the full (or more complex) model and the nested model with a hypothesized value, respectively. In other words, \\(\\hat\\theta_0)\\) represents a null hypothesis. According to Wilks’ theorem, if the null hypothesis is true, the statistic \\(\\Lambda\\) is asymptotically distributed as \\(\\chi^2 (k)\\), where \\(k\\) is the number of parameters that are constrained in the nested model (representing the null hypothesis) but free in the full (comparison) model. The \\(p\\) value is the probability of obtaining a value of \\(\\Lambda\\) as large or larger than the one observed given that the null hypothesis is true: \\[ \\begin{aligned} p_{LRT} = 1 - F_{\\chi^2(k)}(\\Lambda) \\end{aligned} \\] where \\(F_{\\chi^2(k)}\\) is the cumulative distribution of the \\(\\chi^2\\) distribution. 9.4.1 Exercise set 9-6 Answer on paper. We wish to calculate the LRT, where the null hypothesis is that the slope is zero. \\[ \\begin{aligned} \\Lambda &amp; = 2(l(\\hat\\theta) - l(\\hat\\theta_0)) \\end{aligned} \\] Here is the formula for the log-likelihood using the least-square estimates. \\[ \\begin{aligned} l(\\mu) =&amp; ~ n~\\text{ln}(\\frac{1}{\\sigma \\sqrt {2 \\pi}}) - \\frac{1}{2\\sigma^2} \\sum_{i = 1}^n (y_i - \\alpha - \\beta x_i)^2 \\\\ \\end{aligned} \\] We can simplify this equation where \\(\\beta = 0\\) and consequently \\(\\hat\\alpha = \\bar y\\) (from the answer to 1a). Here is a function written by Edge to calculate the test statistic \\(\\Lambda\\): lr.stat.slr &lt;- function(x, y){ n &lt;- length(x) #compute MLEs of beta and alpha B.hat &lt;- (sum(x*y)-sum(x)*sum(y)/n)/( sum(x^2) - sum(x)^2/n) A.hat &lt;- (sum(y) - B.hat*sum(x))/n #Compute estimated variance of MLE of beta vhat &lt;- sum((y - A.hat - B.hat*x)^2)/(n-2) #likelihood-ratio statistic lr &lt;- (sum((y - mean(y))^2) - sum((y - A.hat - B.hat*x)^2))/vhat return(lr) } Lambda &lt;- lr.stat.slr(x, y) And we’ll calculate \\(p\\) using \\(k = 1\\) because we have fixed one parameter: 1 - pchisq(q = Lambda, df = 1) ## [1] 2.220751e-05 Compare with the Wald test: 2 * (pnorm(q = -wald, mean = 0, sd = 1)) ## [1] 2.220751e-05 This give the same answer In addition, here I use the anova function in R to do the likelihood ratio test on two linear models, one with and the other without slope term. lm1 &lt;- lm(y ~ x) summary(lm1) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.92127 -0.45577 -0.04136 0.70941 1.83882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0001 1.1247 2.667 0.02573 * ## x 0.5001 0.1179 4.241 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 9 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.6295 ## F-statistic: 17.99 on 1 and 9 DF, p-value: 0.00217 lm2 &lt;- lm(y ~ 1) summary(lm2) ## ## Call: ## lm(formula = y ~ 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.2409 -1.1859 0.0791 1.0691 3.3391 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.5009 0.6125 12.25 2.41e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.032 on 10 degrees of freedom anova(lm1, lm2) ## Analysis of Variance Table ## ## Model 1: y ~ x ## Model 2: y ~ 1 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 9 13.763 ## 2 10 41.273 -1 -27.51 17.99 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notice that the test statistic is the same, but the \\(p\\) value differs, because it appears to use the F distribution instead of the \\(\\chi^2\\) distribution for the hypothesis test. "]
]
