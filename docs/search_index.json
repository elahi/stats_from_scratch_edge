[
["parametric.html", "Chapter 9 Parametric estimation and inference 9.1 Parametric estimation using maximum likelihood 9.2 Parametric interval estimation: the direct approach and Fisher information 9.3 Parametric hypothesis testing using the Wald test 9.4 Parametric hypothesis testing using the likelihood-ratio test", " Chapter 9 Parametric estimation and inference If all of the random variables in our model can be described by a finite set of parameters, we are using parametric estimation and inference. In paremetric estimation, the most important mathematical concept is the likelihood function, or likelihood. The likelihood allows us to compare values of a parameter in terms of their (the values’) ability to explain the observed data. In other words, if the true value of the parameter is y, how likely is the observed dataset? Suppose the observations, which we call \\(d\\), are instances drawn from a random variable \\(D\\), which is governed by a probability distribution function. We do not know what this probability distribution is! We have to think about the data - are they continuous or discrete; are they bounded at 0 or elsewhere; is the variance uniform or not? Based on the answers to these questions, we then assume a probability distribution function, \\(f_D\\). We can use \\(f_D\\) to evaluate the probability of the data, \\(d\\), at every potential value of the parameter \\(\\theta\\) we are trying to estimate, and we call this the likelihood \\(L(\\theta)\\): \\[ \\begin{aligned} L(\\theta) = f_D(d | \\theta) \\end{aligned} \\] It is tempting to think of the likelihood as a probability distribution. But it is not, because the function does not sum or integrate to 1 with respect to \\(\\theta\\) (which is a requirement for a probability distribution function). This is partly why we are defining it (the likelihood) as separate idea: the likelihood is a function of the parameters, and we use it to ask questions about the plausibility of the data (which are fixed), assuming different values of the parameters. If \\(L(\\theta_1|d) &gt; L(\\theta_2|d)\\), then the data we have observed are more likely to have occurred if \\(\\theta = \\theta_1\\) than \\(\\theta = \\theta_2\\). We interpret this result as: \\(\\theta_1\\) is a more plausible value for \\(\\theta\\) than \\(\\theta_2\\). Ok, so how do we actually calculate the likelihood? Suppose the data are \\(n\\) independent observations, \\(x_1, x_2, ..., x_n\\) each with the density function \\(f_X(x)\\), which depends on parameter \\(\\theta\\). This means that these observations are independent and identically distributed, and the joint density function (of the data and the likelihood) is given by: \\[ \\begin{aligned} f_{X_1, X_2, ..., X_n}(x_1, x_2, ..., x_n) =&amp; ~ f_X(x_1) * f_X(x_2) * ... * f_X(x_n) \\\\ =&amp; ~ \\prod_{i = 1}^n f_X(x_i) \\end{aligned} \\] where the symbol \\(\\prod\\) denotes multiplication in the same way that the symbol \\(\\sum\\) denotes addition. Instead of multiplying, we’d prefer to work with sums for numerical reasons. Because the log of a product is the sum of the logarithms of the terms being multiplied, \\[ \\begin{aligned} \\text{ln}(yz) = \\text{ln}(y) + \\text{ln}(z) \\end{aligned} \\] we can express the joint density function as a sum instead: \\[ \\begin{aligned} \\text{ln}[f_X(x_1) * f_X(x_2) * ... * f_X(x_n)] =&amp; ~ \\sum_{i = 1}^n \\text{ln}[f_X(x_i)] \\end{aligned} \\] and define it as follows: \\[ \\begin{aligned} l(\\theta) = \\text{ln}[L(\\theta)] \\end{aligned} \\] The likelihood provides a framework for estimation and inference. 9.0.1 Exercise set 9-1 Is this statement true / false? The value of \\(\\theta\\) that maximizes \\(L(\\theta)\\) is the most probable value of \\(\\theta\\) given the observed data. False - frequentists consider \\(\\theta\\) to be fixed, and thus has no probability distribution. See Edge explanation. Change to: The value of \\(\\theta\\) that maximizes \\(L(\\theta)\\) is the one that maximizes the probability of obtaining the observed data. Write all of the following in the simplest form: The pdf of \\(X\\), a Normal(\\(\\mu, \\sigma^2\\)) random variable: \\[ \\begin{aligned} f_X(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}} \\end{aligned} \\] The log-likelihood of \\(\\mu\\) given an observation of \\(x\\), which is assumed to be an instance of \\(X\\): \\[ \\begin{aligned} l(\\mu) =&amp; ~ \\text{ln}(\\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}}) \\\\ =&amp; ~ \\text{ln}(\\frac{1}{\\sigma \\sqrt{2 \\pi}}) + \\text{ln}(e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}}) \\\\ =&amp; ~ \\text{ln}(\\frac{1}{\\sigma \\sqrt{2 \\pi}}) - \\frac{(x - \\mu)^2}{2 \\sigma^2} \\\\ \\end{aligned} \\] The joint density of \\(X_1\\) and \\(X_2\\), two independent Normal(\\(\\mu, \\sigma^2\\)) random variables. \\[ \\begin{aligned} f_{X_1, X_2}(x_1, x_2) =&amp; ~ \\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{(x_1 - \\mu)^2}{2 \\sigma^2}} * \\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{(x_2 - \\mu)^2}{2 \\sigma^2}} \\\\ &amp; \\text{consider each of these terms as a, b, and c:} \\\\ =&amp; ~ (ab)*(ac) \\\\ =&amp; ~ a^2(bc) \\\\ &amp; \\text{therefore we can write} \\\\ =&amp; ~ \\frac{1}{\\sigma^2 2 \\pi} e^{-\\frac{(x_1 - \\mu)^2}{2 \\sigma^2}} * e^{-\\frac{(x_2 - \\mu)^2}{2 \\sigma^2}} \\\\ &amp; \\text{we know that } a^m * a^n = a^{m + n} \\text{, so} \\\\ =&amp; ~ \\frac{1}{\\sigma^2 2 \\pi} e^{-\\frac{(x_1 - \\mu)^2} {2 \\sigma^2} - \\frac{(x_2 - \\mu)^2}{2 \\sigma^2}} \\\\ &amp; \\text{we can factor out a -1} \\\\ =&amp; ~ \\frac{1}{\\sigma^2 2 \\pi} e^{- \\left ( \\frac{(x_1 - \\mu)^2} {2 \\sigma^2} + \\frac{(x_2 - \\mu)^2}{2 \\sigma^2} \\right )} \\\\ &amp; \\text{and we are left with} \\\\ =&amp; ~ \\frac{1}{\\sigma^2 2 \\pi} e^{-\\frac{(x_1 - \\mu)^2 + (x_2 - \\mu)^2}{2 \\sigma^2}} \\\\ \\end{aligned} \\] The log-likelihood of \\(\\mu\\) given an observation of \\(x_1\\) and \\(x_2\\), which are assumed to be instances of \\(X_1\\) and \\(X_2\\): \\[ \\begin{aligned} l(\\mu) =&amp; ~ \\text{ln} \\left (\\frac{1}{\\sigma^2 2 \\pi} e^{-\\frac{(x_1 - \\mu)^2 + (x_2 - \\mu)^2}{2 \\sigma^2}} \\right) \\\\ =&amp; ~ \\text{ln}(\\frac{1}{\\sigma^2 2 \\pi}) + \\text{ln}( e^{-\\frac{(x_1 - \\mu)^2 + (x_2 - \\mu)^2}{2 \\sigma^2}}) \\\\ =&amp; ~ \\text{ln}(\\frac{1}{\\sigma^2 2 \\pi}) -\\frac{(x_1 - \\mu)^2}{2 \\sigma^2} -\\frac{(x_2 - \\mu)^2}{2 \\sigma^2}\\\\ &amp; \\text{though I don&#39;t know how to get to Edge&#39;s answer, apart from starting with our answer in 1b:} \\\\ =&amp; ~ 2~\\text{ln}(\\frac{1}{\\sigma \\sqrt {2 \\pi}}) -\\frac{(x_1 - \\mu)^2}{2 \\sigma^2} -\\frac{(x_2 - \\mu)^2}{2 \\sigma^2}\\\\ \\end{aligned} \\] The log-likelihood of \\(\\mu\\) given observations of \\(x_1, x_2, ..., x_n\\), which are assumed to be instances of \\(n\\) independent random variables with a Normal(\\(\\mu, \\sigma^2\\)) distribution: \\[ \\begin{aligned} l(\\mu) =&amp; ~ n~\\text{ln}(\\frac{1}{\\sigma \\sqrt {2 \\pi}}) - \\sum_{i = 1}^n \\frac{(x_i - \\mu)^2}{2 \\sigma^2} \\\\ \\end{aligned} \\] 9.1 Parametric estimation using maximum likelihood The maximum-likelihood estimate of a parameter is the value of the parameter that maximizes the probability of observing the data The maximum-likelihood estimate of the parameter \\(\\theta\\) is \\(\\hat \\theta\\): \\[ \\begin{aligned} \\hat \\theta =&amp; ~ \\text{argmax} ~ L(\\theta) \\\\ &amp; \\text{or} \\\\ \\hat \\theta =&amp; ~ \\text{argmax} ~ l(\\theta) \\end{aligned} \\] ‘argmax’ means ‘argument of the maximum’ in this case, it is the value that maximizes the likelihood \\(L(\\theta)\\) or log-likelihood \\(l(\\theta)\\) usually, we use the log-likelihood to find \\(\\hat \\theta\\) To identify \\(\\hat \\theta\\): Write down the likelihood function \\(L(\\theta)\\) Take the log of likelihood function to get \\(l(\\theta)\\), and simplify Maximize \\(l(\\theta)\\) in terms of \\(\\theta\\). The value of \\(\\theta\\) that maximizes \\(l(\\theta)\\) is the maximum-likelihood estimator \\(\\hat \\theta\\) Edge goes through each of these steps to find the MLE of the parameter \\(\\lambda\\) of the exponential distribution, but I won’t repeat it here - mostly because I will probably never complete these steps in practice (if I tried to do this at all, I’d probably try to solve for the MLE numerically). The single most important assumption is that the data are actually being generated by the likelihood function we specified. If we are right, then the MLE has some desirable properties: consistency: \\(\\hat \\theta\\) converges to the true value of \\(\\theta\\) as the sample size increases asymptotically normally distributed: as the \\(n\\) approaches infinity, the distribution of the MLE approaches a normal distribution asymptotic efficiency: in the large samples, there are no consistent estimators with lower mean squared error than MLE functional invariance: this means you can estimate a value (using a defined function) based on \\(\\hat \\theta\\), and it too will be a maximum-likelihood estimator for that derived value Three (repeated) caveats, for emphasis: The MLE is NOT the value of the parameter that is most probable given the data. It is the parameter value that makes the data most probable The appeal of the MLE is efficiency, but this efficiency is defined with respect to the mean squared error, which may not be the appropriate loss function to the problem at hand The MLE is only meaningful if the model plausibly generated the data 9.1.1 Exercise set 9-2 The likelihood function for a Bernoulli distributed dataset is: \\[ \\begin{aligned} L(p) =&amp; ~ \\prod_{i = 1}^n p^{x_i} (1-p)^{1 - x_i} \\end{aligned} \\] The log-likelihood is: \\[ \\begin{aligned} l(p) =&amp; ~ \\text{ln}[L(p)] \\\\ =&amp; ~ \\text{ln} \\left [\\prod_{i = 1}^n p^{x_i} (1-p)^{1 - x_i} \\right] \\\\ =&amp; ~ \\sum_{i = 1}^n \\text{ln}[p^{x_i} (1-p)^{1 - x_i}] \\\\ =&amp; ~ \\sum_{i = 1}^n \\text{ln}[p^{x_i}] + \\text{ln}[(1-p)^{1 - x_i}] \\\\ =&amp; ~ \\sum_{i = 1}^n x_i \\text{ln}(p) + (1 - x_i)\\text{ln}(1-p) \\\\ \\end{aligned} \\] c. First, we will draw ten observations from a Bernoulli(0.6) distribution. We’ll use a binomial distribution with \\(n = k\\) and \\(p = 0.6\\), recognizing that the binomial is basically the sum of n independent Bernoulli trials with \\(p = 0.6\\). n &lt;- 10 trials &lt;- 1 p &lt;- 0.6 set.seed(121) x &lt;- rbinom(n = n, size = trials, prob = p) x ## [1] 1 0 1 0 1 1 1 0 1 1 Because we prefer to use log-likelihoods for mathematical reasons, let’s translate this statement into R code: \\[ \\begin{aligned} l(p) =&amp; ~ \\sum_{i = 1}^n x_i \\text{ln}(p) + (1 - x_i)\\text{ln}(1-p) \\\\ \\end{aligned} \\] ln_vector &lt;- numeric(length(x)) for(i in 1:length(x)){ ln_vector[i] &lt;- x[i]*log(p) + (1 - x[i]) * log(1 - p)} ln_vector ## [1] -0.5108256 -0.9162907 -0.5108256 -0.9162907 -0.5108256 -0.5108256 ## [7] -0.5108256 -0.9162907 -0.5108256 -0.5108256 exp(ln_vector) ## [1] 0.6 0.4 0.6 0.4 0.6 0.6 0.6 0.4 0.6 0.6 To recap, we’ve calculated the log-likelihood of each observation (0 or 1), given a probability of 0.6. Above, I also took the exponent of our vector, to return the likelihood of each observation. Assuming that these observations are IID, we can add up the individual log-likelihoods and get the total log-likelihood of the dataset (n = 10), given \\(p = 0.6\\). We can also take the exponent of the total log-likelihood to get the likelihood: sum(ln_vector) ## [1] -6.324652 exp(sum(ln_vector)) ## [1] 0.00179159 That is just for a single value of p. Now we’ll repeat this process for a vector of p from 0 to 1. First we create function that will calculate the log-likelihood of a dataset given one value of \\(p\\), based on the for loop above: # Function to calculate the log-likelihood for a vector of observations ln_bern &lt;- function(p, x){ ln_vector &lt;- numeric(length(x)) for(i in 1:length(x)){ ln_vector[i] &lt;- x[i]*log(p) + (1 - x[i]) * log(1 - p)} ln &lt;- sum(ln_vector) return(ln) } Now we create a vector for \\(p\\), loop through all the values, then plot: # Sequence of p p_vec &lt;- seq(0.001, 0.999, by = 0.001) ln_vec &lt;- numeric(length(p_vec)) # Log-likelihood of the data for(i in 1:length(p_vec)){ ln_vec[i] &lt;- ln_bern(p = p_vec[i], x = x) } # Get the value of p that maximizes the log-likelihood max_p &lt;- p_vec[which.max(ln_vec)] max_ln &lt;- ln_vec[which.max(ln_vec)] # Likelihood of the data Ln_vec &lt;- exp(ln_vec) max_Ln &lt;- Ln_vec[which.max(Ln_vec)] # Plot par(mfrow = c(1,3)) plot(table(x), xlab = &quot;&quot;, ylab = &quot;Frequency&quot;, main = &quot;10 observations, p = 0.6&quot;) plot(p_vec, Ln_vec, type = &quot;l&quot;, xlab = &quot;p&quot;, ylab = &quot;Likelihood&quot;) abline(v = 0.6, col = &quot;red&quot;, lty = &quot;dashed&quot;) points(x = max_p, y = max_Ln, pch = 19) plot(p_vec, ln_vec, type = &quot;l&quot;, xlab = &quot;p&quot;, ylab = &quot;Log likelihood&quot;) abline(v = 0.6, col = &quot;red&quot;, lty = &quot;dashed&quot;) points(x = max_p, y = max_ln, pch = 19) Figure 9.1: This is awesome. We just found the maximum likelihood estimate of p (the black points), going from the mathematical expression we derived, and translating it to R code. The red dashed line is the true value of p. Savor this moment. Below is Edge’s answer, but there is a disconnect between the answer in part b and translating the mathematical expression into R code. Specifically, his function takes a shortcut by using the binomial distribution instead of the Bernoulli. But it is a good check on my approach above: n &lt;- 10 trials &lt;- 1 p &lt;- 0.6 set.seed(121) x &lt;- rbinom(n = n, size = trials, prob = p) # Function to calculate likelihood for a vector of observations Ln_bern &lt;- function(p, x){ k &lt;- sum(x) n &lt;- length(x) Ln &lt;- numeric(length(p)) for(i in 1:length(p)){ Ln[i] &lt;- prod(p[i]^k * (1 - p[i])^(n - k)) } return(Ln) } # Sequence of p p_vec &lt;- seq(0.001, 0.999, by = 0.001) # Likelihood of the data Ln &lt;- Ln_bern(p = p_vec, x = x) max_p &lt;- p_vec[which.max(Ln)] max_Ln &lt;- Ln[which.max(Ln)] # Log-likelihood of the data ln &lt;- log(Ln) max_ln &lt;- ln[which.max(ln)] # Plot par(mfrow = c(1,3)) plot(table(x), xlab = &quot;&quot;, ylab = &quot;Frequency&quot;, main = &quot;10 observations, p = 0.6&quot;) plot(p_vec, Ln, type = &quot;l&quot;, xlab = &quot;p&quot;, ylab = &quot;Likelihood&quot;) abline(v = 0.6, col = &quot;red&quot;, lty = &quot;dashed&quot;) points(x = max_p, y = max_Ln, pch = 19) plot(p_vec, ln, type = &quot;l&quot;, xlab = &quot;p&quot;, ylab = &quot;Log likelihood&quot;) abline(v = 0.6, col = &quot;red&quot;, lty = &quot;dashed&quot;) points(x = max_p, y = max_ln, pch = 19) Figure 9.2: Same results as above. Ponder the differences in the code for calculating the likelihood. max_p ## [1] 0.7 summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00 0.25 1.00 0.70 1.00 1.00 The MLE is 0.7, which is the same as the proportion of observations equal to 1. Suppose that \\(X_1, X_2, ..., X_n\\) are distributed as Normal(\\(\\theta, \\sigma^2\\)). What is the MLE of \\(\\theta\\)? First, we get the expression for the log-likelihood of \\(\\theta\\): \\[ \\begin{aligned} l(\\theta) =&amp; ~ n~\\text{ln}(\\frac{1}{\\sigma \\sqrt {2 \\pi}}) - \\sum_{i = 1}^n \\frac{(x_i - \\theta)^2}{2 \\sigma^2} \\\\ =&amp; ~ n~\\text{ln}(\\frac{1}{\\sigma \\sqrt {2 \\pi}}) - \\frac{1}{2 \\sigma^2} \\sum_{i = 1}^n (x_i - \\theta)^2 \\\\ =&amp; ~ n~\\text{ln}(\\frac{1}{\\sigma \\sqrt {2 \\pi}}) - \\frac{1}{2 \\sigma^2} \\sum_{i = 1}^n (x_i^2 - 2x_i \\theta + \\theta^2) \\\\ \\end{aligned} \\] Then we take the derivative and show that the MLE is the sample mean, \\(\\bar x\\); see Edge solution for the remaining details. See Edge solution. 9.1.2 Maximum-likelihood estimation for simple linear regression \\[ \\begin{aligned} Y_i =&amp; \\alpha + \\beta x_i + \\epsilon \\end{aligned} \\] \\(\\alpha\\) and \\(\\beta\\) are fixed constants the observations \\(x_i\\) are no longer considered random, but are fixed constants (this is different from the method of moments estimator) the only random variable in the model is the disturbance term, \\(\\epsilon\\), and it is assumed to have a parametric distribution (~ Normal(\\(0, \\sigma^2\\))) the dependent variable is considered to be random (despite the fact that it is observed?) 9.1.2.1 Assumptions Linearity: \\(\\text{E}(\\epsilon | X = x) = ~ 0\\) Homoscedasticity: \\(\\text{Var}(\\epsilon | X = x) = 0\\) for all \\(x\\) Independence of units: for all i and \\(j \\neq i\\), \\(\\epsilon_i\\) is independent of \\(\\epsilon_j\\) Distribution: for all i, \\(\\epsilon_i\\) is drawn from a normal distribution Edge goes through the steps necessary to derive the maximum likelihood estimator for \\(\\alpha\\) and \\(\\beta\\), but I won’t repeat it here. It turns out that the MLE for these terms is equivalent to the expressions we derived using least-squares in chapter 3. Thus we can interpret the least-squares line as the maximum likelihood estimator of the ‘true’ line under the assumption of normally distributed, independent, homoscedastic disturbances It also turns out the the ML estimates are the same as the MOM estimates, but the assumptions used to justify each are different (MOM estimates do not invoke normality and constant variance of the disturbances), and thus provide different guarantees (e.g., ML estimates are asymptotically efficient and functionally invariant) 9.1.3 Exercise set 9-3 Solution on paper. 9.2 Parametric interval estimation: the direct approach and Fisher information 9.2.1 Exercise set 9-4 9.3 Parametric hypothesis testing using the Wald test 9.3.1 Exercise set 9-5 9.4 Parametric hypothesis testing using the likelihood-ratio test 9.4.1 Exercise set 9-6 "]
]
