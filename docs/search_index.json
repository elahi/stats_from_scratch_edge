[
["semiparametric.html", "Chapter 8 Semiparametric estimation and inference 8.1 Semiparametric point estimation using the method of moments 8.2 Semiparametric interval estimation using the bootstrap 8.3 Semiparametric hypothesis testing using permutation tests 8.4 Chapter summary", " Chapter 8 Semiparametric estimation and inference Parametric: governed by parameters; e.g., normal distributions Nonparametric: not governed by parameters; used when we cannot assume that the distribution of a random variable follows a particular probability distribution Semiparametric: a model whose behavior is partially governed by parameters Empirical distribution function: summarizes all the information that the data provide about a random variable’s distribution The empirical distribution function \\(\\hat{F}_n(z)\\) gives the proportion of observations in a sample that are less than or equal to a constant \\(z\\): \\[ \\begin{aligned} \\hat{F}_n(z) =&amp; \\frac{1}{n} \\sum_{i = 1}^n I_{X_i \\leq z} \\end{aligned} \\] where \\(I\\) is an indicator variable that states whether observation \\(X_i\\) is less than or equal to \\(z\\). 8.0.1 Exercise set 8-1 Comparing an ECDF with a CDF for the normal distribution As the sample size increases, the ECDF approximates the CDF of the normal distribution. n &lt;- 500 x.vals &lt;- seq(-3, 3, length.out = 10000) Fx &lt;- pnorm(x.vals, 0, 1) plot(x.vals, Fx, xlab = &quot;z&quot;, ylab = &quot;F(z)&quot;, type = &quot;l&quot;) x &lt;- rnorm(n, 0, 1) lines(ecdf(x), verticals = TRUE, do.points = FALSE, lty = 2) Comparing an ECDF with a CDF for the Poisson distribution set.seed(22) n &lt;- 30 lambda &lt;- 10 x.vals &lt;- seq(0, 30, by = 1) Fx &lt;- ppois(x.vals, lambda = lambda) plot(x.vals, Fx, xlab = &quot;z&quot;, ylab = &quot;F(z)&quot;, type = &quot;p&quot;) x &lt;- rpois(n, lambda = lambda) lines(ecdf(x), verticals = TRUE, do.points = FALSE, lty = 2) See handwritten notes. 8.1 Semiparametric point estimation using the method of moments 8.1.1 Introduction to moments Methods of moments: a semiparametric approach to estimation Method of maximum likelihood: a parametric approach to estimation Suppose \\(X\\) is a random variable. The \\(j\\)th moment of the distribution is: \\[ \\begin{aligned} \\mu_j =&amp; E(X^j) \\end{aligned} \\] where \\(\\mu\\) is used as the symbol for a moment. For example \\(\\mu_1 = E(X)\\). It is easy to express the first moment of \\(X\\) as an expression using \\(X\\) - but what about \\(\\mu_2\\)? \\[ \\begin{aligned} \\mu_2 =&amp; E(X^2) \\end{aligned} \\] Now we have expressed the second moment, \\(\\mu_2\\), as the expectation of a new random variable, \\(X^2\\). At least this is how I interpret it. From my reading, I gather this is not so satisfying, because the second moment is then usually rearranged in the following way. First, we need to recall the definition of Var(\\(X\\)): \\[ \\begin{aligned} \\text{Var}(X) = \\text{E}(X^2) - [\\text{E}(X)]^2 \\\\ \\end{aligned} \\] So, if we rearrange we get: \\[ \\begin{aligned} \\text{E}(X^2) = \\text{Var}(X) + [\\text{E}(X)]^2 \\\\ \\end{aligned} \\] We can continue on with 3rd, 4th, 5th, etc, moments. But expressing these in terms of \\(X\\) is, I am guessing, a harder challenge. These additional moments describe further the probability distribution, but typically knowing the first and second moments is enough for practical purposes (our purposes). As an aside, the third moment gives us the skewness, and the fourth moment gives us the kurtosis, of the distribution. Finally, it is worth noting the notation for a moment - here I have used \\(\\mu\\), becuase it is often used in statistics notes (that I found online). This can be confusing, because we often use \\(\\mu\\) for the population mean. But we won’t do that here, for clarity. So, let us say that \\(X\\) follows a Normal(\\(\\theta\\), \\(\\sigma^2\\)) distribution. In this case, we know that E(\\(X\\)) = \\(\\theta\\), and that Var(\\(X\\)) = \\(\\sigma^2\\). So we can express the first and second moments as: \\[ \\begin{aligned} \\mu_1 =&amp; ~ \\text{E}(X) \\\\ =&amp; ~ \\theta \\\\ \\mu_2 =&amp; ~ \\text{E}(X^2) \\\\ =&amp; \\text{Var}(X) + [\\text{E}(X)]^2 \\\\ =&amp; ~ \\sigma^2 + \\theta^2 \\\\ \\end{aligned} \\] So we have expressed the moments in terms of the parameters of the distribution we are trying estimate. We can rearrange these, to express the parameters in terms of \\(X\\) - which is exactly what we want to do, since we are trying to estimate \\(\\theta\\) and \\(\\sigma^2\\)! \\[ \\begin{aligned} \\theta =&amp; ~ \\text{E}(X) \\\\ \\sigma^2 =&amp; ~ \\text{E}(X^2) - [\\text{E}(X)]^2 \\\\ \\end{aligned} \\] To paraphrase Edge, we can estimate the moments of arbitrary distributions (i.e., distributions that are not described by parameters) by following these steps: Write the equations that give the moments of a random variable in terms of the parameters being estimated Solve the equations from (1) for the desired parameters, giving expressions for the parameters in terms of the moments Estimate the moments and plug the estimates into the expressions for the parameters from (2) 8.1.2 Plug-in estimators Plug-in estimation: perhaps I’m oversimplifying here, but this sounds like awesome jargon for ‘calculate your estimator from data’. In other words, if you want the population mean - collect some data and calculate the mean. Then ‘plug that in’ to your population mean. We can express a plug-in estimator of the \\(k\\)th moment of \\(X\\), E(\\(X^j\\)) as the \\(k\\)th sample moment: \\[ \\begin{aligned} \\overline{X^k} = \\frac{1}{n} \\sum_{i = 1}^n X_i^k \\end{aligned} \\] So to recap: a moment describes the random variable \\(X\\). A sample moment is an estimate of the moment using independent samples (\\(X_1, X_2, X_3\\)) drawn from \\(X\\). 8.1.3 Exercise set 8-2 Supose \\(X_1, X_2, ..., X_n\\) are I.I.D observations. What is the plug-in estimator of the variance of \\(X\\)? \\[ \\begin{aligned} \\sigma^2 =&amp; ~ \\text{E}(X^2) - [\\text{E}(X)]^2 \\\\ \\hat\\sigma^2 =&amp; ~ \\frac{1}{n} \\sum_{i = 1}^n X_i^2 - [\\frac{1}{n} \\sum_{i = 1}^n X_i]^2 \\\\ \\end{aligned} \\] b. What is the plug-in estimator for the standard deviation of \\(X\\)? \\[ \\begin{aligned} \\hat\\sigma =&amp; ~ \\sqrt{\\frac{1}{n} \\sum_{i = 1}^n X_i^2 - [\\frac{1}{n} \\sum_{i = 1}^n X_i]^2} \\\\ \\end{aligned} \\] What is the plug-in estimator of the covariance of \\(X\\) and \\(Y\\)? Here is the formula for the covariance, as a reminder: \\[ \\begin{aligned} \\text{Cov}(X,Y) =&amp; ~ \\text{E}([X - \\text{E}(X)][Y - \\text{E}(Y)]) \\\\ =&amp; ~ \\text{E}(XY) - \\text{E}(X)\\text{E}(Y) \\end{aligned} \\] Let’s use the 2nd equation: \\[ \\begin{aligned} \\text{Cov}(X,Y) =&amp; ~ \\frac{1}{n} \\sum_{i = 1}^n X_iY_i - [\\frac{1}{n} \\sum_{i = 1}^n X_i][\\frac{1}{n} \\sum_{i = 1}^n Y_i] \\\\ \\end{aligned} \\] What is the plug-in estimator of the correlation of \\(X\\) and \\(Y\\)? Here is the formula for the correlation, as a reminder: \\[ \\begin{aligned} \\text{Cor}(X,Y) =&amp; ~ \\rho_{X,Y} \\\\ =&amp; ~ \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}} \\\\ =&amp; ~ \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\\\\ \\end{aligned} \\] Plugging in, we get: \\[ \\begin{aligned} \\rho_{X,Y} =&amp; ~ \\frac {\\frac{1}{n} \\sum_{i = 1}^n X_iY_i - [\\frac{1}{n} \\sum_{i = 1}^n X_i][\\frac{1}{n} \\sum_{i = 1}^n Y_i]} {\\sqrt{ \\frac{1}{n} \\sum_{i = 1}^n X_i^2 - [\\frac{1}{n} \\sum_{i = 1}^n X_i]^2}] [\\frac{1}{n} \\sum_{i = 1}^n Y_i^2] } \\\\ \\end{aligned} \\] Though the plug-in estimator of the variance is consistent, it is biased downward. Demonstrate the downward bias in R with simulations. This is what I did first: n_obs &lt;- 5 n_sims &lt;- 10000 set.seed(1010) x &lt;- mat.samps(n = n_obs, nsim = n_sims) x_var &lt;- apply(X = x, MARGIN = 1, FUN = var) mean(x_var) ## [1] 1.004699 The result is very close to 1. This surprised me, given that I was expecting something below 1. So I ran Edge’s code, and in it, he defines a function to calculate the variance himself. var.pi &lt;- function(vec){ return(sum((vec-mean(vec))^2)/length(vec)) } vars.pi &lt;- apply(x, 1, var.pi) mean(vars.pi) ## [1] 0.8037593 Why the difference? Turns out this is a great demonstration of something I had read some time ago, and then buried away because it frankly was not super relevant. Let’s inspect the help function for var: ?var And in the details you’ll see this note: The denominator n - 1 is used which gives an unbiased estimator of the (co)variance for i.i.d. observations. These functions return NA when there is only one observation (whereas S-PLUS has been returning NaN), and fail if x has length zero. Thanks to this book, this statement make sense. Let’s calculate the variance using n and n-1 as the denominator: vec &lt;- x[1,] vec ## [1] 0.4974519 1.1826813 1.0876947 -1.3540474 -0.9988671 # Default function to calculate the variance in R var(vec) ## [1] 1.406506 # Edge&#39;s function, with the denominator of n var.pi(vec) ## [1] 1.125205 # Beating a dead horse: sum((vec-mean(vec))^2) / (length(vec)) ## [1] 1.125205 # But now we change the denominator from n, to n-1 # Unbiased estimator for the variance # Gives the same result as `var` sum((vec-mean(vec))^2) / (length(vec) - 1) ## [1] 1.406506 I have not read ahead, but I suspect Edge will discuss this discrepancy soon.. Repeating the simulation, but with samples sizes from 2 to 10. n_sims &lt;- 10000 n_vec &lt;- 2:10 variance_vec &lt;- vector(length = length(n_vec)) for(i in 1:length(n_vec)){ n_obs &lt;- n_vec[i] x &lt;- mat.samps(n = n_obs, nsim = n_sims) vars.pi &lt;- apply(x, 1, var.pi) variance_vec[i] &lt;- mean(vars.pi) } variance_vec ## [1] 0.4879436 0.6578587 0.7378372 0.7852684 0.8369602 0.8502939 0.8715858 ## [8] 0.8900192 0.9012010 plot(n_vec, variance_vec, type = &quot;b&quot;, xlab = &quot;Number of samples&quot;, ylab = &quot;Mean variance&quot;, ylim = c(0, 1)) abline(h = 1, lty = 2 , col = &quot;red&quot;) The bias is largest with a sample size of 2, and the plug-in estimator gets closer to the true value with increasing sample size. I did not think to convert these decimal outputs to fractions as indicated in Edge’s solution, even having stumbled upon the unbiased estimator above. So this result is not yet intuitive: \\[ \\begin{aligned} \\text{E}(\\hat\\sigma^2_n) =&amp; ~ \\frac{n - 1}{n} \\sigma^2 \\\\ \\end{aligned} \\] This equation demonstrates the bias of the plug-in estimator for the variance. If we multiply it by \\(n/(n-1)\\), we get the sample variance (\\(s^2\\)), an unbiased estimator: \\[ \\begin{aligned} s^2 =&amp; ~ \\frac{\\sum_{i = 1}^n (X_i - \\overline X_i)^2}{n - 1} \\\\ \\end{aligned} \\] See Edge solution. It explains how we get the expectation of the plug-in estimator. Did not try. 8.1.4 The method of moments We apply the method of moments to the model for simple linear regresion: \\[ \\begin{aligned} Y = \\alpha + \\beta X + \\epsilon \\\\ \\end{aligned} \\] If we make the assumption that the expectation of the disturbance term is 0 for all values of \\(X\\): \\[ \\begin{aligned} \\text{E}(\\epsilon | X = x) =&amp; ~ 0 \\\\ \\end{aligned} \\] then the conditional expectation of \\(Y\\), given any \\(x\\), can be predicted using a line with a slope \\(\\beta\\) and intercept \\(\\alpha\\): \\[ \\begin{aligned} \\text{E}(Y) = \\alpha + \\beta_{\\mu_X} \\\\ \\end{aligned} \\] If this assumption is not true, then the relationship between X and Y is not linear! Now that we have made the linearity assumption (see Edge’s Figure 8-4), we can proceed with the MOM estimators for \\(\\alpha\\) and \\(\\beta\\). We know that the covariance of \\(X\\) and \\(Y\\) is: \\[ \\begin{aligned} \\text{Cov}(X,Y) = \\beta \\sigma_X^2 \\\\ \\end{aligned} \\] and that: \\[ \\begin{aligned} \\mu_X =&amp; ~ \\text{E}(X) \\\\ \\sigma_X^2 =&amp; ~ \\text{Var}(X) \\\\ \\text{Cov}(X,Y) =&amp; ~ \\text{E}(XY) - \\text{E}(X)\\text{E}(Y) \\\\ \\text{Var}(X) =&amp; ~ \\text{E}(X^2) - [\\text{E}(X)]^2 \\\\ \\end{aligned} \\] we can get expressions for the parameters \\(\\alpha\\) and \\(\\beta\\) in terms of the moments of \\(X\\) and \\(Y\\): \\[ \\begin{aligned} \\alpha =&amp; ~ \\text{E}(Y) - \\beta \\text{E}(X) \\\\ \\beta =&amp; ~ \\frac{\\text{Cov}(X,Y)}{\\sigma_X^2} \\\\ =&amp; \\frac{\\text{E}(XY) - \\text{E}(X)\\text{E}(Y)}{\\text{E}(X^2) - [\\text{E}(X)]^2} \\\\ \\end{aligned} \\] The expression for \\(\\beta\\) is entirely in terms of moments, as opposed to the expression for \\(\\alpha\\). So we’ll plug in sample moments for the moments in the expression for \\(\\beta\\), multiply the numerator and denominator by \\(\\frac{n}{n}\\), and get: \\[ \\begin{aligned} \\tilde{\\beta} =&amp; \\frac{\\sum X_i Y_i - \\frac{1}{n} (\\sum X_i) (\\sum Y_i)} {\\sum X_i^2 - \\frac{1}{n} (\\sum X_i)^2 }\\\\ \\end{aligned} \\] (try working through the algebra to get this expression yourself). Here’s the expression for \\(\\alpha\\): \\[ \\begin{aligned} \\tilde{\\alpha} =&amp; ~ \\frac{1}{n} \\sum Y_i - \\tilde{\\beta} \\frac{1}{n} \\sum X_i \\\\ =&amp; ~ \\frac{\\sum Y_i - \\tilde{\\beta} \\sum X_i} {n} \\end{aligned} \\] These expressions are essentially identical to the expressions for \\(\\alpha\\) and \\(\\beta\\) that we derived for the coefficients of the least squares line in chapter 3. 8.1.5 Exercise set 8-3 See solution on paper. See solution in Edge. 8.2 Semiparametric interval estimation using the bootstrap With enough high quality data, the empirical distribution function is a reasonable approximation to the true cumulative distribution function. We can leverage this fact to estimate uncertainty intervals to accompany our plug-in estimators. Bootstrap: a procedure that resamples a single dataset to create many simulated samples, which permits the calculation of standard errors and confidence intervals. Take B ‘bootstrap’ samples, with each sample being a set of n observations drawn with replacement from the original observations. Label these samples \\(D_1^*\\), \\(D_1^*\\), …, \\(D_B^*\\). For each of the simulated amples, calculate the statistic of interest \\(\\hat \\vartheta\\) for each simulated dataset. That is, calculate \\(\\hat \\vartheta(D_1^*)\\), \\(\\hat \\vartheta(D_2^*)\\), …, \\(\\hat \\vartheta(D_B^*)\\). These values form the bootstrap distribution of \\(\\hat \\vartheta(D)\\). We will apply this procedure to our dataset of paired x, y data to calculate the bootstrap distribution of \\(\\tilde \\beta\\), the method of moments estimator of \\(\\beta\\). We need to assume that the \\(x, y\\) pairs in the sample represent independent draws from a joint distribution. 8.2.0.1 Assumptions For all i and \\(j \\neq i\\), \\(X_i, Y_i\\) are independent of \\(X_j, Y_j\\). This is the independence of units assumption For all i, \\(X_i, Y_i\\) are drawn from the same joint distribution \\(F_{X,Y}(x,y)\\). This is the distribution assumption. 8.2.0.2 Bootstrapping the Anscombe dataset # Original fertilizer use and cereal yield data d &lt;- cbind(anscombe$x1, anscombe$y1) [order(anscombe$x1), ] # One bootstrap sample set.seed(8675309) db &lt;- cbind(anscombe$x1, anscombe$y1) [sample(1:11, replace = TRUE), ] d; db ## [,1] [,2] ## [1,] 4 4.26 ## [2,] 5 5.68 ## [3,] 6 7.24 ## [4,] 7 4.82 ## [5,] 8 6.95 ## [6,] 9 8.81 ## [7,] 10 8.04 ## [8,] 11 8.33 ## [9,] 12 10.84 ## [10,] 13 7.58 ## [11,] 14 9.96 ## [,1] [,2] ## [1,] 8 6.95 ## [2,] 14 9.96 ## [3,] 12 10.84 ## [4,] 12 10.84 ## [5,] 13 7.58 ## [6,] 4 4.26 ## [7,] 5 5.68 ## [8,] 7 4.82 ## [9,] 7 4.82 ## [10,] 11 8.33 ## [11,] 7 4.82 The function boot.samp returns a bootstrap sample of either a vector or the rows of a matrix. The function beta.mm calculates the methods of moments estimate of \\(\\beta\\). boot.samp ## function (x) ## { ## if (is.null(dim(x))) { ## x &lt;- matrix(x, ncol = 1) ## } ## n &lt;- nrow(x) ## boot.inds &lt;- sample(1:n, replace = TRUE) ## x[boot.inds, ] ## } ## &lt;bytecode: 0x7fbf6c300ee8&gt; ## &lt;environment: namespace:stfspack&gt; beta.mm ## function (x, y) ## { ## n &lt;- length(x) ## (sum(x * y) - (1/n) * sum(x) * sum(y))/(sum(x^2) - (1/n) * ## sum(x)^2) ## } ## &lt;bytecode: 0x7fbf6703bc40&gt; ## &lt;environment: namespace:stfspack&gt; We use a for loop to draw the bootstrap sample: set.seed(8675309) B &lt;- 10000 boot.dist &lt;- numeric(B) dat &lt;- cbind(anscombe$x1, anscombe$y2) for(i in 1:B){ samp &lt;- boot.samp(dat) boot.dist[i] &lt;- beta.mm(samp[,1], samp[,2]) } We can plot a histogram and calculate the standard error of the estimator, \\(\\tilde \\beta\\) (recalling that the standard error of an estimate is its standard deviation). # Plot histogram hist(boot.dist, xlim = c(-0.2, 1.2), breaks = 30, xlab = expression(paste(&quot;Bootstrapped &quot;, beta)), main = &quot;&quot;, col = &quot;gray&quot;, border = &quot;white&quot;) # Estimate standard error of boostrapped beta sd(boot.dist) ## [1] 0.165262 8.2.0.3 Approximating confidence intervals for \\(\\tilde \\beta\\) We consider three options: If we assume the sampling distribution is normal, then we can use the bootstrap estimate to identify a confidence interval (\\(1 - \\alpha\\) interval). # Normal bootstrap interval a &lt;- 0.05 b_mm &lt;- beta.mm(dat[,1], dat[,2]) b_sd &lt;- sd(boot.dist) z &lt;- qnorm(p = 1 - (a / 2), mean = 0, sd = 1) b_mm - b_sd*z; b_mm + b_sd*z ## [1] 0.1760924 ## [1] 0.8239076 Use the percentiles of the bootstrap distribution to bound a confidence interval (\\(1 - \\alpha ~ percentile\\) interval). # 95% percentile interval quantile(boot.dist, c(0.025, 0.975)) ## 2.5% 97.5% ## 0.1630270 0.8459399 Calculate bootstrap pivotal intervals (see Box 8-1; skipped for now). 8.2.1 Exercise set 8-4 Sensitivity of a bootstrapped mean to n, B. Try the boot.dist.1d function: n &lt;- 5 B &lt;- 10 sim &lt;- rnorm(n = n, mean = 0, sd = 1) boot.dist.1d(x = sim, B = B, FUN = mean) ## [1] -0.22323507 -0.01956961 0.04809411 -0.21668665 0.20288450 -0.01956961 ## [7] -0.13718622 -0.26937485 -0.33757747 -0.10288316 Use the wrapper function: n &lt;- 5 B &lt;- 10 wrap.bm(n = n, B = B) ## $`boot m` ## [1] -0.7622767 ## ## $`boot se` ## [1] 0.2741837 Changing n: # Adjust wrapper function wrap.bm2 &lt;- function (n, B, mu = 0, sigma = 1, FUN = mean, ...){ sim &lt;- rnorm(n, mu, sigma) boots &lt;- boot.dist.1d(sim, B, FUN = FUN, ...) hist(boots, main = paste(&quot;n = &quot;, n, &quot;; &quot;, &quot;B = &quot;, B, sep = &quot;&quot;), xlab = &quot;Bootstrap distribution&quot;, xlim = c(-1.5, 1.5), col = &quot;gray&quot;, border = &quot;white&quot;) } B &lt;- 10000 n &lt;- 5 par(mfrow = c(2, 2)) set.seed(3) wrap.bm2(n = n, B = B) n &lt;- 10 wrap.bm2(n = n, B = B) n &lt;- 50 wrap.bm2(n = n, B = B) n &lt;- 100 wrap.bm2(n = n, B = B) Changing B: n &lt;- 100 B &lt;- 10 par(mfrow = c(2, 2)) set.seed(101) wrap.bm2(n = n, B = B) B &lt;- 100 wrap.bm2(n = n, B = B) B &lt;- 1000 wrap.bm2(n = n, B = B) B &lt;- 5000 wrap.bm2(n = n, B = B) Sensitivity of a bootstrapped midrange to n, B. New functions: midrange &lt;- function(x){ (min(x) + max(x)) / 2 } wrap.bm3 &lt;- function (n, B, mu = 0, sigma = 1, FUN = mean, ...){ sim &lt;- rnorm(n, mu, sigma) boots &lt;- boot.dist.1d(sim, B, FUN = FUN, ...) sd(boots) } Changing n: B &lt;- 10000 my_seq &lt;- seq(1:10) n_vector &lt;- 2^my_seq se_vector &lt;- numeric(length = length(my_seq)) se_pred_vector &lt;- pi / (sqrt(24 * log(n_vector))) for(i in 1:length(my_seq)){ set.seed(101) se_vector[i] &lt;- wrap.bm3(n = n_vector[i], B = B, FUN = midrange) } par(mfrow = c(1,1)) plot(n_vector, se_pred_vector, type = &quot;l&quot;, main = paste(&quot;B = &quot;, B, sep = &quot;&quot;), ylim = c(0, 0.8), xlab = &quot;Sample size (n)&quot;, ylab = &quot;SE of midrange&quot;) points(n_vector, se_vector, col = &quot;red&quot;) Changing B: n &lt;- 100 my_seq &lt;- seq(1:10) B_vector &lt;- 2^my_seq se_vector &lt;- numeric(length = length(my_seq)) se_pred_vector &lt;- rep(pi / (sqrt(24 * log(n))), length(my_seq)) for(i in 1:length(my_seq)){ set.seed(101) se_vector[i] &lt;- wrap.bm3(n = n, B = B_vector[i], FUN = midrange) } par(mfrow = c(1,1)) plot(B_vector, se_pred_vector, type = &quot;l&quot;, main = paste(&quot;n = &quot;, n, sep = &quot;&quot;), ylim = c(0, 0.5), xlab = &quot;Bootstrap samples (B)&quot;, ylab = &quot;SE of midrange&quot;) points(n_vector, se_vector, col = &quot;red&quot;) Skipped. 8.3 Semiparametric hypothesis testing using permutation tests Here we will test the hypothesis that the estimated slope from the Anscombe dataset differs from a null hypothesis, where \\(\\beta = 0\\). This results in a simplified equation, where there is no assocation between \\(X\\) and \\(Y\\): \\[ \\begin{aligned} Y = \\alpha + \\epsilon \\end{aligned} \\] We will use a permutation test to test the null hypothesis. Let’s go over the assumptions of our test. 8.3.0.1 Assumptions Linearity: \\(\\text{E}(\\epsilon | X = x) =&amp; ~ 0\\) Independence of units: for all i and \\(j \\neq i\\), \\(X_i, Y_i\\) are independent of \\(X_j, Y_j\\) Distribution: for all i, \\(X_i, Y_i\\) are drawn from the same joint distribution \\(F_{X,Y}(x,y)\\) Independence of X and disturbances: for all \\(i\\), \\(X_i\\) and \\(\\epsilon_i\\) are independent Notice that we made no assumptions aboute the distribution of the disturbance term, \\(\\epsilon\\). The procedure for a permutation test is: Choose a test statistic and calculate it using the original data (\\(s_d\\)). Permute (i.e., shuffle) the data randomly and in such a way that if the null hypothesis were true, the hypothetical dataset is just as probable as the original data. Calculate the test statistic using the permuted data and save the value. Repeat steps (2) and (3) many times. The resulting test statistics are called a ‘permutation distribution’. Compare the original statistic \\(s_d\\) with the permutation distribution. Note that the permuted sample is somewhat like a sample that might be observed if the \\(X\\) and \\(Y\\) values were independent. Here’s the original dataset, and a permuted one: # Original fertilizer use and cereal yield data d &lt;- cbind(anscombe$x1, anscombe$y1) [order(anscombe$x1), ] # One permuted sample set.seed(8675309) db &lt;- cbind(anscombe$x1, anscombe$y1[sample(1:11, replace = FALSE)])[order(anscombe$x1), ] d; db ## [,1] [,2] ## [1,] 4 4.26 ## [2,] 5 5.68 ## [3,] 6 7.24 ## [4,] 7 4.82 ## [5,] 8 6.95 ## [6,] 9 8.81 ## [7,] 10 8.04 ## [8,] 11 8.33 ## [9,] 12 10.84 ## [10,] 13 7.58 ## [11,] 14 9.96 ## [,1] [,2] ## [1,] 4 8.81 ## [2,] 5 4.26 ## [3,] 6 9.96 ## [4,] 7 8.04 ## [5,] 8 8.33 ## [6,] 9 10.84 ## [7,] 10 6.95 ## [8,] 11 5.68 ## [9,] 12 7.58 ## [10,] 13 7.24 ## [11,] 14 4.82 If we do this a number of times, we can plot the histogram of permuted samples, along with a line to represent the original \\(\\tilde \\beta\\): set.seed(8675309) nperms &lt;- 10000 perm_dist &lt;- numeric(nperms) dat &lt;- cbind(anscombe$x1, anscombe$y1) for(i in 1:nperms){ samp &lt;- perm.samp(dat) perm_dist[i] &lt;- beta.mm(samp[,1], samp[,2]) } b_orig &lt;- beta.mm(anscombe$x1, anscombe$y1) b_orig ## [1] 0.5000909 # Plot histogram hist(perm_dist, xlim = c(-0.6, 0.6), breaks = 30, xlab = expression(paste(beta, &quot; from permuted samples&quot;)), main = &quot;&quot;, col = &quot;gray&quot;, border = &quot;white&quot;) abline(v = b_orig, col = &quot;red&quot;, lty = 2) We can compute a two-sided \\(p\\) value testing the null hypothesis that \\(\\beta = 0\\) by calculating the proportion of the permuted samples for which the absolute value of \\(\\tilde \\beta_p\\) is greater than the absolute value of \\(\\tilde \\beta\\) from the original data: mean(abs(perm_dist) &gt;= b_orig) ## [1] 0.0033 In summary we tested the hypothesis that: \\(\\beta = 0\\) AND the \\(X\\) and \\(Y\\) data are linear the permuted units are independent all observations are drawn from a common distribution the \\(X_i\\) are independent of the disturbance \\(\\epsilon_i\\) Therefore, in principle, the low \\(p\\) value indicates that one of the above are unlikely to be true. Usually, we focus on the null hypothesis, but it is important to remember the assumptions. 8.3.1 Exercise set 8-5 20 wheat fields, 10 received treatment application (substance Z). Design a permutation procedure to assess the claim that substance Z changes the wheat yield. The null hypothesis is that there is no association between yield and the treatment. Alternatively, the treatment may have altered the yield. To test this, we can randomly permute the control / treatment labels for the 20 fields. Then we can calculate the mean yield for each treatment, and compare the permutation distribution to the mean yield for each treatment for the original dataset. Edge has a thorough explanation, and in it he highlights that a low p-value may also result from a change in the variance due to substance Z. Thus, we are really testing the null hypothesis that substance Z does not affect the distribution of wheat yield. This might be worth simulating. # 10 pairs beta_vec &lt;- c(0, 0.1, 0.2) n_pairs &lt;- 10 n_datasets &lt;- 20 p_mat &lt;- matrix(nrow = 500, ncol = length(beta_vec)) for(i in 1:length(beta_vec)){ p_mat[,i] &lt;- sim.perm.B(n = n_pairs, nsim = n_datasets, a = 3, b = beta_vec[i]) } colnames(p_mat) &lt;- c(&quot;b_0&quot;, &quot;b_0.1&quot;, &quot;b_0.2&quot;) df10 &lt;- as_tibble(p_mat) # 50 pairs beta_vec &lt;- c(0, 0.1, 0.2) n_pairs &lt;- 50 n_datasets &lt;- 20 p_mat &lt;- matrix(nrow = 500, ncol = length(beta_vec)) for(i in 1:length(beta_vec)){ p_mat[,i] &lt;- sim.perm.B(n = n_pairs, nsim = n_datasets, a = 3, b = beta_vec[i]) } colnames(p_mat) &lt;- c(&quot;b_0&quot;, &quot;b_0.1&quot;, &quot;b_0.2&quot;) df50 &lt;- as_tibble(p_mat) # 100 pairs beta_vec &lt;- c(0, 0.1, 0.2) n_pairs &lt;- 100 n_datasets &lt;- 20 p_mat &lt;- matrix(nrow = 500, ncol = length(beta_vec)) for(i in 1:length(beta_vec)){ p_mat[,i] &lt;- sim.perm.B(n = n_pairs, nsim = n_datasets, a = 3, b = beta_vec[i]) } colnames(p_mat) &lt;- c(&quot;b_0&quot;, &quot;b_0.1&quot;, &quot;b_0.2&quot;) df100 &lt;- as_tibble(p_mat) # Get proportion of significant tests colMeans(df10 &lt; 0.05) ## b_0 b_0.1 b_0.2 ## 0.05 0.10 0.10 colMeans(df50 &lt; 0.05) ## b_0 b_0.1 b_0.2 ## 0.00 0.30 0.55 colMeans(df100 &lt; 0.05) ## b_0 b_0.1 b_0.2 ## 0.1 0.6 1.0 # Plot library(patchwork) p10 &lt;- df10 %&gt;% gather(key = beta, value = p) %&gt;% ggplot(aes(x = beta, p)) + geom_boxplot() + scale_y_continuous(limits = c(0, 1)) + ggtitle(&quot;n = 10 pairs&quot;) + geom_hline(yintercept = 0.05, color = &quot;red&quot;) p50 &lt;- df50 %&gt;% gather(key = beta, value = p) %&gt;% ggplot(aes(x = beta, p)) + geom_boxplot() + scale_y_continuous(limits = c(0, 1)) + ggtitle(&quot;n = 50 pairs&quot;) + geom_hline(yintercept = 0.05, color = &quot;red&quot;) p100 &lt;- df100 %&gt;% gather(key = beta, value = p) %&gt;% ggplot(aes(x = beta, p)) + geom_boxplot() + scale_y_continuous(limits = c(0, 1)) + ggtitle(&quot;n = 100 pairs&quot;) + geom_hline(yintercept = 0.05, color = &quot;red&quot;) p10 + p50 + p100 8.4 Chapter summary "]
]
