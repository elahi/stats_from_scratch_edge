[
["index.html", "Notes on Statistical Thinking from Scratch Preface", " Notes on Statistical Thinking from Scratch Robin Elahi 2020-05-23 Preface This project is a set of notes for Statistical Thinking from Scratch, by M.D. Edge. The goal is to become more comfortable with the nitty gritty underlying the statistical tools I commonly use - essentially, regression. Edge takes a unique approach in that he takes a small dataset, and dissects regression ‘from scratch’. Each of these bookdown chapters corresponds to each of Edge’s 10 book chapters. The impetus for this bookdown project was the cancellation of my spring 2020 course Experimental Design and Probability due to COVID-19. So, I’ll be filling in some gaps in my stats toolkit and learning how to use bookdown. The stfs package can be installed from Github: install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;mdedge/stfspack&quot;) library(stfspack) As Edge points out in his Prelude, the typical introductory course in biological statistics (including my own) revolves around learning a series of tests: t-tests, regression, ANOVA . In STFS, the focus instead is on one procedure - simple linear regression - and takes ‘little for granted’ (translation: we’ll be working through the math, lightly). The primary dataset consists of 11 observations. To heck with big data. So in this project, you will find mostly code - I’ll leave the exposition to Edge, who does a nice job explaining the concepts in his textbook. I’ll chime in whenever it seems useful. The chapters will work through the following: 1. Probability 2. Estimation (using data to guess the values that describe a data generating process) 3. Inference (testing hypotheses about the processes that might have generated an observed dataset) How to publish a bookdown project on github https://community.rstudio.com/t/hosting-bookdown-in-github/20427 http://seankross.com/2016/11/17/How-to-Start-a-Bookdown-Book.html "],
["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction library(stfspack) library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) Figure 1.1 shows data on the amount of fertilizer applied to cropland (x-axis), and the cereal yields (y-axis), for each of 11 countries in Africa. There is a positive relationship, but is it strong? Is it weak? How are we to reason about these data? anscombe %&gt;% ggplot(aes(x1, y1)) + geom_point() + labs(x = &quot;Fertilizer consumption (kg/hectare)&quot;, y = &quot;Cereal yield (100kg/hectare)&quot;) Figure 1.1: Fertilizer consumption and cereal yield in 11 sub-Saharan African countries Statistics, that’s how! Statistics allows us to reason from data, and rests on a mathematical framework. It is worth understanding, even minimally, this framework. That’s why we are reading this book. Chapter 1 provides an overview of simple linear regression, which allows us to identify a line that ‘best’ fits the data. We can use the lm() function in R to fit a simple linear regression: mod.fit &lt;- lm(y1 ~ x1, data = anscombe) anscombe %&gt;% ggplot(aes(x1, y1)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(x = &quot;Fertilizer consumption (kg/hectare)&quot;, y = &quot;Cereal yield (100kg/hectare)&quot;) Figure 1.2: The agriculture data with a line of best fit from the simple linear regression summary(mod.fit) ## ## Call: ## lm(formula = y1 ~ x1, data = anscombe) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.92127 -0.45577 -0.04136 0.70941 1.83882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0001 1.1247 2.667 0.02573 * ## x1 0.5001 0.1179 4.241 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 9 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.6295 ## F-statistic: 17.99 on 1 and 9 DF, p-value: 0.00217 What do the following columns in the ‘Coefficients’ table refer to? the Estimates: (Intercept) and x1 Std. Error Pr(&gt;|t|) How do each of these relate to the ideas of estimation and inference? Next, a cautionary tale. Here we fit another regression model to a different set of (fake) data, that gives the exact same regression results. mod.fit2 &lt;- lm(y3 ~ x3, data = anscombe) anscombe %&gt;% ggplot(aes(x3, y3)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(x = &quot;Fertilizer consumption (kg/hectare)&quot;, y = &quot;Cereal yield (100kg/hectare)&quot;) Figure 1.3: The data underlying the analysis of the variables y3 and x3 in the anscombe data set You should treat the results of the table with suspicion, given the figure above. summary(mod.fit2) ## ## Call: ## lm(formula = y3 ~ x3, data = anscombe) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.1586 -0.6146 -0.2303 0.1540 3.2411 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0025 1.1245 2.670 0.02562 * ## x3 0.4997 0.1179 4.239 0.00218 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.236 on 9 degrees of freedom ## Multiple R-squared: 0.6663, Adjusted R-squared: 0.6292 ## F-statistic: 17.97 on 1 and 9 DF, p-value: 0.002176 The rest of the book will give us the foundation to interpret all of the values in the regression table, and the underlying assumptions of the linear model. "],
["r-eda.html", "Chapter 2 R and exploratory data analysis 2.1 Inspecting the dataframe 2.2 Histograms 2.3 Summarising data 2.4 Loops 2.5 Functions 2.6 Boxplots 2.7 Scatterplots 2.8 Exercise set 2-2", " Chapter 2 R and exploratory data analysis library(stfspack) library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) This chapter is about getting comfortable with R. Edge uses base R for data wrangling and plotting, but here I’ll also recreate the chapter exercises using the tidyverse. For a deeper dive into R and the tidyverse: Data carpentry’s ecology lesson R for data science 2.1 Inspecting the dataframe head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa glimpse(iris) ## Observations: 150 ## Variables: 5 ## $ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4… ## $ Sepal.Width &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3… ## $ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1… ## $ Petal.Width &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0… ## $ Species &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, … summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## 2.2 Histograms # Base hist(iris$Sepal.Length, xlab = &quot;Sepal Length&quot;, main = &quot;&quot;) # Tidyverse iris %&gt;% ggplot(aes(Sepal.Length)) + geom_histogram(bins = 8, col = &quot;white&quot;) + labs(x = &quot;Sepal Length&quot;) 2.3 Summarising data # Base tapply(iris$Sepal.Length, iris$Species, mean) ## setosa versicolor virginica ## 5.006 5.936 6.588 # Tidyverse iris %&gt;% group_by(Species) %&gt;% summarise(mean = mean(Sepal.Length)) ## # A tibble: 3 x 2 ## Species mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 5.01 ## 2 versicolor 5.94 ## 3 virginica 6.59 Note that the tidyverse output is a tibble (essentially a dataframe), which is a consistent feature of the tidy approach to data wrangling. Not that you can’t do this in base R with the aggregate function - which is how I used to do it BT (before tidyverse): # Base option 2 aggregate(iris$Sepal.Length, list(iris$Species), mean) ## Group.1 x ## 1 setosa 5.006 ## 2 versicolor 5.936 ## 3 virginica 6.588 # You can check the output using str() aggregate(iris$Sepal.Length, list(iris$Species), mean) %&gt;% str() ## &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ Group.1: Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 2 3 ## $ x : num 5.01 5.94 6.59 2.4 Loops for(i in 1:3){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 i &lt;- 1 print(i) ## [1] 1 unique(iris$Species) ## [1] setosa versicolor virginica ## Levels: setosa versicolor virginica for(i in unique(iris$Species)){ print(mean(iris$Sepal.Length[iris$Species == i])) } ## [1] 5.006 ## [1] 5.936 ## [1] 6.588 2.5 Functions conditional.mean &lt;- function(x, y){ for(i in unique(y)){ print(mean(x[y == i])) } } conditional.mean(x = iris$Sepal.Length, y = iris$Species) ## [1] 5.006 ## [1] 5.936 ## [1] 6.588 conditional.mean(x = iris$Sepal.Width, y = iris$Species) ## [1] 3.428 ## [1] 2.77 ## [1] 2.974 2.6 Boxplots # Base boxplot(iris$Sepal.Length ~ iris$Species, xlab = &quot;Species&quot;, ylab = &quot;Sepal length&quot;) # Tidyverse iris %&gt;% ggplot(aes(Species, Sepal.Length)) + geom_boxplot() + labs(x = &quot;Species&quot;, y = &quot;Sepal length&quot;) 2.7 Scatterplots # Base plot(iris$Sepal.Length, iris$Sepal.Width, xlab = &quot;Sepal length&quot;, ylab = &quot;Sepal width&quot;) # Tidyverse iris %&gt;% ggplot(aes(Sepal.Length, Sepal.Width)) + geom_point() + labs(x = &quot;Sepal length&quot;, y = &quot;Sepal width&quot;) With unique symbols for species: # Base plot(iris$Sepal.Length, iris$Sepal.Width, xlab = &quot;Sepal length&quot;, ylab = &quot;Sepal width&quot;, pch = as.numeric(iris$Species)) legend(&quot;topright&quot;, pch = c(1,2,3), legend = c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;)) # Tidyverse iris %&gt;% ggplot(aes(Sepal.Length, Sepal.Width, shape = Species)) + geom_point() + labs(x = &quot;Sepal length&quot;, y = &quot;Sepal width&quot;) 2.8 Exercise set 2-2 Repeat the above analyses (histogram, summarising data, boxplots) for Petal.Width. Install and load a new package gpairs. Run the following line of code. What do you see? gpairs(iris, scatter.pars = list(col = as.numeric(iris$Species))) Install and load the package stfspack if you have not already done so. install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;mdedge/stfspack&quot;) library(stfspack) "],
["best-fit-line.html", "Chapter 3 Line of best fit 3.1 Exercise set 3-1 3.2 Exercise set 3-2", " Chapter 3 Line of best fit library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) 3.1 Exercise set 3-1 Calculate least-squares intercept a and least-squares slope b for the anscombe data. I will use equations 3.8 and 3.9. x &lt;- anscombe$x1 y &lt;- anscombe$y1 xbar &lt;- mean(x) ybar &lt;- mean(y) b &lt;- sum((x - xbar)*(y - ybar)) / sum((x - xbar)^2) b ## [1] 0.5000909 a &lt;- ybar - b*xbar a ## [1] 3.000091 # Check using lm m1 &lt;- lm(y ~ x) summary(m1) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.92127 -0.45577 -0.04136 0.70941 1.83882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0001 1.1247 2.667 0.02573 * ## x 0.5001 0.1179 4.241 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 9 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.6295 ## F-statistic: 17.99 on 1 and 9 DF, p-value: 0.00217 3.2 Exercise set 3-2 On paper On paper On paper Comparing L1 and L2 lines. library(quantreg) x &lt;- anscombe$x1 y &lt;- anscombe$y1 mL1 &lt;- rq(y ~ x, tau = 0.5) mL2 &lt;- lm(y ~ x) plot(x, y) abline(mL2, lty = 1) abline(mL1, lty = 2) legend(&quot;topleft&quot;, legend = c(&quot;L2&quot;, &quot;L1&quot;), lty = c(1, 2)) Dataset with outlier. mL1 &lt;- rq(y3 ~ x3, tau = 0.5, data = anscombe) mL2 &lt;- lm(y3 ~ x3, data = anscombe) plot(y3 ~ x3, data = anscombe) abline(mL2, lty = 1) abline(mL1, lty = 2) legend(&quot;topleft&quot;, legend = c(&quot;L2&quot;, &quot;L1&quot;), lty = c(1, 2)) "],
["probability.html", "Chapter 4 Probability and random variables 4.1 Kolmogorov’s three axioms of probability 4.2 Conditional probability and independence 4.3 Bayes’ Theorem 4.4 Discrete random variables and distributions 4.5 Continuous random variables and distributions 4.6 Probability density functions 4.7 Families of distributions", " Chapter 4 Probability and random variables library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) 4.0.1 Probability vs estimation Here are two quotes from STFS that encapsulate the distinction between probability and estimation: In probability theory, we think about processes that generate data, and we ask \"what can we say about the data generated by such a process? In statistical estimation and inference, we work in the opposite direction. We start with data, and we ask \"what can we say about the process that generated these data? So, we can define probability as the study of data generated by specified processes - and that’s the focus of this chapter. Later on, when we get to data - we’ll be assuming probabilistic models - but we have to accept the fact that the data we observed were not likely to be generated by the assumed model! But we do the best we can. 4.0.2 What is a probability? Frequency view the probability of a given event is simply the proportion of times it occurs over many trials difficult to apply to statements like “what is the probability that it will rain tomorrow” and “what is the probability that a meteor will hit the earth tomorrow” (i.e., one-shot events) Degrees-of-belief view what is the degree of belief in the occurrence of an event that a rational person would have? An aside: is the universe fundamentally probabilistic or deterministic? 4.0.3 Set notation A set (\\(\\Omega\\), in STFS) is an unordered collection of objects (elements). Intersection (\\(\\cap\\)) of two sets is the set of elements that appear in both sets. Union (\\(\\cup\\)) of two sets is the set that has every element that appears in either of the two original sets. The complement of a set (\\(S^C\\)) includes all of the elements not in the original set but present in the sample space (\\(\\Omega\\)) (which contains all possible elements). Subsets are referred to as S, and have the following properties: \\[ \\begin{aligned} S \\cup S^C = \\Omega \\\\ S \\cap S^C = \\emptyset \\end{aligned} \\] where \\(\\emptyset\\) is used to denote the empty set, or the set with no elements. A Venn diagram is useful here: The notation for the probability of an event is typically denoted using either \\(P()\\) or \\([]\\): \\[ \\begin{aligned} P(event) = [event] \\\\ \\end{aligned} \\] 4.1 Kolmogorov’s three axioms of probability Probabilities of any event i (\\(E_i\\)) cannot be negative: \\[ \\begin{aligned} \\ [E_i] \\geq 0 \\\\ \\end{aligned} \\] The probability of the event that includes every outcome is 1: \\[ \\begin{aligned} \\ [\\Omega] = 1 \\\\ \\end{aligned} \\] The probability of observing either of two mutually exclusive events is the sum of their individual probabilities: \\[ \\begin{aligned} \\text{If } E_1 \\cap E_2 = \\emptyset, \\\\ \\text{then } [E_1 \\cup E_2] = [E_1] + [E_2] \\end{aligned} \\] 4.1.1 Exercise set 4-1 Given P(\\(A\\)), we would like to know P(\\(A^C\\)). We already learned the following properties of complements: \\[ \\begin{aligned} A \\cup A^C = \\Omega \\\\ A \\cap A^C = \\emptyset \\end{aligned} \\] The intersection between \\(A\\) and \\(A^C\\) is zero, meaning they don’t share any elements. The union of \\(A\\) and \\(A^C\\) is \\([\\Omega]\\). We also learned the 2nd axiom of probability, \\([\\Omega] = 1\\). Therefore: \\[ \\begin{aligned} \\ [A] + [A^C] = 1 \\\\ \\ [A^C] = 1 - [A] \\end{aligned} \\] On paper. Below I’ve pasted an image of my handwritten version of Edge’s solution - I found it easier to think about once I visualized it using a Venn diagram. 4.2 Conditional probability and independence The law of conditional probability states that: \\[ \\begin{aligned} \\ [A \\mid B] = \\frac{[A \\cap B]}{[B]} \\\\ \\end{aligned} \\] If A and B are independent, then: \\[ \\begin{aligned} \\ [A \\mid B] = [A] \\\\ \\end{aligned} \\] 4.2.1 Exercise set 4-2 If we rearrange the law of probability, we get: \\[ \\begin{aligned} \\ [A \\mid B] [B] = [A \\cap B] \\\\ \\end{aligned} \\] By definition, if A and B are independent, then we can replace \\([A \\mid B]\\) with \\([A]\\) and get: \\[ \\begin{aligned} \\ [A \\cap B] = [A] [B] \\\\ \\end{aligned} \\] If we divide both sides by \\([A]\\), we get: \\[ \\begin{aligned} \\ [B] = \\frac{[A \\cap B]}{[A]} \\\\ \\end{aligned} \\] The right-hand side of the above equation is, by the law of conditional probability, equal to \\([B \\mid A]\\) and thus: \\[ \\begin{aligned} \\ [B] = [B \\mid A]\\\\ \\end{aligned} \\] Suppose you know \\([A \\mid B]\\), \\([A]\\), and \\([B]\\). Calculate \\([B \\mid A]\\). \\[ \\begin{aligned} \\ [A \\mid B] = \\frac{[A \\cap B]}{[B]} \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\ [A \\mid B] [B] = [A \\cap B] \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\ [B \\mid A] = \\frac{[A \\cap B]}{[A]} \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\ [B \\mid A] = \\frac{[A \\mid B] [B]}{[A]} \\\\ \\end{aligned} \\] 4.3 Bayes’ Theorem 4.4 Discrete random variables and distributions Random variable: e.g., the process of rolling a die, \\(X\\) \\(X\\) is random, can take integer values 1-6 \\(X\\) is not a number yet - it is the unrealized outcome of a random process The realization of that process is an instance - which is a number Capital \\(X\\): random variable Lower case \\(x\\): instance All the probability information is contained in its distribution. In our case, \\(X\\) is a discrete random variable, meaning that the number of outcomes is countable, in principle. Two ways to represent the distribution: Probability mass function (pmf), \\(f_X(x) = P(X = x)\\) Cumulative distribution function (cdf), \\(F_X(x) = P(X \\leq x)\\) The cdf is a series of partial sums of the pmf, \\[ \\begin{aligned} F_X(x) = P(X \\leq x) = \\sum_{x_i \\leq x} f_X(x_i) \\end{aligned} \\] and increases monotonically in \\(x\\). 4.4.1 Exercise set 4-3 Flipping a fair coin 3 times. X is a random variable that represents the number of heads observed, and the sample space \\(\\Omega\\) contains the elements {0, 1, 2, 3}. Here are all of the ways we can observe these elements: x = 0: (T, T, T) x = 1: (H, T, T); (T, H, T); (T, T, H) x = 2: (H, H, T); (H, T, H); (T, H, H) x = 3: (H, H, H) There are 8 possible instances. Thus, the probability mass function is: \\[ \\begin{aligned} f_X(0) =&amp; f_X(3) = 1/8 \\\\ f_X(1) =&amp; f_X(2) = 3/8 \\\\ f_X(x) =&amp; 0 \\text{ for all other } x \\\\ \\end{aligned} \\] If we sum \\(f_X(x_i)\\) for all possible \\(x_i\\), the sum would be 1 (1/8 + 1/8 + 3/8 + 3/8). In terms of \\(F_X\\), what is \\(P(a \\lt X \\le b)\\), if \\(b \\gt a\\)? Hint: notice that \\(a \\lt X \\le b\\) if and only if \\(X \\leq b\\) and \\(X &gt; a\\). Note that the cdf, \\(F_X\\), for \\(x = a\\) and \\(x = b\\) are given by the following: \\[ \\begin{aligned} F_X(a) =&amp; P(X \\le a) \\\\ F_X(b) =&amp; P(X \\le b) \\\\ \\end{aligned} \\] We want the probability of observing a value \\(x\\) between \\(a\\) and \\(b\\), so: \\[ \\begin{aligned} P(a \\lt X \\le b) = F_X(b) - F_X(a) \\\\ \\end{aligned} \\] 4.5 Continuous random variables and distributions What about values that are not countable - anything with a decimal? Impossible to get a specific number, or instance (e.g., the probability a person weighs exactly 70kg), and thus we cannot use the pmf. But we can still use the cdf: \\[ \\begin{aligned} F_X(x) = P(X \\leq x) \\end{aligned} \\] That is, we can ask about the probability that a person will weigh 70kg or less, or whether a person will weigh between 70 and 71kg. 4.5.1 Exercise set 4-4 \\(X\\) is a random variable that takes values in the interval [0, 1], and the probability distribution is uniform. Draw the cumulative distribution function \\(F_X(x)\\) for \\(x \\in [-1, 2]\\). x &lt;- c(0,1) Fx &lt;- x plot(x, Fx, type = &quot;l&quot;, xlim = c(-1,2)) lines(c(-1, 0), c(0, 0)) lines(c(1,2), c(1, 1)) If \\(X\\) were more likely to land in [0.4, 0.6] than in any other region of length 0.2, the line between 0.4 and 0.6 would be steeper. x1 &lt;- c(0,0.4) x2 &lt;- c(0.4, 0.6) x3 &lt;- c(0.6, 1) Fx1 &lt;- c(0,0.3) Fx2 &lt;- c(0.3, 0.7) Fx3 &lt;- c(0.7, 1) plot(x1, Fx1, type = &quot;l&quot;, xlim = c(-1,2), ylim = c(0,1), xlab = &quot;x&quot;, ylab = &quot;Fx&quot;) lines(x2, Fx2) lines(x3, Fx3) lines(c(-1, 0), c(0, 0)) lines(c(1,2), c(1, 1)) 4.6 Probability density functions For a continuous random variable, the pdf is the derivative of the cdf Recall that the cdf for a discrete random variable is a series of partial sums, given by: \\[ \\begin{aligned} F_X(x) = P(X \\leq x) = \\sum_{x_i \\leq x} f_X(x_i) \\end{aligned} \\] In an analogous fashion, we can integrate a continuous function to get the cdf of a continous random variable. We define the probability density function \\(f_X\\) (pdf) of a continuous random variable as: \\[ \\begin{aligned} F_X(x) =&amp; \\int_{- \\infty}^{x} f_X(u) du \\\\ \\end{aligned} \\] Below I have re-created Fig 4-4, with a pdf in the upper panel and a cdf in the lower panel - for an exponential random variable with rate 1. 4.6.1 Additional viz We can visualize the principle of question 3 from Exercise Set 4-3 using the cdf of a standard normal distribution, where \\(a = -1\\) and \\(b = 1\\): 4.6.2 Exercise set 4-5 If \\(f_X(x)\\) is a probability density funcion, then total area under \\(f_X(x)\\) is 1. Yes, \\(f_x\\) can be a probability density funcion, because the area under the function is 1. This situation is different from a probability mass function, because the y-axis values for a pdf can be &gt; 1 (in this case, the maximum y is 10). 4.7 Families of distributions Two requirements for mass or density functions: Must be non-negative Sum of all values is 1 (mass), or area under curve is 1 (density) Distribution family similarly shaped distributions summaries of their behavior can be computed from the same functions but parameter values differ 4.7.1 Exercise set 4-6 The probability mass function of the Poisson distribution is \\(P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\). Plugging in the appropriate values for \\(k\\) and \\(\\lambda\\) gives: (i). \\(e^{-5}\\) (ii). \\(5e^{-5}\\) (iii). \\(\\frac{25}{2}e^{-5}\\) Use the probability mass function of the geometric distribution with parameter 1/2. If our first “heads” occurs on the 6th flip, then we have five tails before it. We plug \\(p\\) = 1/2 and \\(k\\) = 5 into \\(P(X = k) = (1-p)^kp\\) to get 1/64. Consider a Poisson distribution with parameter \\(\\lambda\\) = 5. If we want to know the value of the probability mass function for x = 2, \\(f_X(2)\\), we use the dpois() function: dpois(2, lambda = 5) ## [1] 0.08422434 To get the value of the cumulative distribution function \\(F_X(2)\\), we use ppois(): ppois(2, lambda = 5) ## [1] 0.124652 If we want to know the inverse of the cumulative distribution function. That is, we want to know the value of \\(q\\) that solves the equation \\(F_X(q) = p\\). What is the number \\(q\\) such that the probability tha the random variable is less than or equal to \\(q\\) is \\(p\\); \\(q\\) is the \\(pth\\) percentile of the distribution. We can get this using qpois(): qpois(0.124652, lambda = 5) ## [1] 2 The inverse of the cdf is also called the quantile function. Using the standard normal distribution (mean = 0, sd = 1), plot the probability density function for x \\(\\in\\) [-3, 3]. x &lt;- seq(-3, 3, by = 0.1) fx &lt;- dnorm(x = x) plot(x, fx, type = &quot;l&quot;, main = &quot;PDF&quot;, col = &quot;red&quot;) Plot the cdf for x \\(\\in\\) [-3, 3]. x &lt;- seq(-3, 3, by = 0.1) fx &lt;- pnorm(q = x) plot(x, fx, type = &quot;l&quot;, main = &quot;CDF&quot;, col = &quot;red&quot;) What value of \\(x\\) is at the 97.5th percentile of the standard normal? qnorm(p = 0.975) ## [1] 1.959964 Simulating from a normal distribution and from a uniform distribution: n &lt;- 1000 x &lt;- rnorm(n) hist(x) x &lt;- runif(n, min = 0, max = 1) hist(x) Now take those values between 0 and 1, and feed them into the qnorm function to get the values at which we see those quantiles: y &lt;- qnorm(p = x, mean = 0, sd = 1) hist(y) These values are normally distributed around 0. This plot is saying that most of the probability (in fact ~95%) lies between -2 &lt; y &lt; 2 (2 standard deviations). r &lt;- seq(-3, 3, length.out = 1000) cdf &lt;- pnorm(r) #Draw the normal cumulative distribution function. plot(r, cdf, type = &quot;l&quot;, xaxs = &quot;i&quot;, yaxs = &quot;i&quot;, xlim = c(-3, 3), xlab = expression(italic(x)), ylab = expression(paste(italic(F[X]), &quot;(&quot;, italic(x), &quot;)&quot;, sep = &quot;&quot;)), lwd = 2) #Draw light grey lines representing random samples from the #standard normal distribution. x &lt;- rnorm(100) for(i in x){ lines(c(i,i), c(min(x), pnorm(i)), col = rgb(190, 190, 190, alpha = 60, max = 255)) lines(c(min(x)-1,i), c(pnorm(i), pnorm(i)), col = rgb(190, 190, 190, alpha = 60, max = 255)) } 4.7.2 Additional exercise (Courtesy Blondin &amp; Goodman) Assume that destructive earthquakes occur in California every 20 years. You’d like to know how probable an earthquake is between now and some future date. What distribution family describes the waiting time until California’s next earthquake? What value should we use for this distribution’s parameter? [Hint: convert 20 years to a rate] Use the exponential distribution, where \\(\\lambda = 1/20\\): \\[ \\begin{aligned} f_X(x) = \\lambda e ^{-\\lambda x} \\\\ \\end{aligned} \\] What is the probability that an earthquake will happen in the next 10 years? [Hint: You need to find a cumulative distribution function for the distribution you chose, either online or by integration]. Rules for integrating “e raised to the x power”: \\[ \\begin{aligned} \\int e^xdx =&amp; e^x + c \\\\ \\int e^{ax}dx =&amp; \\frac{1}{a} e^{ax} + c \\\\ \\int be^{ax}dx =&amp; \\frac{b}{a} e^{ax} + c \\\\ \\end{aligned} \\] With these rules in hand, we can get the cdf by integrating the pdf for the exponential distribution: \\[ \\begin{aligned} f_X(x) =&amp; \\lambda e ^{-\\lambda x} \\\\ F_X(X = x) =&amp; \\int_{0}^{x} \\lambda e^{-\\lambda x} dx \\\\ =&amp; \\frac{1}{- \\lambda} \\lambda e^{-\\lambda x} \\bigg\\rvert_{0}^{x} \\\\ =&amp; -e^{-\\lambda x} \\rvert_{0}^{x} \\\\ =&amp; -e^{-\\lambda x} \\rvert_{0}^{x} \\\\ =&amp; -e^{-\\lambda x} - (-e^{-\\lambda 0}) \\\\ =&amp; -e^{-\\lambda x} - (-1) \\\\ =&amp; 1 -e^{-\\lambda x} \\\\ \\end{aligned} \\] Then, plug in \\(x = 10\\) and \\(\\lambda = 1/20\\): 1 - exp(-(1/20) * (10)) ## [1] 0.3934693 Check to make sure we’ve done this right using the pexp function in R: pexp(q = 10, rate = 1/20) ## [1] 0.3934693 What is the probability that an earthquake will occur between 10 and 20 years from now? pexp(q = 20, rate = 1/20) - pexp(q = 10, rate = 1/20) ## [1] 0.2386512 If you have time, write the CDF of your chosen distribution as an R function. It should take a value x and a parameter, and return a probability P(X ≤ x). Use your function to plot the cumulative distribution as a function of x. exp_cdf &lt;- function(x, lambda) 1 - exp(-lambda * x) x &lt;- seq(0, 80, by = 0.1) p &lt;- exp_cdf(x = x, lambda = 1/20) plot(p ~ x, type = &quot;l&quot;, xlab = &quot;x&quot;, ylab = &quot;FX(x)&quot;) "],
["randomvars.html", "Chapter 5 Properties of random variables 5.1 Expected values and the law of large numbers 5.2 Variance and standard deviation 5.3 Joint distributions, covariance, and correlation 5.4 Conditional distribution, expectation, variance 5.5 The central limit theorem", " Chapter 5 Properties of random variables library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) 5.1 Expected values and the law of large numbers When summarizing a probability distribution, it is useful to have a measure of: Location (Expectation; E(\\(X\\))) Dispersal (Variance; Var(\\(X\\))) In this section, we’re focusing on the expectation. The expectation of a discrete random variable is the average: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\sum_{i = 1}^{k}x_i P(X = x_i) \\\\ =&amp; \\sum_{i = 1}^{k}x_i f_X(x_i) \\\\ \\end{aligned} \\] If \\(Y\\) represents a six-sided die, then: \\[ \\begin{aligned} \\text{E}(Y) =&amp; \\sum_{i = 1}^{k}y_i f_Y(y_i) \\\\ =&amp; 1(1/6) + 2(1/6) + 3(1/6) + 4(1/6) + 5(1/6) + 6(1/6) \\\\ =&amp; 21/6 \\\\ =&amp; 7/2 \\\\ \\end{aligned} \\] If \\(X\\) is continuous: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\int_{- \\infty}^{\\infty} x f_X(x) dx \\\\ \\end{aligned} \\] Here we are integrating over the probability density function, rather than summing over the mass density function. 5.1.1 Weak law of large numbers The expectation is more like a long-term average, rather than an actual instance (7/2 is not a possible instance of a dice roll). \\(X_i\\) are i.i.d. Assume E(\\(X_1\\)) = E(\\(X_2\\)) = … = E(\\(X_n\\)) = \\(\\mu\\) Define \\(\\overline{X}_n\\) as the mean of the observations: \\[ \\begin{aligned} \\overline{X}_n = \\frac{1}{n} (X_1 + X_2 + X_3 + ... + X_n) \\end{aligned} \\] As \\(n \\rightarrow \\infty\\), \\(\\overline{X}_n\\) “converges in probability” to \\(\\mu\\). This means for any positive constant \\(\\delta\\), \\[ \\begin{aligned} lim_{n \\rightarrow \\infty} \\text{P}(|\\overline{X}_n - \\mu| &gt; \\delta) = 0 \\end{aligned} \\] 5.1.2 Handy facts about expectations The expectation of a constant times a random variable is the constant times the expectation of the random variable: \\[ \\begin{aligned} \\text{E}(aX) = a \\text{E}(X) \\end{aligned} \\] The expectation of a constant is the constant: \\[ \\begin{aligned} \\text{E}(c) = c \\end{aligned} \\] The expectation of a sum of random variables is the sum of the expectations of those random variables: \\[ \\begin{aligned} \\text{E}(X + Y) = \\text{E}(X) + \\text{E}(Y) \\end{aligned} \\] Putting all these facts together, we can calculate the expectation of two random variables \\(X\\) and \\(Y\\) as: \\[ \\begin{aligned} \\text{E}(aX + bY + c) = a \\text{E}(X) + b \\text{E}(Y) + c \\end{aligned} \\] This is called the linearity of expectation, which we will use frequently in the exercises. Linearity does not hold for other measures of location (e.g., median, mode). This fact accounts, in part, for the privileged status of the mean in statistics. To calculate the expectation of a function: \\[ \\begin{aligned} \\text{E}[g(X)] = \\sum_{i = 1}^{k} g(x_i)f_X(x_i) \\end{aligned} \\] \\[ \\begin{aligned} \\text{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x)f_X(x)dx \\end{aligned} \\] 5.1.3 Exercise set 5-1 1a. Expected value of a Bernoulli random variable with parameter p? \\[ \\begin{aligned} f_X(x) = \\text{P}(X = x) = p^x(1 - p)^{1-x} \\text{ for } x \\in \\text{{0, 1}} \\\\ \\end{aligned} \\] Because there are only two outcomes (0 or 1), we can compute the expectation directly: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\sum_0^1 x f_X(x) \\\\ =&amp; \\sum_0^1 x p^x(1 - p)^{1-x} \\\\ =&amp; 0 p^0(1 - p)^{1-0} + 1 p^1(1 - p)^{1-1} \\\\ =&amp; 0 p^0(1 - p)^{1} + 1 p^1(1 - p)^{0} \\\\ =&amp; 0 (1) (1-p) + p(1) \\\\ =&amp; 0 + p \\\\ =&amp; p \\end{aligned} \\] 1b. What is the expected value of a binomial random variable with parameters \\(n\\) and \\(p\\)? Here’s the pmf for the binomial distribution: \\[ \\begin{aligned} f_X(x) = \\text{P}(X = x) = \\binom{n}{x} p^x(1 - p)^{n - x} \\text{ for } x \\in \\text{{0, 1, 2, ..., n}} \\\\ \\end{aligned} \\] If we plug that into the equation for E(\\(X\\)), we get: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\sum_0^n x f_X(x) \\\\ =&amp; \\sum_0^n x \\binom{n}{x} p^x(1 - p)^{n - x} \\\\ \\end{aligned} \\] Well, I don’t know how to evaluate this sum directly, considering the upper limit of \\(n\\) is infinite. So we’ll use the fact that the binomial is the sum of \\(n\\) independent Bernoulli trials (\\(X_i\\)). \\[ \\begin{aligned} \\text{E}(X) =&amp; \\text{E}(\\sum_{i=1}^nX_i) \\end{aligned} \\] Because the expectation is linear, the expectation of the sum is the sum of the expectations; we can rearrange: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\sum_{i=1}^n \\text{E}(X_i) \\end{aligned} \\] From 1a, we can substitute \\(p\\) for \\(\\text{E}(X_i)\\): \\[ \\begin{aligned} \\text{E}(X) =&amp; \\sum_{i=1}^n p \\\\ =&amp; np \\end{aligned} \\] 1c. What is the expected value of a discrete uniform random variable with parameters \\(a\\) and \\(b\\)? The probability mass function is: \\[ \\begin{aligned} \\text{P}(X = k) =&amp; \\frac{1}{b - a + 1} \\\\ \\end{aligned} \\] The expectation is: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\sum_{x = a}^b x f_X(x) \\\\ =&amp; \\sum_{x = a}^b x \\frac{1}{b - a + 1} \\\\ =&amp; \\frac{1}{b - a + 1} \\sum_{x = a}^b x \\\\ \\end{aligned} \\] We were given a hint that is useful now: for integers \\(a\\) and \\(b\\) with \\(b &gt; a\\), the sum of all the integers including \\(a\\) and \\(b\\), is: \\[ \\begin{aligned} \\sum_{k = a}^b k =&amp; \\frac{(a + b)(b - a + 1)}{2} \\\\ \\end{aligned} \\] So, plugging that hint in we get: \\[ \\begin{aligned} =&amp; \\frac{1}{b - a + 1} \\times \\frac{(a + b)(b - a + 1)}{2} \\\\ =&amp; \\frac{a + b}{2} \\\\ \\end{aligned} \\] 1d. What is the expected value of a continuous uniform random variable with parameters \\(a\\) and \\(b\\)? The probability density function is: \\[ \\begin{aligned} \\text{P}(X) =&amp; \\frac{1}{b - a} \\\\ \\end{aligned} \\] The expectation is: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\int_{a}^b x f_X(x) dx \\\\ =&amp; \\int_{a}^b x \\frac{1}{b - a} dx \\\\ =&amp; \\frac{1}{b - a} \\int_{a}^b x dx \\\\ \\end{aligned} \\] Now we have to integrate the 2nd term: \\[ \\begin{aligned} =&amp; \\frac{1}{b - a} \\times \\frac{1}{2} x^2 \\bigg\\rvert_{a}^{b} \\\\ =&amp; \\frac{1}{b - a} \\times (\\frac{b^2}{2} - \\frac{a^2}{2}) \\\\ =&amp; \\frac{1}{b - a} \\times (\\frac{b^2 - a^2}{2}) \\\\ \\end{aligned} \\] We use the hint from earlier, that \\(b^2 - a^2 = (b-a)(b+a)\\): \\[ \\begin{aligned} =&amp; \\frac{1}{b - a} \\times (\\frac{(b-a)(b+a)}{2}) \\\\ =&amp; \\frac{a + b}{2} \\\\ \\end{aligned} \\] Exploring the law of large numbers by simulation. In Edge’s code block below, samp.size represents \\(n\\) in the weak law of large numbers (above); n.samps represents independent random variables \\(X_n\\). The expectation for all \\(X_i\\) is \\(\\mu\\). samp.size &lt;- 20 n.samps &lt;- 1000 samps &lt;- rnorm(samp.size * n.samps, mean = 0, sd = 1) # Each column represents a random variable, X_i # Each row represents a sample (instance) drawn from X_i samp.mat &lt;- matrix(samps, ncol = n.samps) str(samp.mat) ## num [1:20, 1:1000] -0.838 0.625 0.846 -0.187 0.24 ... # Here we calculate the sample mean for each X_i (column) samp.means &lt;- colMeans(samp.mat) str(samp.means) ## num [1:1000] -0.4343 -0.1757 -0.2925 0.2837 0.0551 ... hist(samp.means) 2a. What happens if we change samp.size (i.e., \\(n\\))? n_vector &lt;- c(1, 5, 20, 50, 100, 1000) samp_means_mat &lt;- matrix(data = NA, nrow = n.samps, ncol = length(n_vector)) calculate_sample_means &lt;- function(samp.size = 20, n.samps = 1000){ samps &lt;- rnorm(samp.size * n.samps, mean = 0, sd = 1) samp.mat &lt;- matrix(samps, ncol = n.samps) samp.means &lt;- colMeans(samp.mat) return(samp.means) } par(mfrow = c(2,3)) set.seed(21) for(i in 1:length(n_vector)){ samp_size_i &lt;- n_vector[i] samp_means_i &lt;- calculate_sample_means(samp.size = samp_size_i) hist(samp_means_i, xlim = c(-3, 3), ylim = c(0, 250), xlab = &quot;Sample mean&quot;, main = paste(&quot;n = &quot;, samp_size_i, sep = &quot;&quot;), col = &quot;red&quot;) } 2b. Using the exponential distribution. n_vector &lt;- c(1, 5, 20, 50, 100, 1000) samp_means_mat &lt;- matrix(data = NA, nrow = n.samps, ncol = length(n_vector)) calculate_sample_means_exp &lt;- function(samp.size = 20, n.samps = 1000){ samps &lt;- rexp(samp.size * n.samps, rate = 1) samp.mat &lt;- matrix(samps, ncol = n.samps) samp.means &lt;- colMeans(samp.mat) return(samp.means) } par(mfrow = c(2,3)) set.seed(21) for(i in 1:length(n_vector)){ samp_size_i &lt;- n_vector[i] samp_means_i &lt;- calculate_sample_means_exp(samp.size = samp_size_i) hist(samp_means_i, xlab = &quot;Sample mean&quot;, main = paste(&quot;n = &quot;, samp_size_i, sep = &quot;&quot;), col = &quot;red&quot;) } 5.2 Variance and standard deviation The variance is a measurement of dispersal - i.e., how spread out is the distribution? And spread out from what, exactly? It is useful to think about the distance \\(X_i\\) takes from the expectation, E\\((X)\\): \\(X - \\text{E}(X)\\). What if we took the expectation of this - i.e., the average value of the distance from the mean? \\[ \\begin{aligned} \\text{E}(X - \\text{E}(X)) \\\\ \\text{by linearity of expectation, we get:} \\\\ \\text{E}(X) - \\text{E}(\\text{E}(X)) \\\\ \\text{E}(X) - \\text{E}(X) \\\\ 0 \\end{aligned} \\] This won’t work - we need to find a way to constrain the expression inside the parentheses to be non-negative. One way to do this is to use the mean absolute deviation, \\(|X - \\text{E}(X)|\\). Another way is to use the mean squared deviation, \\([X - \\text{E}(X)]^2\\). The squared term constrains the variance to be \\(\\ge 0\\): \\[ \\begin{aligned} \\text{Var}(X) =&amp; \\text{E}([X - \\text{E}(X)]^2) \\\\ \\end{aligned} \\] The mean squared deviation has two mathematical advantages: It is easier to compute mathematically than an analogous quanitity using absolute deviations (but why?) The variances of linear functions of random variables are ‘beautifully behaved’, whereas the analogous quantities for absolute deviations can be a hassle. I will just take Edge’s word on these two points for now. 5.2.1 Beautiful properties of the variance The variance can be rewritten as: \\[ \\begin{aligned} \\text{Var}(X) = \\text{E}(X^2) - [\\text{E}(X)]^2 \\\\ \\end{aligned} \\] which is generally easier to compute. Adding a constant to a random variable does not affect the variance: \\[ \\begin{aligned} \\text{Var}(a + cX) = c^2\\text{Var}(X) \\\\ \\end{aligned} \\] where \\(a\\) and \\(c\\) are constants. If \\(X\\) and \\(Y\\) are independent random variables, then: \\[ \\begin{aligned} \\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) \\\\ \\end{aligned} \\] One big problem with the variance is that is in the wrong (\\(X^2\\)) units. To fix this, we calculate the standard deviation: \\[ \\begin{aligned} \\text{SD}(X) = \\sqrt{\\text{Var}(X)} \\\\ \\end{aligned} \\] SD is usually larger (never smaller) than MAD, and is more sensitive to large deviations. 5.2.2 Exercise set 5-2 I had to walk through Edge’s solutions bit by bit; my handwritten version is here. 5.3 Joint distributions, covariance, and correlation This section covers four key concepts: Joint probability distribution: the probability distribution of the joint occurrence of \\(X\\) and \\(Y\\) Marginal distribution of X: the probability distribution of \\(X\\), summing (integrating) over all values of \\(Y\\) Covariance: a measurement of the extent to which \\(X\\) and \\(Y\\) depart from independence Correlation: covariance rescaled to go from -1 to 1 5.3.1 Joint probability distributions Joint probability distribution: the probability distribution of the joint occurrence of \\(X\\) and \\(Y\\) The joint cumulative probability distribution of two random variables \\(X\\) and \\(Y\\) is given by: \\[ \\begin{aligned} F_{X,Y}(x, y) = \\text{P} (X \\leq x ~\\cap ~ Y \\leq y) \\end{aligned} \\] Here is the corresponding joint probability mass function: \\[ \\begin{aligned} f_{X,Y}(x, y) = \\text{P} (X = x ~\\cap ~ Y = y) \\end{aligned} \\] And for two continuous variables, we can recover the cumulative distribution function by integrating the probability density function with respect to \\(X\\) and \\(Y\\): \\[ \\begin{aligned} F_{X,Y}(x, y) =&amp; ~ \\text{P}(X \\leq x ~\\cap ~ Y \\leq y) \\\\ =&amp; \\int_{- \\infty}^{x} \\int_{- \\infty}^{y} f_{X,Y}(x, y) dx dy \\end{aligned} \\] 5.3.2 Marginal distributions Marginal distribution of X: the probability distribution of \\(X\\), summing (integrating) over all values of \\(Y\\) For discrete random variables, the marginal distribution of \\(X\\) is: \\[ \\begin{aligned} f_{X}(x) =&amp; ~ \\text{P} (X = x) \\\\ =&amp; ~ \\sum_{y} \\text{P} (X = x ~\\cap ~ Y = y) \\\\ =&amp; \\sum_{y} f_{X,Y}(x,y) \\end{aligned} \\] For continuous random variables, the marginal distribution of \\(X\\) is: \\[ \\begin{aligned} f_{X}(x) =&amp; \\int_{- \\infty}^{\\infty} f_{X,Y}(x, y) dy \\end{aligned} \\] 5.3.3 Covariance Covariance is a measurement of the extent to which \\(X\\) and \\(Y\\) depart from independence Such a measure should have two basic properties: The number should be positive when \\(X\\) and \\(Y\\) increase or decrease together The number should be negative when \\(X\\) increases and \\(Y\\) decreases (and vice versa). Consider the random variable \\([X - \\text{E}(X)][Y - \\text{E}(Y)]\\). If we sample a joint probability distribution, resulting in a set (\\(\\Omega\\)) of pairs of \\((x, y)\\), ask yourself: Is the sign positive or negative when most of the pairs \\((x, y)\\) are such that \\(x &gt; \\text{E}(X)\\) and \\(y &gt; \\text{E}(Y)\\)? Is the sign positive or negative when most of the pairs \\((x, y)\\) are such that \\(x &lt; \\text{E}(X)\\) and \\(y &lt; \\text{E}(Y)\\)? Is the sign positive or negative when most of the pairs \\((x, y)\\) are such that \\(x &gt; \\text{E}(X)\\) and \\(y &lt; \\text{E}(Y)\\)? Is the sign positive or negative when most of the pairs \\((x, y)\\) are such that \\(x &lt; \\text{E}(X)\\) and \\(y &gt; \\text{E}(Y)\\)? Hopefully, you have convinced yourself that the random variable \\([X - \\text{E}(X)][Y - \\text{E}(Y)]\\) satisfies the two aforementioned properties. Now we take the expectation of this random variable to arrive at the covariance. Conveniently (or purposefully?), the covariance is an extension of the variance: \\[ \\begin{aligned} \\text{Cov}(X,Y) =&amp; ~ \\text{E}([X - \\text{E}(X)][Y - \\text{E}(Y)]) \\\\ =&amp; ~ \\text{E}(XY) - \\text{E}(X)\\text{E}(Y) \\end{aligned} \\] If you replace \\(\\text{E}(Y)\\) with \\(\\text{E}(X)\\) in the above equation, you should recover the definition of Var\\((X)\\), \\(\\text{E}(X^2) - [\\text{E}(X)]^2\\). If \\(X\\) and \\(Y\\) are independent, then \\(\\text{Cov}(X,Y) = 0\\) (we showed this in an earlier problem set). However, if \\(\\text{Cov}(X,Y) = 0\\), that does not necessarily imply that \\(X\\) and \\(Y\\) are independent. 5.3.4 Correlation Correlation: covariance rescaled to go from -1 to 1 The covariance is not a pure measure of the linear dependence between two variables, because it is sensitive to the scaling of the variables. Therefore, we cannot use the covariance to compare the strengths of different bivariate relationships. In other words, we cannot use the covariance to answer the question: Is the relationship between cereal yield and fertilizer consumption stronger than the relationship between career earnings and college GPA?. Instead, we calculate the correlation: \\[ \\begin{aligned} \\text{Cor}(X,Y) =&amp; ~ \\rho_{X,Y} \\\\ =&amp; ~ \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}} \\\\ =&amp; ~ \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\\\\ \\end{aligned} \\] You can prove to yourself that the correlation is bounded from -1 to 1 using a simple heuristic. Which variable should be the most correlated with \\(X\\)? Well, that would be \\(X\\). Plugging in \\(X\\) for \\(Y\\), we get: \\[ \\begin{aligned} \\text{Cor}(X,X) =&amp; ~ \\frac{\\text{Cov}(X,X)}{\\sigma_X \\sigma_X} \\\\ =&amp; ~ \\frac{\\text{Var}(X)}{\\text{Var}(X)} \\\\ =&amp; ~ 1 \\end{aligned} \\] Using the same logic, \\(-X\\), is the least correlated with \\(X\\). Try working through the algebra, you should get -1: \\[ \\begin{aligned} \\text{Cor}(X,-X) =&amp; ~ \\frac{\\text{Cov}(X,-X)}{\\sqrt{\\text{Var}(X)\\text{Var}(-X)}} \\\\ =&amp; ~ \\frac{\\text{E}(X \\times -X) - \\text{E}(X)\\text{E}(-X)}{\\sqrt{\\text{Var}(X)\\text{Var}(-X)}} \\\\ \\end{aligned} \\] 5.3.5 Additional exercise I will add one problem, to reinforce the concepts of joint and marginal distributions, with two discrete random variables. This problem covers similar ideas to Edge’s first exercise in set 5-3. You watched 100 female birds last spring, and recorded the number of offspring per bird (X; 1, 2, or 3 chicks). You also recorded the age of each mom (Y; 1, 2, or 3 years). You observed: 10 1-yr olds, all with one chick. 27 2-yr olds; 13 had one chick, 12 had two chicks, and 2 had three chicks. 63 3-yr olds; 23 had one chick, 36 had two chicks, and 4 had three chicks. Calculate: The probability of observing each possible outcome (e.g., a 1-yr old bird has 1 chick; a 1-yr old bird has 2 chicks; etc.). The probability of observing a 1-yr old bird; a 2-yr old bird; and a 3-yr old bird. The probability of observing 1 chick per mom; 2 chicks per mom; 3 chicks per mom. STOP! NO PEEKING ! ANSWER IS BELOW: Wait for it… …wait for it … …here it is: an excel (gasp!) plot! The key here is to recognize that yellow represents the joint probabilities of X and Y; the green and blue represents the marginal probabilities of X and Y, respectively. Stare at this until it clicks. A similar principle applies to continuous distributions, but rather than summing across Y, we integrate across Y to get the marginal distribution of X. 5.3.6 Exercise set 5-3 I had to walk through Edge’s solutions bit by bit; my handwritten version is here. 5.4 Conditional distribution, expectation, variance For two discrete random variables, the conditional probability mass function is: \\[ \\begin{aligned} f_{X|Y}(x |Y = y) =&amp; \\text{P}(X = x | Y = y) \\\\ =&amp; \\frac{\\text{P} (X = x ~\\cap ~ Y = y)}{\\text{P}(Y = y)} \\\\ =&amp; \\frac{f_{X,Y}(x,y)}{f_{Y}(y)} \\end{aligned} \\] For two continuous random variables, the conditional probability density function is defined similarly: \\[ \\begin{aligned} f_{X|Y}(x |Y = y) =&amp; \\frac{f_{X,Y}(x,y)}{f_{Y}(y)} \\end{aligned} \\] Edge has a nice visualization and explanation of conditional distribution in his Fig 5-5. 5.5 The central limit theorem Natural populations are large, so we usually gather just a sample and use that as a surrogate for the whole population. If we take \\(n\\) samples, then another \\(n\\) samples, and then another \\(n\\) samples, and calculate \\(\\overline{X}_1\\), \\(\\overline{X}_2\\), and \\(\\overline{X}_3\\), differences in our estimate of \\(\\overline{X}\\) are due to sampling variation. The weak law of large numbers (above) tells us that as \\(n\\) approaches \\(\\infty\\), our estimate of \\(\\overline{X}_n\\) approaches the true population mean, \\(\\mu\\), and that the Var(\\(\\overline{X}_n\\)) = \\(\\sigma^2 / n\\), approaches 0. But what is the shape of this distribution? That is where the central limit theorem (CLT) comes in. As \\(n\\) approaches \\(\\infty\\), the distribution of \\(\\overline{X}_n\\) converges to a normal distribution with expectation \\(\\mu\\) and variance \\(\\sigma^2 / n\\). An importance consequences of the CLT is the surpising result that the distribution of sample means \\(\\overline{X}_n\\) is approximately normal even when the distribution of the individual observations are not normally distributed! The implications of the CLT are huge: it allows us to use the normal distribution (and the powerful set of analytical tools that depend on it) in real-world situations where the underlying data are not normally distributed, as long as we have enough samples. What is enough? A general rule of thumb is 30, but will vary with the underlying probability distribution of the population. You will explore this using simulations in the problem set below. "]
]
