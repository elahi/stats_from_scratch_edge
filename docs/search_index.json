[
["index.html", "Notes on Statistical Thinking from Scratch Preface", " Notes on Statistical Thinking from Scratch Robin Elahi 2020-03-26 Preface This project is a set of notes for Statistical Thinking from Scratch, by M.D. Edge. The goal is to become more comfortable with the nitty gritty underlying the statistical tools I commonly use - essentially, regression. Edge takes a unique approach in that he takes a small dataset, and dissects regression ‘from scratch’. Each of these bookdown chapters corresponds to each of Edge’s 10 book chapters. The impetus for this bookdown project was the cancellation of my spring 2020 course Experimental Design and Probability due to COVID-19. So, I’ll be filling in some gaps in my stats toolkit and learning how to use bookdown. The stfs package can be installed from Github: install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;mdedge/stfspack&quot;) library(stfspack) As Edge points out in his Prelude, the typical introductory course in biological statistics (including my own) revolves around learning a series of tests: t-tests, regression, ANOVA . In STFS, the focus instead is on one procedure - simple linear regression - and takes ‘little for granted’ (translation: we’ll be working through the math, lightly). The primary dataset consists of 11 observations. To heck with big data. So in this project, you will find mostly code - I’ll leave the exposition to Edge, who does a nice job explaining the concepts in his textbook. I’ll chime in whenever it seems useful - I welcome your comments (send them to my twitter handle: elahi_r). The chapters will work through the following: 1. Probability 2. Estimation (using data to guess the values that describe a data generating process) 3. Inference (testing hypotheses about the processes that might have generated an observed dataset) How to publish a bookdown project on github https://community.rstudio.com/t/hosting-bookdown-in-github/20427 http://seankross.com/2016/11/17/How-to-Start-a-Bookdown-Book.html "],
["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction library(stfspack) library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) Figure 1.1 shows data on the amount of fertilizer applied to cropland (x-axis), and the cereal yields (y-axis), for each of 11 countries in Africa. There is a positive relationship, but is it strong? Is it weak? How are we to reason about these data? anscombe %&gt;% ggplot(aes(x1, y1)) + geom_point() + labs(x = &quot;Fertilizer consumption (kg/hectare)&quot;, y = &quot;Cereal yield (100kg/hectare)&quot;) Figure 1.1: Fertilizer consumption and cereal yield in 11 sub-Saharan African countries Statistics, that’s how! Statistics allows us to reason from data, and rests on a mathematical framework. It is worth understanding, even minimally, this framework. That’s why we are reading this book. Chapter 1 provides an overview of simple linear regression, which allows us to identify a line that ‘best’ fits the data. We can use the lm() function in R to fit a simple linear regression: mod.fit &lt;- lm(y1 ~ x1, data = anscombe) anscombe %&gt;% ggplot(aes(x1, y1)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(x = &quot;Fertilizer consumption (kg/hectare)&quot;, y = &quot;Cereal yield (100kg/hectare)&quot;) Figure 1.2: The agriculture data with a line of best fit from the simple linear regression summary(mod.fit) ## ## Call: ## lm(formula = y1 ~ x1, data = anscombe) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.92127 -0.45577 -0.04136 0.70941 1.83882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0001 1.1247 2.667 0.02573 * ## x1 0.5001 0.1179 4.241 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 9 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.6295 ## F-statistic: 17.99 on 1 and 9 DF, p-value: 0.00217 What do the following columns in the ‘Coefficients’ table refer to? the Estimates: (Intercept) and x1 Std. Error Pr(&gt;|t|) How do each of these relate to the ideas of estimation and inference? Next, a cautionary tale. Here we fit another regression model to a different set of (fake) data, that gives the exact same regression results. mod.fit2 &lt;- lm(y3 ~ x3, data = anscombe) anscombe %&gt;% ggplot(aes(x3, y3)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(x = &quot;Fertilizer consumption (kg/hectare)&quot;, y = &quot;Cereal yield (100kg/hectare)&quot;) Figure 1.3: The data underlying the analysis of the variables y3 and x3 in the anscombe data set You should treat the results of the table with suspicion, given the figure above. summary(mod.fit2) ## ## Call: ## lm(formula = y3 ~ x3, data = anscombe) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.1586 -0.6146 -0.2303 0.1540 3.2411 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0025 1.1245 2.670 0.02562 * ## x3 0.4997 0.1179 4.239 0.00218 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.236 on 9 degrees of freedom ## Multiple R-squared: 0.6663, Adjusted R-squared: 0.6292 ## F-statistic: 17.97 on 1 and 9 DF, p-value: 0.002176 The rest of the book will give us the foundation to interpret all of the values in the regression table, and the underlying assumptions of the linear model. "],
["r-eda.html", "Chapter 2 R and exploratory data analysis 2.1 Inspecting the dataframe 2.2 Histograms 2.3 Summarising data 2.4 Loops 2.5 Functions 2.6 Boxplots 2.7 Scatterplots 2.8 Exercise set 2-2", " Chapter 2 R and exploratory data analysis library(stfspack) library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) This chapter is about getting comfortable with R. Edge uses base R for data wrangling and plotting, but here I’ll also recreate the chapter exercises using the tidyverse. For a deeper dive into R and the tidyverse: Data carpentry’s ecology lesson R for data science 2.1 Inspecting the dataframe head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa glimpse(iris) ## Observations: 150 ## Variables: 5 ## $ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4… ## $ Sepal.Width &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3… ## $ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1… ## $ Petal.Width &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0… ## $ Species &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, … summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## 2.2 Histograms # Base hist(iris$Sepal.Length, xlab = &quot;Sepal Length&quot;, main = &quot;&quot;) # Tidyverse iris %&gt;% ggplot(aes(Sepal.Length)) + geom_histogram(bins = 8, col = &quot;white&quot;) + labs(x = &quot;Sepal Length&quot;) 2.3 Summarising data # Base tapply(iris$Sepal.Length, iris$Species, mean) ## setosa versicolor virginica ## 5.006 5.936 6.588 # Tidyverse iris %&gt;% group_by(Species) %&gt;% summarise(mean = mean(Sepal.Length)) ## # A tibble: 3 x 2 ## Species mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 5.01 ## 2 versicolor 5.94 ## 3 virginica 6.59 Note that the tidyverse output is a tibble (essentially a dataframe), which is a consistent feature of the tidy approach to data wrangling. Not that you can’t do this in base R with the aggregate function - which is how I used to do it BT (before tidyverse): # Base option 2 aggregate(iris$Sepal.Length, list(iris$Species), mean) ## Group.1 x ## 1 setosa 5.006 ## 2 versicolor 5.936 ## 3 virginica 6.588 # You can check the output using str() aggregate(iris$Sepal.Length, list(iris$Species), mean) %&gt;% str() ## &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ Group.1: Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 2 3 ## $ x : num 5.01 5.94 6.59 2.4 Loops for(i in 1:3){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 i &lt;- 1 print(i) ## [1] 1 unique(iris$Species) ## [1] setosa versicolor virginica ## Levels: setosa versicolor virginica for(i in unique(iris$Species)){ print(mean(iris$Sepal.Length[iris$Species == i])) } ## [1] 5.006 ## [1] 5.936 ## [1] 6.588 2.5 Functions conditional.mean &lt;- function(x, y){ for(i in unique(y)){ print(mean(x[y == i])) } } conditional.mean(x = iris$Sepal.Length, y = iris$Species) ## [1] 5.006 ## [1] 5.936 ## [1] 6.588 conditional.mean(x = iris$Sepal.Width, y = iris$Species) ## [1] 3.428 ## [1] 2.77 ## [1] 2.974 2.6 Boxplots # Base boxplot(iris$Sepal.Length ~ iris$Species, xlab = &quot;Species&quot;, ylab = &quot;Sepal length&quot;) # Tidyverse iris %&gt;% ggplot(aes(Species, Sepal.Length)) + geom_boxplot() + labs(x = &quot;Species&quot;, y = &quot;Sepal length&quot;) 2.7 Scatterplots # Base plot(iris$Sepal.Length, iris$Sepal.Width, xlab = &quot;Sepal length&quot;, ylab = &quot;Sepal width&quot;) # Tidyverse iris %&gt;% ggplot(aes(Sepal.Length, Sepal.Width)) + geom_point() + labs(x = &quot;Sepal length&quot;, y = &quot;Sepal width&quot;) With unique symbols for species: # Base plot(iris$Sepal.Length, iris$Sepal.Width, xlab = &quot;Sepal length&quot;, ylab = &quot;Sepal width&quot;, pch = as.numeric(iris$Species)) legend(&quot;topright&quot;, pch = c(1,2,3), legend = c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;)) # Tidyverse iris %&gt;% ggplot(aes(Sepal.Length, Sepal.Width, shape = Species)) + geom_point() + labs(x = &quot;Sepal length&quot;, y = &quot;Sepal width&quot;) 2.8 Exercise set 2-2 Repeat the above analyses (histogram, summarising data, boxplots) for Petal.Width. Install and load a new package gpairs. Run the following line of code. What do you see? gpairs(iris, scatter.pars = list(col = as.numeric(iris$Species))) Install and load the package stfspack if you have not already done so. install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;mdedge/stfspack&quot;) library(stfspack) "],
["best-fit-line.html", "Chapter 3 Line of best fit 3.1 Exercise set 3-1 3.2 Exercise set 3-2", " Chapter 3 Line of best fit library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) 3.1 Exercise set 3-1 Calculate least-squares intercept a and least-squares slope b for the anscombe data. I will use equations 3.8 and 3.9. x &lt;- anscombe$x1 y &lt;- anscombe$y1 xbar &lt;- mean(x) ybar &lt;- mean(y) b &lt;- sum((x - xbar)*(y - ybar)) / sum((x - xbar)^2) b ## [1] 0.5000909 a &lt;- ybar - b*xbar a ## [1] 3.000091 # Check using lm m1 &lt;- lm(y ~ x) summary(m1) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.92127 -0.45577 -0.04136 0.70941 1.83882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0001 1.1247 2.667 0.02573 * ## x 0.5001 0.1179 4.241 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 9 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.6295 ## F-statistic: 17.99 on 1 and 9 DF, p-value: 0.00217 3.2 Exercise set 3-2 On paper On paper On paper Comparing L1 and L2 lines. library(quantreg) x &lt;- anscombe$x1 y &lt;- anscombe$y1 mL1 &lt;- rq(y ~ x, tau = 0.5) mL2 &lt;- lm(y ~ x) plot(x, y) abline(mL2, lty = 1) abline(mL1, lty = 2) legend(&quot;topleft&quot;, legend = c(&quot;L2&quot;, &quot;L1&quot;), lty = c(1, 2)) Dataset with outlier. mL1 &lt;- rq(y3 ~ x3, tau = 0.5, data = anscombe) mL2 &lt;- lm(y3 ~ x3, data = anscombe) plot(y3 ~ x3, data = anscombe) abline(mL2, lty = 1) abline(mL1, lty = 2) legend(&quot;topleft&quot;, legend = c(&quot;L2&quot;, &quot;L1&quot;), lty = c(1, 2)) "],
["probability.html", "Chapter 4 Probability and random variables 4.1 Probability vs estimation 4.2 What is a probability? 4.3 Exercise set 4-1 4.4 Exercise set 4-2", " Chapter 4 Probability and random variables library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) 4.1 Probability vs estimation Here are two quotes from STFS that encapsulate the distinction between probability and estimation: In probability theory, we think about processes that generate data, and we ask \"what can we say about the data generated by such a process? In statistical estimation and inference, we work in the opposite direction. We start with data, and we ask \"what can we say about the process that generated these data? So, we can define probability as the study of data generated by specified processes - and that’s the focus of this chapter. Later on, when we get to data - we’ll be assuming probabilistic models - but we have to accept the fact that the data we observed were not likely to be generated by the assumed model! But we do the best we can. 4.2 What is a probability? Frequency view the probability of a given event is simply the proportion of times it occurs over many trials difficult to apply to statements like “what is the probability that it will rain tomorrow” and “what is the probability that a meteor will hit the earth tomorrow” (i.e., one-shot events) Degrees-of-belief view what is the degree of belief in the occurrence of an event that a rational person would have? An aside: is the universe fundamentally probabilistic or deterministic? 4.2.1 Set notation A set (\\(\\Omega\\), in STFS) is an unordered collection of objects (elements). Intersection (\\(\\cap\\)) of two sets is the set of elements that appear in both sets. Union (\\(\\cup\\)) of two sets is the set that has every element that appears in either of the two original sets. The complement of a set (\\(S^C\\)) includes all of the elements not in the original set but present in the sample space (\\(\\Omega\\)) (which contains all possible elements). Subsets are referred to as S, and have the following properties: \\[ \\begin{aligned} S \\cup S^C = \\Omega \\\\ S \\cap S^C = \\emptyset \\end{aligned} \\] where \\(\\emptyset\\) is used to denote the empty set, or the set with no elements. A Venn diagram is useful here: 4.2.2 Kolmogorov’s three axioms of probability Probabilities cannot be negative The probability of the event that includes every outcome is 1. The probability of observing either of two mutually exclusive events is the sum of their individual probabilities 4.3 Exercise set 4-1 4.4 Exercise set 4-2 "]
]
