[
["index.html", "Notes on Statistical Thinking from Scratch Preface", " Notes on Statistical Thinking from Scratch Robin Elahi 2020-05-06 Preface This project is a set of notes for Statistical Thinking from Scratch, by M.D. Edge. The goal is to become more comfortable with the nitty gritty underlying the statistical tools I commonly use - essentially, regression. Edge takes a unique approach in that he takes a small dataset, and dissects regression ‘from scratch’. Each of these bookdown chapters corresponds to each of Edge’s 10 book chapters. The impetus for this bookdown project was the cancellation of my spring 2020 course Experimental Design and Probability due to COVID-19. So, I’ll be filling in some gaps in my stats toolkit and learning how to use bookdown. The stfs package can be installed from Github: install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;mdedge/stfspack&quot;) library(stfspack) As Edge points out in his Prelude, the typical introductory course in biological statistics (including my own) revolves around learning a series of tests: t-tests, regression, ANOVA . In STFS, the focus instead is on one procedure - simple linear regression - and takes ‘little for granted’ (translation: we’ll be working through the math, lightly). The primary dataset consists of 11 observations. To heck with big data. So in this project, you will find mostly code - I’ll leave the exposition to Edge, who does a nice job explaining the concepts in his textbook. I’ll chime in whenever it seems useful. The chapters will work through the following: 1. Probability 2. Estimation (using data to guess the values that describe a data generating process) 3. Inference (testing hypotheses about the processes that might have generated an observed dataset) How to publish a bookdown project on github https://community.rstudio.com/t/hosting-bookdown-in-github/20427 http://seankross.com/2016/11/17/How-to-Start-a-Bookdown-Book.html "],
["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction library(stfspack) library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) Figure 1.1 shows data on the amount of fertilizer applied to cropland (x-axis), and the cereal yields (y-axis), for each of 11 countries in Africa. There is a positive relationship, but is it strong? Is it weak? How are we to reason about these data? anscombe %&gt;% ggplot(aes(x1, y1)) + geom_point() + labs(x = &quot;Fertilizer consumption (kg/hectare)&quot;, y = &quot;Cereal yield (100kg/hectare)&quot;) Figure 1.1: Fertilizer consumption and cereal yield in 11 sub-Saharan African countries Statistics, that’s how! Statistics allows us to reason from data, and rests on a mathematical framework. It is worth understanding, even minimally, this framework. That’s why we are reading this book. Chapter 1 provides an overview of simple linear regression, which allows us to identify a line that ‘best’ fits the data. We can use the lm() function in R to fit a simple linear regression: mod.fit &lt;- lm(y1 ~ x1, data = anscombe) anscombe %&gt;% ggplot(aes(x1, y1)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(x = &quot;Fertilizer consumption (kg/hectare)&quot;, y = &quot;Cereal yield (100kg/hectare)&quot;) Figure 1.2: The agriculture data with a line of best fit from the simple linear regression summary(mod.fit) ## ## Call: ## lm(formula = y1 ~ x1, data = anscombe) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.92127 -0.45577 -0.04136 0.70941 1.83882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0001 1.1247 2.667 0.02573 * ## x1 0.5001 0.1179 4.241 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 9 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.6295 ## F-statistic: 17.99 on 1 and 9 DF, p-value: 0.00217 What do the following columns in the ‘Coefficients’ table refer to? the Estimates: (Intercept) and x1 Std. Error Pr(&gt;|t|) How do each of these relate to the ideas of estimation and inference? Next, a cautionary tale. Here we fit another regression model to a different set of (fake) data, that gives the exact same regression results. mod.fit2 &lt;- lm(y3 ~ x3, data = anscombe) anscombe %&gt;% ggplot(aes(x3, y3)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(x = &quot;Fertilizer consumption (kg/hectare)&quot;, y = &quot;Cereal yield (100kg/hectare)&quot;) Figure 1.3: The data underlying the analysis of the variables y3 and x3 in the anscombe data set You should treat the results of the table with suspicion, given the figure above. summary(mod.fit2) ## ## Call: ## lm(formula = y3 ~ x3, data = anscombe) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.1586 -0.6146 -0.2303 0.1540 3.2411 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0025 1.1245 2.670 0.02562 * ## x3 0.4997 0.1179 4.239 0.00218 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.236 on 9 degrees of freedom ## Multiple R-squared: 0.6663, Adjusted R-squared: 0.6292 ## F-statistic: 17.97 on 1 and 9 DF, p-value: 0.002176 The rest of the book will give us the foundation to interpret all of the values in the regression table, and the underlying assumptions of the linear model. "],
["r-eda.html", "Chapter 2 R and exploratory data analysis 2.1 Inspecting the dataframe 2.2 Histograms 2.3 Summarising data 2.4 Loops 2.5 Functions 2.6 Boxplots 2.7 Scatterplots 2.8 Exercise set 2-2", " Chapter 2 R and exploratory data analysis library(stfspack) library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) This chapter is about getting comfortable with R. Edge uses base R for data wrangling and plotting, but here I’ll also recreate the chapter exercises using the tidyverse. For a deeper dive into R and the tidyverse: Data carpentry’s ecology lesson R for data science 2.1 Inspecting the dataframe head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa glimpse(iris) ## Observations: 150 ## Variables: 5 ## $ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4… ## $ Sepal.Width &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3… ## $ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1… ## $ Petal.Width &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0… ## $ Species &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, … summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## 2.2 Histograms # Base hist(iris$Sepal.Length, xlab = &quot;Sepal Length&quot;, main = &quot;&quot;) # Tidyverse iris %&gt;% ggplot(aes(Sepal.Length)) + geom_histogram(bins = 8, col = &quot;white&quot;) + labs(x = &quot;Sepal Length&quot;) 2.3 Summarising data # Base tapply(iris$Sepal.Length, iris$Species, mean) ## setosa versicolor virginica ## 5.006 5.936 6.588 # Tidyverse iris %&gt;% group_by(Species) %&gt;% summarise(mean = mean(Sepal.Length)) ## # A tibble: 3 x 2 ## Species mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 5.01 ## 2 versicolor 5.94 ## 3 virginica 6.59 Note that the tidyverse output is a tibble (essentially a dataframe), which is a consistent feature of the tidy approach to data wrangling. Not that you can’t do this in base R with the aggregate function - which is how I used to do it BT (before tidyverse): # Base option 2 aggregate(iris$Sepal.Length, list(iris$Species), mean) ## Group.1 x ## 1 setosa 5.006 ## 2 versicolor 5.936 ## 3 virginica 6.588 # You can check the output using str() aggregate(iris$Sepal.Length, list(iris$Species), mean) %&gt;% str() ## &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ Group.1: Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 2 3 ## $ x : num 5.01 5.94 6.59 2.4 Loops for(i in 1:3){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 i &lt;- 1 print(i) ## [1] 1 unique(iris$Species) ## [1] setosa versicolor virginica ## Levels: setosa versicolor virginica for(i in unique(iris$Species)){ print(mean(iris$Sepal.Length[iris$Species == i])) } ## [1] 5.006 ## [1] 5.936 ## [1] 6.588 2.5 Functions conditional.mean &lt;- function(x, y){ for(i in unique(y)){ print(mean(x[y == i])) } } conditional.mean(x = iris$Sepal.Length, y = iris$Species) ## [1] 5.006 ## [1] 5.936 ## [1] 6.588 conditional.mean(x = iris$Sepal.Width, y = iris$Species) ## [1] 3.428 ## [1] 2.77 ## [1] 2.974 2.6 Boxplots # Base boxplot(iris$Sepal.Length ~ iris$Species, xlab = &quot;Species&quot;, ylab = &quot;Sepal length&quot;) # Tidyverse iris %&gt;% ggplot(aes(Species, Sepal.Length)) + geom_boxplot() + labs(x = &quot;Species&quot;, y = &quot;Sepal length&quot;) 2.7 Scatterplots # Base plot(iris$Sepal.Length, iris$Sepal.Width, xlab = &quot;Sepal length&quot;, ylab = &quot;Sepal width&quot;) # Tidyverse iris %&gt;% ggplot(aes(Sepal.Length, Sepal.Width)) + geom_point() + labs(x = &quot;Sepal length&quot;, y = &quot;Sepal width&quot;) With unique symbols for species: # Base plot(iris$Sepal.Length, iris$Sepal.Width, xlab = &quot;Sepal length&quot;, ylab = &quot;Sepal width&quot;, pch = as.numeric(iris$Species)) legend(&quot;topright&quot;, pch = c(1,2,3), legend = c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;)) # Tidyverse iris %&gt;% ggplot(aes(Sepal.Length, Sepal.Width, shape = Species)) + geom_point() + labs(x = &quot;Sepal length&quot;, y = &quot;Sepal width&quot;) 2.8 Exercise set 2-2 Repeat the above analyses (histogram, summarising data, boxplots) for Petal.Width. Install and load a new package gpairs. Run the following line of code. What do you see? gpairs(iris, scatter.pars = list(col = as.numeric(iris$Species))) Install and load the package stfspack if you have not already done so. install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;mdedge/stfspack&quot;) library(stfspack) "],
["best-fit-line.html", "Chapter 3 Line of best fit 3.1 Exercise set 3-1 3.2 Exercise set 3-2", " Chapter 3 Line of best fit library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) 3.1 Exercise set 3-1 Calculate least-squares intercept a and least-squares slope b for the anscombe data. I will use equations 3.8 and 3.9. x &lt;- anscombe$x1 y &lt;- anscombe$y1 xbar &lt;- mean(x) ybar &lt;- mean(y) b &lt;- sum((x - xbar)*(y - ybar)) / sum((x - xbar)^2) b ## [1] 0.5000909 a &lt;- ybar - b*xbar a ## [1] 3.000091 # Check using lm m1 &lt;- lm(y ~ x) summary(m1) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.92127 -0.45577 -0.04136 0.70941 1.83882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0001 1.1247 2.667 0.02573 * ## x 0.5001 0.1179 4.241 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 9 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.6295 ## F-statistic: 17.99 on 1 and 9 DF, p-value: 0.00217 3.2 Exercise set 3-2 On paper On paper On paper Comparing L1 and L2 lines. library(quantreg) x &lt;- anscombe$x1 y &lt;- anscombe$y1 mL1 &lt;- rq(y ~ x, tau = 0.5) mL2 &lt;- lm(y ~ x) plot(x, y) abline(mL2, lty = 1) abline(mL1, lty = 2) legend(&quot;topleft&quot;, legend = c(&quot;L2&quot;, &quot;L1&quot;), lty = c(1, 2)) Dataset with outlier. mL1 &lt;- rq(y3 ~ x3, tau = 0.5, data = anscombe) mL2 &lt;- lm(y3 ~ x3, data = anscombe) plot(y3 ~ x3, data = anscombe) abline(mL2, lty = 1) abline(mL1, lty = 2) legend(&quot;topleft&quot;, legend = c(&quot;L2&quot;, &quot;L1&quot;), lty = c(1, 2)) "],
["probability.html", "Chapter 4 Probability and random variables 4.1 Kolmogorov’s three axioms of probability 4.2 Conditional probability and independence 4.3 Bayes’ Theorem 4.4 Discrete random variables and distributions 4.5 Continuous random variables and distributions 4.6 Probability density functions 4.7 Families of distributions", " Chapter 4 Probability and random variables library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) 4.0.1 Probability vs estimation Here are two quotes from STFS that encapsulate the distinction between probability and estimation: In probability theory, we think about processes that generate data, and we ask \"what can we say about the data generated by such a process? In statistical estimation and inference, we work in the opposite direction. We start with data, and we ask \"what can we say about the process that generated these data? So, we can define probability as the study of data generated by specified processes - and that’s the focus of this chapter. Later on, when we get to data - we’ll be assuming probabilistic models - but we have to accept the fact that the data we observed were not likely to be generated by the assumed model! But we do the best we can. 4.0.2 What is a probability? Frequency view the probability of a given event is simply the proportion of times it occurs over many trials difficult to apply to statements like “what is the probability that it will rain tomorrow” and “what is the probability that a meteor will hit the earth tomorrow” (i.e., one-shot events) Degrees-of-belief view what is the degree of belief in the occurrence of an event that a rational person would have? An aside: is the universe fundamentally probabilistic or deterministic? 4.0.3 Set notation A set (\\(\\Omega\\), in STFS) is an unordered collection of objects (elements). Intersection (\\(\\cap\\)) of two sets is the set of elements that appear in both sets. Union (\\(\\cup\\)) of two sets is the set that has every element that appears in either of the two original sets. The complement of a set (\\(S^C\\)) includes all of the elements not in the original set but present in the sample space (\\(\\Omega\\)) (which contains all possible elements). Subsets are referred to as S, and have the following properties: \\[ \\begin{aligned} S \\cup S^C = \\Omega \\\\ S \\cap S^C = \\emptyset \\end{aligned} \\] where \\(\\emptyset\\) is used to denote the empty set, or the set with no elements. A Venn diagram is useful here: The notation for the probability of an event is typically denoted using either \\(P()\\) or \\([]\\): \\[ \\begin{aligned} P(event) = [event] \\\\ \\end{aligned} \\] 4.1 Kolmogorov’s three axioms of probability Probabilities of any event i (\\(E_i\\)) cannot be negative: \\[ \\begin{aligned} \\ [E_i] \\geq 0 \\\\ \\end{aligned} \\] The probability of the event that includes every outcome is 1: \\[ \\begin{aligned} \\ [\\Omega] = 1 \\\\ \\end{aligned} \\] The probability of observing either of two mutually exclusive events is the sum of their individual probabilities: \\[ \\begin{aligned} \\text{If } E_1 \\cap E_2 = \\emptyset, \\\\ \\text{then } [E_1 \\cup E_2] = [E_1] + [E_2] \\end{aligned} \\] 4.1.1 Exercise set 4-1 Given P(\\(A\\)), we would like to know P(\\(A^C\\)). We already learned the following properties of complements: \\[ \\begin{aligned} A \\cup A^C = \\Omega \\\\ A \\cap A^C = \\emptyset \\end{aligned} \\] The intersection between \\(A\\) and \\(A^C\\) is zero, meaning they don’t share any elements. The union of \\(A\\) and \\(A^C\\) is \\([\\Omega]\\). We also learned the 2nd axiom of probability, \\([\\Omega] = 1\\). Therefore: \\[ \\begin{aligned} \\ [A] + [A^C] = 1 \\\\ \\ [A^C] = 1 - [A] \\end{aligned} \\] On paper. Below I’ve pasted an image of my handwritten version of Edge’s solution - I found it easier to think about once I visualized it using a Venn diagram. 4.2 Conditional probability and independence The law of conditional probability states that: \\[ \\begin{aligned} \\ [A \\mid B] = \\frac{[A \\cap B]}{[B]} \\\\ \\end{aligned} \\] If A and B are independent, then: \\[ \\begin{aligned} \\ [A \\mid B] = [A] \\\\ \\end{aligned} \\] 4.2.1 Exercise set 4-2 If we rearrange the law of probability, we get: \\[ \\begin{aligned} \\ [A \\mid B] [B] = [A \\cap B] \\\\ \\end{aligned} \\] By definition, if A and B are independent, then we can replace \\([A \\mid B]\\) with \\([A]\\) and get: \\[ \\begin{aligned} \\ [A \\cap B] = [A] [B] \\\\ \\end{aligned} \\] If we divide both sides by \\([A]\\), we get: \\[ \\begin{aligned} \\ [B] = \\frac{[A \\cap B]}{[A]} \\\\ \\end{aligned} \\] The right-hand side of the above equation is, by the law of conditional probability, equal to \\([B \\mid A]\\) and thus: \\[ \\begin{aligned} \\ [B] = [B \\mid A]\\\\ \\end{aligned} \\] Suppose you know \\([A \\mid B]\\), \\([A]\\), and \\([B]\\). Calculate \\([B \\mid A]\\). \\[ \\begin{aligned} \\ [A \\mid B] = \\frac{[A \\cap B]}{[B]} \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\ [A \\mid B] [B] = [A \\cap B] \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\ [B \\mid A] = \\frac{[A \\cap B]}{[A]} \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\ [B \\mid A] = \\frac{[A \\mid B] [B]}{[A]} \\\\ \\end{aligned} \\] 4.3 Bayes’ Theorem 4.4 Discrete random variables and distributions Random variable: e.g., the process of rolling a die, \\(X\\) \\(X\\) is random, can take integer values 1-6 \\(X\\) is not a number yet - it is the unrealized outcome of a random process The realization of that process is an instance - which is a number Capital \\(X\\): random variable Lower case \\(x\\): instance All the probability information is contained in its distribution. In our case, \\(X\\) is a discrete random variable, meaning that the number of outcomes is countable, in principle. Two ways to represent the distribution: Probability mass function (pmf), \\(f_X(x) = P(X = x)\\) Cumulative distribution function (cdf), \\(F_X(x) = P(X \\leq x)\\) The cdf is a series of partial sums of the pmf, \\[ \\begin{aligned} F_X(x) = P(X \\leq x) = \\sum_{x_i \\leq x} f_X(x_i) \\end{aligned} \\] and increases monotonically in \\(x\\). 4.4.1 Exercise set 4-3 Flipping a fair coin 3 times. X is a random variable that represents the number of heads observed, and the sample space \\(\\Omega\\) contains the elements {0, 1, 2, 3}. Here are all of the ways we can observe these elements: x = 0: (T, T, T) x = 1: (H, T, T); (T, H, T); (T, T, H) x = 2: (H, H, T); (H, T, H); (T, H, H) x = 3: (H, H, H) There are 8 possible instances. Thus, the probability mass function is: \\[ \\begin{aligned} f_X(0) =&amp; f_X(3) = 1/8 \\\\ f_X(1) =&amp; f_X(2) = 3/8 \\\\ f_X(x) =&amp; 0 \\text{ for all other } x \\\\ \\end{aligned} \\] If we sum \\(f_X(x_i)\\) for all possible \\(x_i\\), the sum would be 1 (1/8 + 1/8 + 3/8 + 3/8). In terms of \\(F_X\\), what is \\(P(a \\lt X \\le b)\\), if \\(b \\gt a\\)? Hint: notice that \\(a \\lt X \\le b\\) if and only if \\(X \\leq b\\) and \\(X &gt; a\\). Note that the cdf, \\(F_X\\), for \\(x = a\\) and \\(x = b\\) are given by the following: \\[ \\begin{aligned} F_X(a) =&amp; P(X \\le a) \\\\ F_X(b) =&amp; P(X \\le b) \\\\ \\end{aligned} \\] We want the probability of observing a value \\(x\\) between \\(a\\) and \\(b\\), so: \\[ \\begin{aligned} P(a \\lt X \\le b) = F_X(b) - F_X(a) \\\\ \\end{aligned} \\] 4.5 Continuous random variables and distributions What about values that are not countable - anything with a decimal? Impossible to get a specific number, or instance (e.g., the probability a person weighs exactly 70kg), and thus we cannot use the pmf. But we can still use the cdf: \\[ \\begin{aligned} F_X(x) = P(X \\leq x) \\end{aligned} \\] That is, we can ask about the probability that a person will weigh 70kg or less, or whether a person will weigh between 70 and 71kg. 4.5.1 Exercise set 4-4 \\(X\\) is a random variable that takes values in the interval [0, 1], and the probability distribution is uniform. Draw the cumulative distribution function \\(F_X(x)\\) for \\(x \\in [-1, 2]\\). x &lt;- c(0,1) Fx &lt;- x plot(x, Fx, type = &quot;l&quot;, xlim = c(-1,2)) lines(c(-1, 0), c(0, 0)) lines(c(1,2), c(1, 1)) If \\(X\\) were more likely to land in [0.4, 0.6] than in any other region of length 0.2, the line between 0.4 and 0.6 would be steeper. x1 &lt;- c(0,0.4) x2 &lt;- c(0.4, 0.6) x3 &lt;- c(0.6, 1) Fx1 &lt;- c(0,0.3) Fx2 &lt;- c(0.3, 0.7) Fx3 &lt;- c(0.7, 1) plot(x1, Fx1, type = &quot;l&quot;, xlim = c(-1,2), ylim = c(0,1), xlab = &quot;x&quot;, ylab = &quot;Fx&quot;) lines(x2, Fx2) lines(x3, Fx3) lines(c(-1, 0), c(0, 0)) lines(c(1,2), c(1, 1)) 4.6 Probability density functions For a continuous random variable, the pdf is the derivative of the cdf Recall that the cdf for a discrete random variable is a series of partial sums, given by: \\[ \\begin{aligned} F_X(x) = P(X \\leq x) = \\sum_{x_i \\leq x} f_X(x_i) \\end{aligned} \\] In an analogous fashion, we can integrate a continuous function to get the cdf of a continous random variable. We define the probability density function \\(f_X\\) (pdf) of a continuous random variable as: \\[ \\begin{aligned} F_X(x) =&amp; \\int_{- \\infty}^{x} f_X(u) du \\\\ \\end{aligned} \\] Below I have re-created Fig 4-4, with a pdf in the upper panel and a cdf in the lower panel - for an exponential random variable with rate 1. 4.6.1 Additional viz We can visualize the principle of question 3 from Exercise Set 4-3 using the cdf of a standard normal distribution, where \\(a = -1\\) and \\(b = 1\\): 4.6.2 Exercise set 4-5 If \\(f_X(x)\\) is a probability density funcion, then total area under \\(f_X(x)\\) is 1. Yes, \\(f_x\\) can be a probability density funcion, because the area under the function is 1. This situation is different from a probability mass function, because the y-axis values for a pdf can be &gt; 1 (in this case, the maximum y is 10). 4.7 Families of distributions Two requirements for mass or density functions: Must be non-negative Sum of all values is 1 (mass), or area under curve is 1 (density) Distribution family similarly shaped distributions summaries of their behavior can be computed from the same functions but parameter values differ 4.7.1 Exercise set 4-6 The probability mass function of the Poisson distribution is \\(P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\). Plugging in the appropriate values for \\(k\\) and \\(\\lambda\\) gives: (i). \\(e^{-5}\\) (ii). \\(5e^{-5}\\) (iii). \\(\\frac{25}{2}e^{-5}\\) Use the probability mass function of the geometric distribution with parameter 1/2. If our first “heads” occurs on the 6th flip, then we have five tails before it. We plug \\(p\\) = 1/2 and \\(k\\) = 5 into \\(P(X = k) = (1-p)^kp\\) to get 1/64. Consider a Poisson distribution with parameter \\(\\lambda\\) = 5. If we want to know the value of the probability mass function for x = 2, \\(f_X(2)\\), we use the dpois() function: dpois(2, lambda = 5) ## [1] 0.08422434 To get the value of the cumulative distribution function \\(F_X(2)\\), we use ppois(): ppois(2, lambda = 5) ## [1] 0.124652 If we want to know the inverse of the cumulative distribution function. That is, we want to know the value of \\(q\\) that solves the equation \\(F_X(q) = p\\). What is the number \\(q\\) such that the probability tha the random variable is less than or equal to \\(q\\) is \\(p\\); \\(q\\) is the \\(pth\\) percentile of the distribution. We can get this using qpois(): qpois(0.124652, lambda = 5) ## [1] 2 The inverse of the cdf is also called the quantile function. Using the standard normal distribution (mean = 0, sd = 1), plot the probability density function for x \\(\\in\\) [-3, 3]. x &lt;- seq(-3, 3, by = 0.1) fx &lt;- dnorm(x = x) plot(x, fx, type = &quot;l&quot;, main = &quot;PDF&quot;, col = &quot;red&quot;) Plot the cdf for x \\(\\in\\) [-3, 3]. x &lt;- seq(-3, 3, by = 0.1) fx &lt;- pnorm(q = x) plot(x, fx, type = &quot;l&quot;, main = &quot;CDF&quot;, col = &quot;red&quot;) What value of \\(x\\) is at the 97.5th percentile of the standard normal? qnorm(p = 0.975) ## [1] 1.959964 Simulating from a normal distribution and from a uniform distribution: n &lt;- 1000 x &lt;- rnorm(n) hist(x) x &lt;- runif(n, min = 0, max = 1) hist(x) Now take those values between 0 and 1, and feed them into the qnorm function to get the values at which we see those quantiles: y &lt;- qnorm(p = x, mean = 0, sd = 1) hist(y) These values are normally distributed around 0. This plot is saying that most of the probability (in fact ~95%) lies between -2 &lt; y &lt; 2 (2 standard deviations). r &lt;- seq(-3, 3, length.out = 1000) cdf &lt;- pnorm(r) #Draw the normal cumulative distribution function. plot(r, cdf, type = &quot;l&quot;, xaxs = &quot;i&quot;, yaxs = &quot;i&quot;, xlim = c(-3, 3), xlab = expression(italic(x)), ylab = expression(paste(italic(F[X]), &quot;(&quot;, italic(x), &quot;)&quot;, sep = &quot;&quot;)), lwd = 2) #Draw light grey lines representing random samples from the #standard normal distribution. x &lt;- rnorm(100) for(i in x){ lines(c(i,i), c(min(x), pnorm(i)), col = rgb(190, 190, 190, alpha = 60, max = 255)) lines(c(min(x)-1,i), c(pnorm(i), pnorm(i)), col = rgb(190, 190, 190, alpha = 60, max = 255)) } 4.7.2 Additional exercise (Courtesy Blondin &amp; Goodman) Assume that destructive earthquakes occur in California every 20 years. You’d like to know how probable an earthquake is between now and some future date. What distribution family describes the waiting time until California’s next earthquake? What value should we use for this distribution’s parameter? [Hint: convert 20 years to a rate] Use the exponential distribution, where \\(\\lambda = 1/20\\): \\[ \\begin{aligned} f_X(x) = \\lambda e ^{-\\lambda x} \\\\ \\end{aligned} \\] What is the probability that an earthquake will happen in the next 10 years? [Hint: You need to find a cumulative distribution function for the distribution you chose, either online or by integration]. Rules for integrating “e raised to the x power”: \\[ \\begin{aligned} \\int e^xdx =&amp; e^x + c \\\\ \\int e^{ax}dx =&amp; \\frac{1}{a} e^{ax} + c \\\\ \\int be^{ax}dx =&amp; \\frac{b}{a} e^{ax} + c \\\\ \\end{aligned} \\] With these rules in hand, we can get the cdf by integrating the pdf for the exponential distribution: \\[ \\begin{aligned} f_X(x) =&amp; \\lambda e ^{-\\lambda x} \\\\ F_X(X = x) =&amp; \\int_{0}^{x} \\lambda e^{-\\lambda x} dx \\\\ =&amp; \\frac{1}{- \\lambda} \\lambda e^{-\\lambda x} \\bigg\\rvert_{0}^{x} \\\\ =&amp; -e^{-\\lambda x} \\rvert_{0}^{x} \\\\ =&amp; -e^{-\\lambda x} \\rvert_{0}^{x} \\\\ =&amp; -e^{-\\lambda x} - (-e^{-\\lambda 0}) \\\\ =&amp; -e^{-\\lambda x} - (-1) \\\\ =&amp; 1 -e^{-\\lambda x} \\\\ \\end{aligned} \\] Then, plug in \\(x = 10\\) and \\(\\lambda = 1/20\\): 1 - exp(-(1/20) * (10)) ## [1] 0.3934693 Check to make sure we’ve done this right using the pexp function in R: pexp(q = 10, rate = 1/20) ## [1] 0.3934693 What is the probability that an earthquake will occur between 10 and 20 years from now? pexp(q = 20, rate = 1/20) - pexp(q = 10, rate = 1/20) ## [1] 0.2386512 If you have time, write the CDF of your chosen distribution as an R function. It should take a value x and a parameter, and return a probability P(X ≤ x). Use your function to plot the cumulative distribution as a function of x. exp_cdf &lt;- function(x, lambda) 1 - exp(-lambda * x) x &lt;- seq(0, 80, by = 0.1) p &lt;- exp_cdf(x = x, lambda = 1/20) plot(p ~ x, type = &quot;l&quot;, xlab = &quot;x&quot;, ylab = &quot;FX(x)&quot;) "],
["randomvars.html", "Chapter 5 Properties of random variables 5.1 Expected values and the law of large numbers 5.2 Variance and standard deviation 5.3 Joint distributions, covariance, and correlation 5.4 Conditional distribution, expectation, variance 5.5 The central limit theorem 5.6 A probabilistic model for simple linear regression", " Chapter 5 Properties of random variables library(tidyverse) theme_set(theme_bw(base_size = 12) + theme(strip.background = element_blank(), panel.grid = element_blank())) 5.1 Expected values and the law of large numbers When summarizing a probability distribution, it is useful to have a measure of: Location (Expectation; E(\\(X\\))) Dispersal (Variance; Var(\\(X\\))) In this section, we’re focusing on the expectation. The expectation of a discrete random variable is the average: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\sum_{i = 1}^{k}x_i P(X = x_i) \\\\ =&amp; \\sum_{i = 1}^{k}x_i f_X(x_i) \\\\ \\end{aligned} \\] If \\(Y\\) represents a six-sided die, then: \\[ \\begin{aligned} \\text{E}(Y) =&amp; \\sum_{i = 1}^{k}y_i f_Y(y_i) \\\\ =&amp; 1(1/6) + 2(1/6) + 3(1/6) + 4(1/6) + 5(1/6) + 6(1/6) \\\\ =&amp; 21/6 \\\\ =&amp; 7/2 \\\\ \\end{aligned} \\] If \\(X\\) is continuous: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\int_{- \\infty}^{\\infty} x f_X(x) dx \\\\ \\end{aligned} \\] Here we are integrating over the probability density function, rather than summing over the mass density function. 5.1.1 Weak law of large numbers The expectation is more like a long-term average, rather than an actual instance (7/2 is not a possible instance of a dice roll). \\(X_i\\) are i.i.d. Assume E(\\(X_1\\)) = E(\\(X_2\\)) = … = E(\\(X_n\\)) = \\(\\mu\\) Define \\(\\overline{X}_n\\) as the mean of the observations: \\[ \\begin{aligned} \\overline{X}_n = \\frac{1}{n} (X_1 + X_2 + X_3 + ... + X_n) \\end{aligned} \\] As \\(n \\rightarrow \\infty\\), \\(\\overline{X}_n\\) “converges in probability” to \\(\\mu\\). This means for any positive constant \\(\\delta\\), \\[ \\begin{aligned} lim_{n \\rightarrow \\infty} \\text{P}(|\\overline{X}_n - \\mu| &gt; \\delta) = 0 \\end{aligned} \\] 5.1.2 Handy facts about expectations The expectation of a constant times a random variable is the constant times the expectation of the random variable: \\[ \\begin{aligned} \\text{E}(aX) = a \\text{E}(X) \\end{aligned} \\] The expectation of a constant is the constant: \\[ \\begin{aligned} \\text{E}(c) = c \\end{aligned} \\] The expectation of a sum of random variables is the sum of the expectations of those random variables: \\[ \\begin{aligned} \\text{E}(X + Y) = \\text{E}(X) + \\text{E}(Y) \\end{aligned} \\] Putting all these facts together, we can calculate the expectation of two random variables \\(X\\) and \\(Y\\) as: \\[ \\begin{aligned} \\text{E}(aX + bY + c) = a \\text{E}(X) + b \\text{E}(Y) + c \\end{aligned} \\] This is called the linearity of expectation, which we will use frequently in the exercises. Linearity does not hold for other measures of location (e.g., median, mode). This fact accounts, in part, for the privileged status of the mean in statistics. To calculate the expectation of a function: \\[ \\begin{aligned} \\text{E}[g(X)] = \\sum_{i = 1}^{k} g(x_i)f_X(x_i) \\end{aligned} \\] \\[ \\begin{aligned} \\text{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x)f_X(x)dx \\end{aligned} \\] 5.1.3 Exercise set 5-1 1a. Expected value of a Bernoulli random variable with parameter p? \\[ \\begin{aligned} f_X(x) = \\text{P}(X = x) = p^x(1 - p)^{1-x} \\text{ for } x \\in \\text{{0, 1}} \\\\ \\end{aligned} \\] Because there are only two outcomes (0 or 1), we can compute the expectation directly: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\sum_0^1 x f_X(x) \\\\ =&amp; \\sum_0^1 x p^x(1 - p)^{1-x} \\\\ =&amp; 0 p^0(1 - p)^{1-0} + 1 p^1(1 - p)^{1-1} \\\\ =&amp; 0 p^0(1 - p)^{1} + 1 p^1(1 - p)^{0} \\\\ =&amp; 0 (1) (1-p) + p(1) \\\\ =&amp; 0 + p \\\\ =&amp; p \\end{aligned} \\] 1b. What is the expected value of a binomial random variable with parameters \\(n\\) and \\(p\\)? Here’s the pmf for the binomial distribution: \\[ \\begin{aligned} f_X(x) = \\text{P}(X = x) = \\binom{n}{x} p^x(1 - p)^{n - x} \\text{ for } x \\in \\text{{0, 1, 2, ..., n}} \\\\ \\end{aligned} \\] If we plug that into the equation for E(\\(X\\)), we get: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\sum_0^n x f_X(x) \\\\ =&amp; \\sum_0^n x \\binom{n}{x} p^x(1 - p)^{n - x} \\\\ \\end{aligned} \\] Well, I don’t know how to evaluate this sum directly, considering the upper limit of \\(n\\) is infinite. So we’ll use the fact that the binomial is the sum of \\(n\\) independent Bernoulli trials (\\(X_i\\)). \\[ \\begin{aligned} \\text{E}(X) =&amp; \\text{E}(\\sum_{i=1}^nX_i) \\end{aligned} \\] Because the expectation is linear, the expectation of the sum is the sum of the expectations; we can rearrange: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\sum_{i=1}^n \\text{E}(X_i) \\end{aligned} \\] From 1a, we can substitute \\(p\\) for \\(\\text{E}(X_i)\\): \\[ \\begin{aligned} \\text{E}(X) =&amp; \\sum_{i=1}^n p \\\\ =&amp; np \\end{aligned} \\] 1c. What is the expected value of a discrete uniform random variable with parameters \\(a\\) and \\(b\\)? The probability mass function is: \\[ \\begin{aligned} \\text{P}(X = k) =&amp; \\frac{1}{b - a + 1} \\\\ \\end{aligned} \\] The expectation is: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\sum_{x = a}^b x f_X(x) \\\\ =&amp; \\sum_{x = a}^b x \\frac{1}{b - a + 1} \\\\ =&amp; \\frac{1}{b - a + 1} \\sum_{x = a}^b x \\\\ \\end{aligned} \\] We were given a hint that is useful now: for integers \\(a\\) and \\(b\\) with \\(b &gt; a\\), the sum of all the integers including \\(a\\) and \\(b\\), is: \\[ \\begin{aligned} \\sum_{k = a}^b k =&amp; \\frac{(a + b)(b - a + 1)}{2} \\\\ \\end{aligned} \\] So, plugging that hint in we get: \\[ \\begin{aligned} =&amp; \\frac{1}{b - a + 1} \\times \\frac{(a + b)(b - a + 1)}{2} \\\\ =&amp; \\frac{a + b}{2} \\\\ \\end{aligned} \\] 1d. What is the expected value of a continuous uniform random variable with parameters \\(a\\) and \\(b\\)? The probability density function is: \\[ \\begin{aligned} \\text{P}(X) =&amp; \\frac{1}{b - a} \\\\ \\end{aligned} \\] The expectation is: \\[ \\begin{aligned} \\text{E}(X) =&amp; \\int_{a}^b x f_X(x) dx \\\\ =&amp; \\int_{a}^b x \\frac{1}{b - a} dx \\\\ =&amp; \\frac{1}{b - a} \\int_{a}^b x dx \\\\ \\end{aligned} \\] Now we have to integrate the 2nd term: \\[ \\begin{aligned} =&amp; \\frac{1}{b - a} \\times (\\frac{b^2}{2} - \\frac{a^2}{2}) \\\\ =&amp; \\frac{1}{b - a} \\times (\\frac{b^2 - a^2}{2}) \\\\ \\end{aligned} \\] We use the hint from earlier, that \\(b^2 - a^2 = (b-a)(b+a)\\): \\[ \\begin{aligned} =&amp; \\frac{1}{b - a} \\times (\\frac{(b-a)(b+a)}{2}) \\\\ =&amp; \\frac{a + b}{2} \\\\ \\end{aligned} \\] Exploring the law of large numbers by simulation. In Edge’s code block below, samp.size represents \\(n\\) in the weak law of large numbers (above); n.samps represents independent random variables \\(X_n\\). The expectation for all \\(X_i\\) is \\(\\mu\\). samp.size &lt;- 20 n.samps &lt;- 1000 samps &lt;- rnorm(samp.size * n.samps, mean = 0, sd = 1) # Each column represents a random variable, X_i # Each row represents a sample (instance) drawn from X_i samp.mat &lt;- matrix(samps, ncol = n.samps) str(samp.mat) ## num [1:20, 1:1000] 0.539 0.339 -1.548 0.287 0.536 ... # Here we calculate the sample mean for each X_i (column) samp.means &lt;- colMeans(samp.mat) str(samp.means) ## num [1:1000] 0.171 -0.598 -0.318 0.518 0.286 ... hist(samp.means) 2a. What happens if we change samp.size (i.e., \\(n\\))? n_vector &lt;- c(1, 5, 20, 50, 100, 1000) samp_means_mat &lt;- matrix(data = NA, nrow = n.samps, ncol = length(n_vector)) calculate_sample_means &lt;- function(samp.size = 20, n.samps = 1000){ samps &lt;- rnorm(samp.size * n.samps, mean = 0, sd = 1) samp.mat &lt;- matrix(samps, ncol = n.samps) samp.means &lt;- colMeans(samp.mat) return(samp.means) } par(mfrow = c(2,3)) set.seed(21) for(i in 1:length(n_vector)){ samp_size_i &lt;- n_vector[i] samp_means_i &lt;- calculate_sample_means(samp.size = samp_size_i) hist(samp_means_i, xlim = c(-3, 3), ylim = c(0, 250), xlab = &quot;Sample mean&quot;, main = paste(&quot;n = &quot;, samp_size_i, sep = &quot;&quot;), col = &quot;red&quot;) } 2b. Using the exponential distribution. n_vector &lt;- c(1, 5, 20, 50, 100, 1000) samp_means_mat &lt;- matrix(data = NA, nrow = n.samps, ncol = length(n_vector)) calculate_sample_means_exp &lt;- function(samp.size = 20, n.samps = 1000){ samps &lt;- rexp(samp.size * n.samps, rate = 1) samp.mat &lt;- matrix(samps, ncol = n.samps) samp.means &lt;- colMeans(samp.mat) return(samp.means) } par(mfrow = c(2,3)) set.seed(21) for(i in 1:length(n_vector)){ samp_size_i &lt;- n_vector[i] samp_means_i &lt;- calculate_sample_means_exp(samp.size = samp_size_i) hist(samp_means_i, xlab = &quot;Sample mean&quot;, main = paste(&quot;n = &quot;, samp_size_i, sep = &quot;&quot;), col = &quot;red&quot;) } 5.2 Variance and standard deviation 5.3 Joint distributions, covariance, and correlation 5.4 Conditional distribution, expectation, variance 5.5 The central limit theorem 5.5.1 Exercise set 5-4 Bean machine in action! library(animation) nball &lt;- 500 #change the number of balls nlayer &lt;- 10 #change the number of rows of pegs on the board rate &lt;- 10 #change the speed at which the balls fall ani.options(nmax = nball + nlayer - 2, interval = 1/rate) quincunx(balls = nball, layers = nlayer) Exploring the beta distribution To see what the beta distribution looks like for a given set of shape parameters, set the sample size to 1. For example: library(stfspack) # dosm.beta.hist(n = 1, nsim = 10000, shape1 = 1, shape2 = 1) will give you a histogram of 10,000 observations from a beta distribution with parameters 1 and 1. If you increase the sample size, then the distribution of the sample mean gets closer to normality. Try this — starting with samples of size 1 and increasing the sample size — with the following sets of parameter values: (1, 1), (0.2, 0.2), (2, 0.5), (0.5, 2), (3, 3). Feel free to try other parameter sets — it’s fun. What do you notice? sims &lt;- 1000 s1 &lt;- 0.2 # change this s2 &lt;- 0.2 # change this par(mfrow = c(2,3)) dosm.beta.hist(n = 1, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.4976551 0.4193736 0.1758742 dosm.beta.hist(n = 4, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.51873441 0.21324943 0.04547532 dosm.beta.hist(n = 8, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.50331631 0.14985449 0.02245637 dosm.beta.hist(n = 16, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.49738079 0.10502289 0.01102981 dosm.beta.hist(n = 32, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.500954728 0.075780685 0.005742712 dosm.beta.hist(n = 64, nsim = sims, shape1 = s1, shape2 = s2) ## mean of DOSM SD of DOSM var of DOSM ## 0.500007026 0.053298265 0.002840705 Let’s deconstruct what is going on with this function, where n = 1 (we simulate 10000 observations from a single set of parameter values). dosm.beta.hist ## function (n, nsim, shape1 = 1, shape2 = 1, ...) ## { ## samps &lt;- rbeta(n * nsim, shape1, shape2) ## sim.mat &lt;- matrix(samps, nrow = nsim) ## dosm &lt;- rowMeans(sim.mat) ## hist(dosm, freq = FALSE, ...) ## x &lt;- seq(0, 1, length.out = 1000) ## lines(x, dnorm(x, mean = mean(dosm), sd = sd(dosm))) ## c(`mean of DOSM` = mean(dosm), `SD of DOSM` = sd(dosm), `var of DOSM` = var(dosm)) ## } ## &lt;bytecode: 0x7ff300ba32b0&gt; ## &lt;environment: namespace:stfspack&gt; nsim &lt;- 10000 n &lt;- 1 s1 &lt;- 0.2 # change this s2 &lt;- 0.2 # change this samps &lt;- rbeta(n * nsim, shape1 = s1, shape2 = s2) str(samps) # here are 10,000 ## num [1:10000] 0.13472 0.70805 0.99361 0.00268 0.58459 ... # We are converting the vector into a matrix # So that we can easily calculate the mean of each row sim.mat &lt;- matrix(samps, nrow = nsim) dim(sim.mat) ## [1] 10000 1 head(sim.mat) ## [,1] ## [1,] 0.13471512 ## [2,] 0.70804823 ## [3,] 0.99361002 ## [4,] 0.00268066 ## [5,] 0.58459079 ## [6,] 0.99641568 # Calculate rowmeans - with n=1, this doesn&#39;t change anything # But change n to anything bigger and inspect the dimensions of the objects dosm &lt;- rowMeans(sim.mat) str(dosm) ## num [1:10000] 0.13472 0.70805 0.99361 0.00268 0.58459 ... head(dosm) # compare these values to sim.mat ## [1] 0.13471512 0.70804823 0.99361002 0.00268066 0.58459079 0.99641568 par(mfrow = c(1,1)) hist(dosm, freq = FALSE) # plotting the simulated values # Set up a vector that goes from 0 to 1 to overlay a normal distribution on the histogram x &lt;- seq(0, 1, length.out = 1000) # Now plot a normal distribution, using the mean and sd of the simulated values lines(x, dnorm(x, mean = mean(dosm), sd = sd(dosm)), col = &quot;red&quot;) The Pareto distribution is a skewed, heavy-tailed, power-law distribution used in description of social, scientific, geophysical, actuarial, and many other types of observable phenomena. It was applied originally to the distribution of wealth in a society, fitting the observation that a large portion of wealth is held by a small fraction of the population. Named after the Italian civil engineer, economist, and sociologist Vilfredo Pareto. Parameters of the rpareto function: a: shape (on the web as \\(\\alpha\\)) b: scale (on the web as \\(x_m\\)) If the shape parameter is \\(\\leq\\) 1, \\(E(X)\\) is \\(\\infty\\). If the shape parameter is \\(\\leq\\) 2, \\(Var(X)\\) is \\(\\infty\\). First we simulate many sampes of size 1000 from a Pareto distribution with shape = 4. # experiment with n and the parameters a and b n &lt;- 100 n_sims &lt;- 10000 a &lt;- 1 b &lt;- 4 x &lt;- rpareto(n = n, a = a, b = b) summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.076 6.120 9.899 29.924 20.622 928.145 # Calculate mean and sd mu &lt;- mean(x) stdev &lt;- sd(x) hist(x, freq = FALSE) # Set up a vector that goes from 0 to 1 to overlay a normal distribution on the histogram x_vals &lt;- seq(min(x), max(x), length.out = 1000) # Now plot a normal distribution, using the mean and sd of the simulated values lines(x_vals, dnorm(x_vals, mean = mu, sd = stdev), col = &quot;red&quot;) # Compare tail to normal compare.tail.to.normal ## function (x, k, mu, sigma) ## { ## mean(x &lt; (mu - k * sigma) | x &gt; (mu + k * sigma))/(1 - (pnorm(k) - ## pnorm(-k))) ## } ## &lt;bytecode: 0x7ff3005eb440&gt; ## &lt;environment: namespace:stfspack&gt; k &lt;- 2 # sds compare.tail.to.normal(x = x, k = k, mu = mu, sigma = stdev) ## [1] 0.2197789 summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.076 6.120 9.899 29.924 20.622 928.145 mu ## [1] 29.92446 stdev ## [1] 95.49789 # This gives the value of the mean, minus the value k*stdev # (i.e., an extreme negative value) # Below I will use my object stdev in place of sigma (the parameter from Edge&#39;s function) (mu - k * stdev) ## [1] -161.0713 # Extreme positive value (mu + k * stdev) ## [1] 220.9202 # This statement asks whether the value in x is an extreme value # The operator &#39;|&#39; is &#39;OR&#39; # Is x extreme negative OR extreme positive? x &lt; (mu - k * stdev) | x &gt; (mu + k * stdev) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [61] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [97] FALSE FALSE FALSE FALSE # We can get the frequencies of this logical vector using table table(x &lt; (mu - k * stdev) | x &gt; (mu + k * stdev)) ## ## FALSE TRUE ## 99 1 # Or, as Edge, does, calculate the average of TRUEs - which is simply the proportion of TRUEs mean(x &lt; (mu - k * stdev) | x &gt; (mu + k * stdev)) ## [1] 0.01 # What proportion/probability of TRUEs would we expect under a normal probability distribution? pnorm(k) # probability of observing a value less than k standard deviations above the mean ## [1] 0.9772499 pnorm(-k) # probability of observing a value less than k standard deviations below the mean ## [1] 0.02275013 (1 - (pnorm(k) - pnorm(-k))) # probability of observing an extreme value ## [1] 0.04550026 # So putting it all together, we have the ratio of: # the probability of observing an extreme value in the data, over the # the probability of observing an extreme value in a normal distribution: mean(x &lt; (mu - k * stdev) | x &gt; (mu + k * stdev))/(1 - (pnorm(k) - pnorm(-k))) ## [1] 0.2197789 compare.tail.to.normal(x = x, k = k, mu = mu, sigma = stdev) ## [1] 0.2197789 # If this ratio is &lt; 1, then the data have fewer extreme values than suggested by a normal # If this ratio is &gt; 1, then the data have more extreme values than suggested by a normal Above, I haven’t computed the means of many simulations - which is the crux of the question! So here I just paste Edge’s solution. In it, he calculates \\(E(X)\\) and \\(Var(X)\\) using the Pareto probability distribution. I have changed n and n.sim to match my values above. #Sample size per simulation (n) and number of simulations. n &lt;- 100 n.sim &lt;- 10000 #Pareto parameters. Variance is finite, and so #CLT applies, if a &gt; 2. For large a, convergence to #normal is better. With small a, convergence is slow, #especially in the tails. a &lt;- 4 b &lt;- 1 #Compute the expectation and variance of the distribution #of the sample mean. a must be above 2 for these expressions #to hold. expec.par &lt;- a*b/(a-1) var.par &lt;- a*b^2 / ((a-1)^2 * (a-2)) sd.mean &lt;- sqrt(var.par / n) #Simulate data sim &lt;- matrix(rpareto(n*n.sim, a, b), nrow = n.sim) # Each column represents ith sample taken per simulation # Each row represents a different simulation sim[1:3, 1:10] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 1.146496 1.029705 1.334946 1.060409 1.013868 1.038984 1.804409 1.299151 ## [2,] 1.342433 1.205436 1.003869 3.416952 2.248103 1.023516 1.006624 1.044363 ## [3,] 1.076866 1.027499 1.061844 1.071026 1.425200 1.069973 1.456830 1.757125 ## [,9] [,10] ## [1,] 1.153548 1.276303 ## [2,] 1.074918 1.304583 ## [3,] 1.042708 1.099849 # Compute sample means. means.sim &lt;- rowMeans(sim) str(means.sim) ## num [1:10000] 1.38 1.34 1.33 1.28 1.34 ... #Draw a histogram of the sample means along with the approximate #normal pdf that follows from the CLT. hist(means.sim, prob = TRUE) curve(dnorm(x, expec.par, sd.mean), add = TRUE, col = &#39;red&#39;) compare.tail.to.normal(means.sim, 1/2, expec.par, sd.mean) ## [1] 0.9639022 compare.tail.to.normal(means.sim, 1, expec.par, sd.mean) ## [1] 0.9407189 compare.tail.to.normal(means.sim, 2, expec.par, sd.mean) ## [1] 0.9384561 compare.tail.to.normal(means.sim, 3, expec.par, sd.mean) ## [1] 2.18535 compare.tail.to.normal(means.sim, 4, expec.par, sd.mean) ## [1] 25.25951 compare.tail.to.normal(means.sim, 5, expec.par, sd.mean) ## [1] 348.8556 compare.tail.to.normal(means.sim, 6, expec.par, sd.mean) ## [1] 0 5.6 A probabilistic model for simple linear regression 5.6.1 Exercise set 5-5 Write the square of the correlation coefficient (eq. 5.30) in terms of the variance of Y (eq. 5.32) and the conditional variance of Y given X (eq. 5.31). \\[ \\begin{aligned} \\text{eq. 5.30: } \\rho_{X,Y} = \\beta \\frac{\\sigma_X}{\\sigma_Y} \\\\ \\text{eq. 5.31: } Var(Y) = \\beta^2 \\sigma_X^2 + \\sigma_{\\epsilon}^2 \\\\ \\text{eq. 5.32: } Var(Y \\mid X = x) = \\sigma_{\\epsilon}^2 \\\\ \\end{aligned} \\] Squaring \\(\\rho_{X,Y}\\), and expressing \\(Var(Y)\\) using the definition from above: \\[ \\begin{aligned} \\rho_{X,Y}^2 = \\beta^2 \\frac{\\sigma_X^2}{\\sigma_Y^2} = \\beta^2 \\frac{\\sigma_X^2}{Var(Y)} \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\rho_{X,Y}^2 = \\beta^2 \\frac{\\sigma_X^2}{\\beta^2 \\sigma_X^2 + \\sigma_{\\epsilon}^2} \\\\ \\end{aligned} \\] Some algebra… \\[ \\begin{aligned} \\rho_{X,Y}^2 = 1 - \\frac{\\sigma_{\\epsilon}^2}{\\beta^2 \\sigma_X^2 + \\sigma_{\\epsilon}^2} \\\\ \\end{aligned} \\] And we use the formulas from above again to restate as: \\[ \\begin{aligned} \\rho_{X,Y}^2 = 1 - \\frac{Var(Y \\mid X = x)}{Var(Y)} \\\\ \\end{aligned} \\] which gives us the ‘proportion of variance explained’. So if there isn’t much variance left in \\(Y\\) after conditioning on \\(X\\) (i.e., the numerator is small relative to the denominator), if we subtract it from 1, we get a high \\(r^2\\). And vice versa. Simulating a regression. library(stfspack) sim.lm ## function (n, a, b, sigma.disturb = 1, mu.x = 8, sigma.x = 2, ## rdisturb = rnorm, rx = rnorm, het.coef = 0) ## { ## x &lt;- sort(rx(n, mu.x, sigma.x)) ## disturbs &lt;- rdisturb(n, 0, sapply(sigma.disturb + scale(x) * ## het.coef, max, 0)) ## y &lt;- a + b * x + disturbs ## cbind(x, y) ## } ## &lt;bytecode: 0x7ff3044dddd8&gt; ## &lt;environment: namespace:stfspack&gt; sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1) head(sim_0_1) ## x y ## [1,] 3.427344 2.949380 ## [2,] 3.616894 3.644965 ## [3,] 3.815677 4.193751 ## [4,] 3.912910 3.364430 ## [5,] 4.458928 6.025582 ## [6,] 5.078764 3.985045 plot(sim_0_1[,1], sim_0_1[,2]) Still using all the default values for parameters: sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1, sigma.disturb = 1, mu.x = 8, sigma.x = 2, rdisturb = rnorm, rx = rnorm, het.coef = 0) plot(sim_0_1[,1], sim_0_1[,2]) Now I’ll change one at a time: sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1, sigma.disturb = 2, mu.x = 8, sigma.x = 2, rdisturb = rnorm, rx = rnorm, het.coef = 0) plot(sim_0_1[,1], sim_0_1[,2]) sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1, sigma.disturb = 1, mu.x = 16, sigma.x = 2, rdisturb = rnorm, rx = rnorm, het.coef = 0) plot(sim_0_1[,1], sim_0_1[,2]) sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1, sigma.disturb = 1, mu.x = 8, sigma.x = 4, rdisturb = rnorm, rx = rnorm, het.coef = 0) plot(sim_0_1[,1], sim_0_1[,2]) sim_0_1 &lt;- sim.lm(n = 50, a = 0, b = 1, sigma.disturb = 1, mu.x = 8, sigma.x = 2, rdisturb = rlaplace, rx = rnorm, het.coef = 0) plot(sim_0_1[,1], sim_0_1[,2]) "]
]
