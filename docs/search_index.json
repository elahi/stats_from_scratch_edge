[
["semiparametric.html", "Chapter 8 Semiparametric estimation and inference 8.1 Semiparametric point estimation using the method of moments 8.2 Semiparametric interval estimation using the bootstrap 8.3 Semiparametric hypothesis testing using permutation tests 8.4 Chapter summary", " Chapter 8 Semiparametric estimation and inference Parametric: governed by parameters; e.g., normal distributions Nonparametric: not governed by parameters; used when we cannot assume that the distribution of a random variable follows a particular probability distribution Semiparametric: a model whose behavior is partially governed by parameters Empirical distribution function: summarizes all the information that the data provide about a random variable’s distribution The empirical distribution function \\(\\hat{F}_n(z)\\) gives the proportion of observations in a sample that are less than or equal to a constant \\(z\\): \\[ \\begin{aligned} \\hat{F}_n(z) =&amp; \\frac{1}{n} \\sum_{i = 1}^n I_{X_i \\leq z} \\end{aligned} \\] where \\(I\\) is an indicator variable that states whether observation \\(X_i\\) is less than or equal to \\(z\\). 8.0.1 Exercise set 8-1 Comparing an ECDF with a CDF for the normal distribution As the sample size increases, the ECDF approximates the CDF of the normal distribution. n &lt;- 500 x.vals &lt;- seq(-3, 3, length.out = 10000) Fx &lt;- pnorm(x.vals, 0, 1) plot(x.vals, Fx, xlab = &quot;z&quot;, ylab = &quot;F(z)&quot;, type = &quot;l&quot;) x &lt;- rnorm(n, 0, 1) lines(ecdf(x), verticals = TRUE, do.points = FALSE, lty = 2) Comparing an ECDF with a CDF for the Poisson distribution set.seed(22) n &lt;- 30 lambda &lt;- 10 x.vals &lt;- seq(0, 30, by = 1) Fx &lt;- ppois(x.vals, lambda = lambda) plot(x.vals, Fx, xlab = &quot;z&quot;, ylab = &quot;F(z)&quot;, type = &quot;p&quot;) x &lt;- rpois(n, lambda = lambda) lines(ecdf(x), verticals = TRUE, do.points = FALSE, lty = 2) See handwritten notes. 8.1 Semiparametric point estimation using the method of moments 8.1.1 Introduction to moments Methods of moments: a semiparametric approach to estimation Method of maximum likelihood: a parametric approach to estimation Suppose \\(X\\) is a random variable. The \\(j\\)th moment of the distribution is: \\[ \\begin{aligned} \\mu_j =&amp; E(X^j) \\end{aligned} \\] where \\(\\mu\\) is used as the symbol for a moment. For example \\(\\mu_1 = E(X)\\). It is easy to express the first moment of \\(X\\) as an expression using \\(X\\) - but what about \\(\\mu_2\\)? \\[ \\begin{aligned} \\mu_2 =&amp; E(X^2) \\end{aligned} \\] Now we have expressed the second moment, \\(\\mu_2\\), as the expectation of a new random variable, \\(X^2\\). At least this is how I interpret it. From my reading, I gather this is not so satisfying, because the second moment is then usually rearranged in the following way. First, we need to recall the definition of Var(\\(X\\)): \\[ \\begin{aligned} \\text{Var}(X) = \\text{E}(X^2) - [\\text{E}(X)]^2 \\\\ \\end{aligned} \\] So, if we rearrange we get: \\[ \\begin{aligned} \\text{E}(X^2) = \\text{Var}(X) + [\\text{E}(X)]^2 \\\\ \\end{aligned} \\] We can continue on with 3rd, 4th, 5th, etc, moments. But expressing these in terms of \\(X\\) is, I am guessing, a harder challenge. These additional moments describe further the probability distribution, but typically knowing the first and second moments is enough for practical purposes (our purposes). As an aside, the third moment gives us the skewness, and the fourth moment gives us the kurtosis, of the distribution. Finally, it is worth noting the notation for a moment - here I have used \\(\\mu\\), becuase it is often used in statistics notes (that I found online). This can be confusing, because we often use \\(\\mu\\) for the population mean. But we won’t do that here, for clarity. So, let us say that \\(X\\) follows a Normal(\\(\\theta\\), \\(\\sigma^2\\)) distribution. In this case, we know that E(\\(X\\)) = \\(\\theta\\), and that Var(\\(X\\)) = \\(\\sigma^2\\). So we can express the first and second moments as: \\[ \\begin{aligned} \\mu_1 =&amp; ~ \\text{E}(X) \\\\ =&amp; ~ \\theta \\\\ \\mu_2 =&amp; ~ \\text{E}(X^2) \\\\ =&amp; \\text{Var}(X) + [\\text{E}(X)]^2 \\\\ =&amp; ~ \\sigma^2 + \\theta^2 \\\\ \\end{aligned} \\] So we have expressed the moments in terms of the parameters of the distribution we are trying estimate. We can rearrange these, to express the parameters in terms of \\(X\\) - which is exactly what we want to do, since we are trying to estimate \\(\\theta\\) and \\(\\sigma^2\\)! \\[ \\begin{aligned} \\theta =&amp; ~ \\text{E}(X) \\\\ \\sigma^2 =&amp; ~ \\text{E}(X^2) - [\\text{E}(X)]^2 \\\\ \\end{aligned} \\] To paraphrase Edge, we can estimate the moments of arbitrary distributions (i.e., distributions that are not described by parameters) by following these steps: Write the equations that give the moments of a random variable in terms of the parameters being estimated Solve the equations from (1) for the desired parameters, giving expressions for the parameters in terms of the moments Estimate the moments and plug the estimates into the expressions for the parameters from (2) 8.1.2 Plug-in estimators Plug-in estimation: perhaps I’m oversimplifying here, but this sounds like awesome jargon for ‘calculate your estimator from data’. In other words, if you want the population mean - collect some data and calculate the mean. Then ‘plug that in’ to your population mean. We can express a plug-in estimator of the \\(k\\)th moment of \\(X\\), E(\\(X^j\\)) as the \\(k\\)th sample moment: \\[ \\begin{aligned} \\overline{X^k} = \\frac{1}{n} \\sum_{i = 1}^n X_i^k \\end{aligned} \\] So to recap: a moment describes the random variable \\(X\\). A sample moment is an estimate of the moment using independent samples (\\(X_1, X_2, X_3\\)) drawn from \\(X\\). 8.1.3 Exercise set 8-2 Supose \\(X_1, X_2, ..., X_n\\) are I.I.D observations. What is the plug-in estimator of the variance of \\(X\\)? \\[ \\begin{aligned} \\sigma^2 =&amp; ~ \\text{E}(X^2) - [\\text{E}(X)]^2 \\\\ \\hat\\sigma^2 =&amp; ~ \\frac{1}{n} \\sum_{i = 1}^n X_i^2 - [\\frac{1}{n} \\sum_{i = 1}^n X_i]^2 \\\\ \\end{aligned} \\] b. What is the plug-in estimator for the standard deviation of \\(X\\)? \\[ \\begin{aligned} \\hat\\sigma =&amp; ~ \\sqrt{\\frac{1}{n} \\sum_{i = 1}^n X_i^2 - [\\frac{1}{n} \\sum_{i = 1}^n X_i]^2} \\\\ \\end{aligned} \\] What is the plug-in estimator of the covariance of \\(X\\) and \\(Y\\)? Here is the formula for the covariance, as a reminder: \\[ \\begin{aligned} \\text{Cov}(X,Y) =&amp; ~ \\text{E}([X - \\text{E}(X)][Y - \\text{E}(Y)]) \\\\ =&amp; ~ \\text{E}(XY) - \\text{E}(X)\\text{E}(Y) \\end{aligned} \\] Let’s use the 2nd equation: \\[ \\begin{aligned} \\text{Cov}(X,Y) =&amp; ~ \\frac{1}{n} \\sum_{i = 1}^n X_iY_i - [\\frac{1}{n} \\sum_{i = 1}^n X_i][\\frac{1}{n} \\sum_{i = 1}^n Y_i] \\\\ \\end{aligned} \\] What is the plug-in estimator of the correlation of \\(X\\) and \\(Y\\)? Here is the formula for the correlation, as a reminder: \\[ \\begin{aligned} \\text{Cor}(X,Y) =&amp; ~ \\rho_{X,Y} \\\\ =&amp; ~ \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}} \\\\ =&amp; ~ \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\\\\ \\end{aligned} \\] Plugging in, we get: \\[ \\begin{aligned} \\rho_{X,Y} =&amp; ~ \\frac {\\frac{1}{n} \\sum_{i = 1}^n X_iY_i - [\\frac{1}{n} \\sum_{i = 1}^n X_i][\\frac{1}{n} \\sum_{i = 1}^n Y_i]} {\\sqrt{ \\frac{1}{n} \\sum_{i = 1}^n X_i^2 - [\\frac{1}{n} \\sum_{i = 1}^n X_i]^2}] [\\frac{1}{n} \\sum_{i = 1}^n Y_i^2] } \\\\ \\end{aligned} \\] Though the plug-in estimator of the variance is consistent, it is biased downward. Demonstrate the downward bias in R with simulations. This is what I did first: n_obs &lt;- 5 n_sims &lt;- 10000 set.seed(1010) x &lt;- mat.samps(n = n_obs, nsim = n_sims) x_var &lt;- apply(X = x, MARGIN = 1, FUN = var) mean(x_var) ## [1] 1.004699 The result is very close to 1. This surprised me, given that I was expecting something below 1. So I ran Edge’s code, and in it, he defines a function to calculate the variance himself. var.pi &lt;- function(vec){ return(sum((vec-mean(vec))^2)/length(vec)) } vars.pi &lt;- apply(x, 1, var.pi) mean(vars.pi) ## [1] 0.8037593 Why the difference? Turns out this is a great demonstration of something I had read some time ago, and then buried away because it frankly was not super relevant. Let’s inspect the help function for var: ?var And in the details you’ll see this note: The denominator n - 1 is used which gives an unbiased estimator of the (co)variance for i.i.d. observations. These functions return NA when there is only one observation (whereas S-PLUS has been returning NaN), and fail if x has length zero. Thanks to this book, this statement make sense. Let’s calculate the variance using n and n-1 as the denominator: vec &lt;- x[1,] vec ## [1] 0.4974519 1.1826813 1.0876947 -1.3540474 -0.9988671 # Default function to calculate the variance in R var(vec) ## [1] 1.406506 # Edge&#39;s function, with the denominator of n var.pi(vec) ## [1] 1.125205 # Beating a dead horse: sum((vec-mean(vec))^2) / (length(vec)) ## [1] 1.125205 # But now we change the denominator from n, to n-1 # Unbiased estimator for the variance # Gives the same result as `var` sum((vec-mean(vec))^2) / (length(vec) - 1) ## [1] 1.406506 I have not read ahead, but I suspect Edge will discuss this discrepancy soon.. Repeating the simulation, but with samples sizes from 2 to 10. n_sims &lt;- 10000 n_vec &lt;- 2:10 variance_vec &lt;- vector(length = length(n_vec)) for(i in 1:length(n_vec)){ n_obs &lt;- n_vec[i] x &lt;- mat.samps(n = n_obs, nsim = n_sims) vars.pi &lt;- apply(x, 1, var.pi) variance_vec[i] &lt;- mean(vars.pi) } variance_vec ## [1] 0.4879436 0.6578587 0.7378372 0.7852684 0.8369602 0.8502939 0.8715858 ## [8] 0.8900192 0.9012010 plot(n_vec, variance_vec, type = &quot;b&quot;, xlab = &quot;Number of samples&quot;, ylab = &quot;Mean variance&quot;, ylim = c(0, 1)) abline(h = 1, lty = 2 , col = &quot;red&quot;) The bias is largest with a sample size of 2, and the plug-in estimator gets closer to the true value with increasing sample size. I did not think to convert these decimal outputs to fractions as indicated in Edge’s solution, even having stumbled upon the unbiased estimator above. So this result is not yet intuitive: \\[ \\begin{aligned} \\text{E}(\\hat\\sigma^2_n) =&amp; ~ \\frac{n - 1}{n} \\sigma^2 \\\\ \\end{aligned} \\] This equation demonstrates the bias of the plug-in estimator for the variance. If we multiply it by \\(n/(n-1)\\), we get the sample variance (\\(s^2\\)), an unbiased estimator: \\[ \\begin{aligned} s^2 =&amp; ~ \\frac{\\sum_{i = 1}^n (X_i - \\overline X_i)^2}{n - 1} \\\\ \\end{aligned} \\] See Edge solution. It explains how we get the expectation of the plug-in estimator. Did not try. 8.1.4 The method of moments 8.1.5 Exercise set 8-3 8.2 Semiparametric interval estimation using the bootstrap 8.2.1 Exercise set 8-4 8.3 Semiparametric hypothesis testing using permutation tests 8.3.1 Exercise set 8-5 8.4 Chapter summary "]
]
