---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE)
```

# Properties of random variables {#randomvars}

```{r load-packages}
library(tidyverse)
theme_set(theme_bw(base_size = 12) + 
            theme(strip.background = element_blank(), 
                  panel.grid = element_blank())) 
```

## Expected values and the law of large numbers

When summarizing a probability distribution, it is useful to have a measure of:

  - Location (*Expectation*; E($X$))
  - Dispersal (*Variance*; Var($X$))

In this section, we're focusing on the expectation. 
  
The expectation of a discrete random variable is the average:

$$
\begin{aligned}
\text{E}(X) =& \sum_{i = 1}^{k}x_i P(X = x_i) \\
=& \sum_{i = 1}^{k}x_i f_X(x_i) \\
\end{aligned}
$$

If $Y$ represents a six-sided die, then:

$$
\begin{aligned}
\text{E}(Y) =& \sum_{i = 1}^{k}y_i f_Y(y_i) \\
=& 1(1/6) + 2(1/6) + 3(1/6) + 4(1/6) + 5(1/6) + 6(1/6) \\
=& 21/6 \\ 
=& 7/2 \\
\end{aligned}
$$

If $X$ is continuous:

$$
\begin{aligned}
\text{E}(X) =& \int_{- \infty}^{\infty} x f_X(x) dx \\
\end{aligned}
$$

Here we are integrating over the probability density function, rather than summing over the mass density function. 

### Weak law of large numbers

The expectation is more like a long-term average, rather than an actual instance (7/2 is not a possible instance of a dice roll). 

$X_i$ are i.i.d.

Assume E($X_1$) = E($X_2$) = ... = E($X_n$) = $\mu$

Define $\overline{X}_n$ as the mean of the observations:

$$
\begin{aligned}
\overline{X}_n = \frac{1}{n} (X_1 + X_2 + X_3 + ... + X_n)
\end{aligned}
$$

As $n \rightarrow \infty$, $\overline{X}_n$ "converges in probability" to $\mu$. This means for any positive constant $\delta$, 

$$
\begin{aligned}
lim_{n \rightarrow \infty} \text{P}(|\overline{X}_n - \mu| > \delta) = 0
\end{aligned}
$$

### Handy facts about expectations

The expectation of a constant times a random variable is the constant times the expectation of the random variable:

$$ 
\begin{aligned}
\text{E}(aX) = a \text{E}(X)
\end{aligned}
$$
The expectation of a constant is the constant:

$$ 
\begin{aligned}
\text{E}(c) = c
\end{aligned}
$$

The expectation of a sum of random variables is the sum of the expectations of those random variables: 

$$ 
\begin{aligned}
\text{E}(X + Y) = \text{E}(X) + \text{E}(Y)
\end{aligned}
$$

Putting all these facts together, we can calculate the expectation of two random variables $X$ and $Y$ as:

$$ 
\begin{aligned}
\text{E}(aX + bY + c) = a \text{E}(X) + b \text{E}(Y) + c
\end{aligned}
$$

This is called the **linearity of expectation**, which we will use frequently in the exercises. Linearity does not hold for other measures of location (e.g., median, mode). This fact accounts, in part, for the privileged status of the mean in statistics. 

To calculate the expectation of a function:

$$ 
\begin{aligned}
\text{E}[g(X)] = \sum_{i = 1}^{k} g(x_i)f_X(x_i)
\end{aligned}
$$

$$ 
\begin{aligned}
\text{E}[g(X)] = \int_{-\infty}^{\infty} g(x)f_X(x)dx
\end{aligned}
$$


### Exercise set 5-1

1a. Expected value of a Bernoulli random variable with parameter *p*?

$$ 
\begin{aligned}
f_X(x) = \text{P}(X = x) = p^x(1 - p)^{1-x} \text{ for } x \in \text{{0, 1}} \\
\end{aligned}
$$

Because there are only two outcomes (0 or 1), we can compute the expectation directly:

$$ 
\begin{aligned}
\text{E}(X) =& \sum_0^1 x f_X(x) \\
=& \sum_0^1 x p^x(1 - p)^{1-x} \\ 
=& 0 p^0(1 - p)^{1-0} + 1 p^1(1 - p)^{1-1} \\
=& 0 p^0(1 - p)^{1} + 1 p^1(1 - p)^{0} \\
=& 0 (1) (1-p) + p(1) \\ 
=& 0 + p \\
=& p
\end{aligned}
$$

1b. What is the expected value of a binomial random variable with parameters $n$ and $p$?

Here's the pmf for the binomial distribution:

$$ 
\begin{aligned}
f_X(x) = \text{P}(X = x) = \binom{n}{x} p^x(1 - p)^{n - x} \text{ for } x \in \text{{0, 1, 2, ..., n}} \\
\end{aligned}
$$

If we plug that into the equation for E($X$), we get:

$$ 
\begin{aligned}
\text{E}(X) =& \sum_0^n x f_X(x) \\
=& \sum_0^n x \binom{n}{x} p^x(1 - p)^{n - x} \\ 
\end{aligned}
$$
Well, I don't know how to evaluate this sum directly, considering the upper limit of $n$ is infinite. So we'll use the fact that the binomial is the sum of $n$ independent Bernoulli trials ($X_i$). 

$$ 
\begin{aligned}
\text{E}(X) =& \text{E}(\sum_{i=1}^nX_i)
\end{aligned}
$$

Because the expectation is linear, the expectation of the sum is the sum of the expectations; we can rearrange:

$$ 
\begin{aligned}
\text{E}(X) =& \sum_{i=1}^n \text{E}(X_i)
\end{aligned}
$$

From 1a, we can substitute $p$ for $\text{E}(X_i)$:

$$ 
\begin{aligned}
\text{E}(X) =& \sum_{i=1}^n p \\
=& np
\end{aligned}
$$

1c. What is the expected value of a discrete uniform random variable with parameters $a$ and $b$?

The probability mass function is:

$$ 
\begin{aligned}
\text{P}(X = k) =& \frac{1}{b - a + 1} \\
\end{aligned}
$$
The expectation is:

$$ 
\begin{aligned}
\text{E}(X) =& \sum_{x = a}^b x f_X(x) \\
=& \sum_{x = a}^b x \frac{1}{b - a + 1}  \\
=& \frac{1}{b - a + 1} \sum_{x = a}^b x \\
\end{aligned}
$$

We were given a hint that is useful now: for integers $a$ and $b$ with $b > a$, the sum of all the integers including $a$ and $b$, is:

$$ 
\begin{aligned}
\sum_{k = a}^b k  =& \frac{(a + b)(b - a + 1)}{2} \\
\end{aligned}
$$

So, plugging that hint in we get:

$$ 
\begin{aligned}
=& \frac{1}{b - a + 1} \times \frac{(a + b)(b - a + 1)}{2} \\
=& \frac{a + b}{2} \\
\end{aligned}
$$

1d. What is the expected value of a continuous uniform random variable with parameters $a$ and $b$?

The probability density function is:

$$ 
\begin{aligned}
\text{P}(X) =& \frac{1}{b - a} \\
\end{aligned}
$$

The expectation is:

$$ 
\begin{aligned}
\text{E}(X) =& \int_{a}^b x f_X(x) dx \\
=& \int_{a}^b x \frac{1}{b - a} dx  \\
=& \frac{1}{b - a} \int_{a}^b x  dx  \\
\end{aligned}
$$

Now we have to integrate the 2nd term:

$$ 
\begin{aligned}
=& \frac{1}{b - a} \times \frac{1}{2} x^2 \bigg\rvert_{a}^{b} \\
=& \frac{1}{b - a} \times (\frac{b^2}{2} - \frac{a^2}{2}) \\
=& \frac{1}{b - a} \times (\frac{b^2 - a^2}{2}) \\
\end{aligned}
$$

We use the hint from earlier, that $b^2 - a^2 = (b-a)(b+a)$:

$$ 
\begin{aligned}
=& \frac{1}{b - a} \times (\frac{(b-a)(b+a)}{2}) \\
=& \frac{a + b}{2} \\
\end{aligned}
$$


2. Exploring the law of large numbers by simulation. In Edge's code block below, `samp.size` represents $n$ in the weak law of large numbers (above); `n.samps` represents independent random variables $X_n$. The expectation for all $X_i$ is $\mu$. 
```{r}
samp.size <- 20
n.samps <- 1000
samps <- rnorm(samp.size * n.samps, mean = 0, sd = 1)
# Each column represents a random variable, X_i
# Each row represents a sample (instance) drawn from X_i
samp.mat <- matrix(samps, ncol = n.samps) 
str(samp.mat)
# Here we calculate the sample mean for each X_i (column)
samp.means <- colMeans(samp.mat)
str(samp.means)
hist(samp.means)
```

2a. What happens if we change `samp.size` (i.e., $n$)? 

```{r, fig.width = 7, fig.height = 7}
n_vector <- c(1, 5, 20, 50, 100, 1000)
samp_means_mat <- matrix(data = NA, nrow = n.samps, ncol = length(n_vector))

calculate_sample_means <- function(samp.size = 20, n.samps = 1000){
  samps <- rnorm(samp.size * n.samps, mean = 0, sd = 1)
  samp.mat <- matrix(samps, ncol = n.samps) 
  samp.means <- colMeans(samp.mat)
  return(samp.means)
}

par(mfrow = c(2,3))
set.seed(21)
for(i in 1:length(n_vector)){
  samp_size_i <- n_vector[i]
  samp_means_i <- calculate_sample_means(samp.size = samp_size_i)
  hist(samp_means_i, xlim = c(-3, 3), ylim = c(0, 250), 
       xlab = "Sample mean", 
       main = paste("n = ", samp_size_i, sep = ""), col = "red")
}
```

2b. Using the exponential distribution. 

```{r, fig.width = 7, fig.height = 7}
n_vector <- c(1, 5, 20, 50, 100, 1000)
samp_means_mat <- matrix(data = NA, nrow = n.samps, ncol = length(n_vector))

calculate_sample_means_exp <- function(samp.size = 20, n.samps = 1000){
  samps <- rexp(samp.size * n.samps, rate = 1)
  samp.mat <- matrix(samps, ncol = n.samps) 
  samp.means <- colMeans(samp.mat)
  return(samp.means)
}

par(mfrow = c(2,3))
set.seed(21)
for(i in 1:length(n_vector)){
  samp_size_i <- n_vector[i]
  samp_means_i <- calculate_sample_means_exp(samp.size = samp_size_i)
  hist(samp_means_i, 
       xlab = "Sample mean", 
       main = paste("n = ", samp_size_i, sep = ""), col = "red")
}
```

## Variance and standard deviation

The variance is a measurement of dispersal - i.e., how spread out is the distribution? And spread out from what, exactly? It is useful to think about the distance $X_i$ takes from the expectation, E$(X)$: $X - \text{E}(X)$. What if we took the expectation of this - i.e., the average value of the distance from the mean?

$$ 
\begin{aligned}
\text{E}(X - \text{E}(X)) \\
\text{by linearity of expectation, we get:} \\
\text{E}(X) - \text{E}(\text{E}(X)) \\
\text{E}(X) - \text{E}(X) \\
0
\end{aligned}
$$

This won't work - we need to find a way to constrain the expression inside the parentheses to be non-negative. One way to do this is to use the mean absolute deviation, $|X - \text{E}(X)|$. Another way is to use the mean squared deviation, $[X - \text{E}(X)]^2$. The squared term constrains the variance to be $\ge 0$:  

$$ 
\begin{aligned}
\text{Var}(X) =& \text{E}([X - \text{E}(X)]^2) \\
\end{aligned}
$$

The mean squared deviation has two mathematical advantages:

1. It is easier to compute mathematically than an analogous quanitity using absolute deviations (but why?)

2. The variances of linear functions of random variables are 'beautifully behaved', whereas the analogous quantities for absolute deviations can be a hassle. 

I will just take Edge's word on these two points for now. 

### Beautiful properties of the variance

The variance can be rewritten as:

$$ 
\begin{aligned}
\text{Var}(X) = \text{E}(X^2) - [\text{E}(X)]^2 \\
\end{aligned}
$$

which is generally easier to compute. 

Adding a constant to a random variable does *not* affect the variance:

$$ 
\begin{aligned}
\text{Var}(a + cX) = c^2\text{Var}(X) \\
\end{aligned}
$$
where $a$ and $c$ are constants. 

If $X$ and $Y$ are independent random variables, then: 

$$ 
\begin{aligned}
\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) \\
\end{aligned}
$$

One big problem with the variance is that is in the wrong ($X^2$) units. To fix this, we calculate the *standard deviation*: 

$$ 
\begin{aligned}
\text{SD}(X) = \sqrt{\text{Var}(X)} \\
\end{aligned}
$$

SD is usually larger (never smaller) than MAD, and is more sensitive to large deviations. 

### Exercise set 5-2

I had to walk through Edge's solutions bit by bit; my handwritten version is [here](images/edge_4_1_3.pdf). 

## Joint distributions, covariance, and correlation

This section covers four key concepts:

1. Joint probability distribution: the probability distribution of the joint occurrence of X and Y

2. Marginal distribution of X: the probability distribution of X, summing (integrating) over all values of Y

3. Covariance: a measurement of the extent to which X and Y depart from independence

4. Correlation: covariance rescaled to go from -1 to 1

I think Edge does a good job, so I am not going to spend the time recreating his equations here. I will add one problem, to reinforce the concepts of joint and marginal distributions, with two discrete random variables. This problem covers similar ideas to Edge's first exercise in set 5-3. 

### Additional exercise

You watched 100 female birds last spring, and recorded the number of offspring per bird (X; 1, 2, or 3 chicks). You also recorded the age of each mom (Y; 1, 2, or 3 years). 

You observed:
10 1-yr olds, all with one chick. 
27 2-yr olds; 13 had one chick, 12 had two chicks, and 2 had three chicks. 
63 3-yr olds; 23 had one chick, 36 had two chicks, and 4 had three chicks. 

Calculate:

1. The probability of observing each possible outcome (e.g., a 1-yr old bird has 1 chick; a 1-yr old bird has 2 chicks; etc.).

2. The probability of observing a 1-yr old bird; a 2-yr old bird; and a 3-yr old bird.

3. The probability of observing 1 chick per mom; 2 chicks per mom; 3 chicks per mom. 

STOP! NO PEEKING ! ANSWER IS BELOW:

Wait for it...

...wait for it ...

...here it is: an excel (gasp!) plot!

![](images/birbs_for_rmd.png){width=200%}

The key here is to recognize that yellow represents the joint probabilities of X and Y; the green and blue represents the marginal probabilities of X and Y, respectively. Stare at this until it clicks. A similar principle applies to continuous distributions, but rather than summing across Y, we integrate across Y to get the marginal distribution of X. 

### Exercise set 5-3

I had to walk through Edge's solutions bit by bit; my handwritten version is [here](images/edge_5_3.pdf). 

## Conditional distribution, expectation, variance

## The central limit theorem

### Exercise set 5-4

1. Bean machine in action!

```{r, eval = FALSE}
library(animation)
nball <- 500 #change the number of balls
nlayer <- 10 #change the number of rows of pegs on the board
rate <- 10 #change the speed at which the balls fall 
ani.options(nmax = nball + nlayer - 2, interval = 1/rate) 
quincunx(balls = nball, layers = nlayer)
```

2. Exploring the beta distribution

To see what the beta distribution looks like for a given set of shape parameters, set the sample size to 1. For example: 

```{r}
library(stfspack)
# dosm.beta.hist(n = 1, nsim = 10000, shape1 = 1, shape2 = 1)
```

will give you a histogram of 10,000 observations from a beta distribution with parameters 1 and 1. If you increase the sample size, then the distribution of the sample mean gets closer to normality. Try this — starting with samples of size 1 and increasing the sample size — with the following sets of parameter values: (1, 1), (0.2, 0.2), (2, 0.5), (0.5, 2), (3, 3). Feel free to try other parameter sets — it’s fun. What do you notice?

```{r, fig.height = 7, fig.width = 7}
sims <- 1000
s1 <- 0.2 # change this
s2 <- 0.2 # change this
par(mfrow = c(2,3))
dosm.beta.hist(n = 1, nsim = sims, shape1 = s1, shape2 = s2)
dosm.beta.hist(n = 4, nsim = sims, shape1 = s1, shape2 = s2)
dosm.beta.hist(n = 8, nsim = sims, shape1 = s1, shape2 = s2)
dosm.beta.hist(n = 16, nsim = sims, shape1 = s1, shape2 = s2)
dosm.beta.hist(n = 32, nsim = sims, shape1 = s1, shape2 = s2)
dosm.beta.hist(n = 64, nsim = sims, shape1 = s1, shape2 = s2)
```

Let's deconstruct what is going on with this function, where n = 1 (we simulate 10000 observations from a single set of parameter values). 

```{r}
dosm.beta.hist

nsim <- 10000
n <- 1
s1 <- 0.2 # change this
s2 <- 0.2 # change this
samps <- rbeta(n * nsim, shape1 = s1, shape2 = s2)
str(samps) # here are 10,000

# We are converting the vector into a matrix
# So that we can easily calculate the mean of each row
sim.mat <- matrix(samps, nrow = nsim)
dim(sim.mat)
head(sim.mat) 

# Calculate rowmeans - with n=1, this doesn't change anything
# But change n to anything bigger and inspect the dimensions of the objects
dosm <- rowMeans(sim.mat)
str(dosm)
head(dosm) # compare these values to sim.mat

par(mfrow = c(1,1))
hist(dosm, freq = FALSE) # plotting the simulated values
# Set up a vector that goes from 0 to 1 to overlay a normal distribution on the histogram
x <- seq(0, 1, length.out = 1000) 
# Now plot a normal distribution, using the mean and sd of the simulated values
lines(x, dnorm(x, mean = mean(dosm), sd = sd(dosm)), col = "red")
```

3. The Pareto distribution is a skewed, heavy-tailed, power-law distribution used in description of social, scientific, geophysical, actuarial, and many other types of observable phenomena. It was applied originally to the distribution of wealth in a society, fitting the observation that a large portion of wealth is held by a small fraction of the population. Named after the Italian civil engineer, economist, and sociologist Vilfredo Pareto. 

Parameters of the `rpareto` function:

  - a: shape (on the web as $\alpha$)
  - b: scale (on the web as $x_m$)
  
If the shape parameter is $\leq$ 1, $E(X)$ is $\infty$. 
If the shape parameter is $\leq$ 2, $Var(X)$ is $\infty$. 

First we simulate many sampes of size 1000 from a Pareto distribution with shape = 4. 

```{r}
# experiment with n and the parameters a and b
n <- 100     
n_sims <- 10000
a <- 1
b <- 4

x <- rpareto(n = n, a = a, b = b)
summary(x)

# Calculate mean and sd
mu <- mean(x)
stdev <- sd(x)

hist(x, freq = FALSE)
# Set up a vector that goes from 0 to 1 to overlay a normal distribution on the histogram
x_vals <- seq(min(x), max(x), length.out = 1000) 
# Now plot a normal distribution, using the mean and sd of the simulated values
lines(x_vals, dnorm(x_vals, mean = mu, sd = stdev), col = "red")
```

```{r}
# Compare tail to normal
compare.tail.to.normal
k <- 2 # sds
compare.tail.to.normal(x = x, k = k, mu = mu, sigma = stdev)

summary(x)
mu
stdev

# This gives the value of the mean, minus the value k*stdev 
# (i.e., an extreme negative value)
# Below I will use my object stdev in place of sigma (the parameter from Edge's function)
(mu - k * stdev)

# Extreme positive value
(mu + k * stdev)

# This statement asks whether the value in x is an extreme value
# The operator '|' is 'OR'
# Is x extreme negative OR extreme positive?
x < (mu - k * stdev) | x > (mu + k * stdev)

# We can get the frequencies of this logical vector using table
table(x < (mu - k * stdev) | x > (mu + k * stdev))
# Or, as Edge, does, calculate the average of TRUEs - which is simply the proportion of TRUEs
mean(x < (mu - k * stdev) | x > (mu + k * stdev))

# What proportion/probability of TRUEs would we expect under a normal probability distribution?
pnorm(k) # probability of observing a value less than k standard deviations above the mean
pnorm(-k) # probability of observing a value less than k standard deviations below the mean
(1 - (pnorm(k) - pnorm(-k))) # probability of observing an extreme value

# So putting it all together, we have the ratio of:
# the probability of observing an extreme value in the data, over the
# the probability of observing an extreme value in a normal distribution:
mean(x < (mu - k * stdev) | x > (mu + k * stdev))/(1 - (pnorm(k) - pnorm(-k)))
compare.tail.to.normal(x = x, k = k, mu = mu, sigma = stdev)

# If this ratio is < 1, then the data have fewer extreme values than suggested by a normal
# If this ratio is > 1, then the data have more extreme values than suggested by a normal
```

Above, I haven't computed the means of many simulations - which is the crux of the question! So here I just paste Edge's solution. In it, he calculates $E(X)$ and $Var(X)$ using the Pareto probability distribution. I have changed `n` and `n.sim` to match my values above. 

```{r}
#Sample size per simulation (n) and number of simulations.
n <- 100
n.sim <- 10000
#Pareto parameters. Variance is finite, and so
#CLT applies, if a > 2. For large a, convergence to
#normal is better. With small a, convergence is slow,
#especially in the tails.
a <- 4
b <- 1
#Compute the expectation and variance of the distribution
#of the sample mean. a must be above 2 for these expressions
#to hold.
expec.par <- a*b/(a-1)
var.par <- a*b^2 / ((a-1)^2 * (a-2))
sd.mean <- sqrt(var.par / n)
#Simulate data
sim <- matrix(rpareto(n*n.sim, a, b), nrow = n.sim)
# Each column represents ith sample taken per simulation
# Each row represents a different simulation
sim[1:3, 1:10]
# Compute sample means.
means.sim <- rowMeans(sim)
str(means.sim)
#Draw a histogram of the sample means along with the approximate
#normal pdf that follows from the CLT.
hist(means.sim, prob = TRUE)
curve(dnorm(x, expec.par, sd.mean), add = TRUE, col = 'red')
compare.tail.to.normal(means.sim, 1/2, expec.par, sd.mean)
compare.tail.to.normal(means.sim, 1, expec.par, sd.mean)
compare.tail.to.normal(means.sim, 2, expec.par, sd.mean)
compare.tail.to.normal(means.sim, 3, expec.par, sd.mean)
compare.tail.to.normal(means.sim, 4, expec.par, sd.mean)
compare.tail.to.normal(means.sim, 5, expec.par, sd.mean)
compare.tail.to.normal(means.sim, 6, expec.par, sd.mean)
```


## A probabilistic model for simple linear regression

### Exercise set 5-5

1. Write the square of the correlation coefficient (eq. 5.30) in terms of the variance of Y (eq. 5.32) and the conditional variance of Y given X (eq. 5.31). 

$$
\begin{aligned}
\text{eq. 5.30: } \rho_{X,Y} = \beta \frac{\sigma_X}{\sigma_Y} \\
\text{eq. 5.31: } Var(Y) = \beta^2 \sigma_X^2 + \sigma_{\epsilon}^2 \\
\text{eq. 5.32: } Var(Y \mid X = x) = \sigma_{\epsilon}^2  \\
\end{aligned}
$$

Squaring $\rho_{X,Y}$, and expressing $Var(Y)$ using the definition from above:

$$
\begin{aligned}
\rho_{X,Y}^2 = \beta^2 \frac{\sigma_X^2}{\sigma_Y^2} = \beta^2 \frac{\sigma_X^2}{Var(Y)} \\
\end{aligned}
$$

$$
\begin{aligned}
\rho_{X,Y}^2 =  \beta^2 \frac{\sigma_X^2}{\beta^2 \sigma_X^2 + \sigma_{\epsilon}^2} \\
\end{aligned}
$$

Some algebra...

$$
\begin{aligned}
\rho_{X,Y}^2 = 1 - \frac{\sigma_{\epsilon}^2}{\beta^2 \sigma_X^2 + \sigma_{\epsilon}^2} \\
\end{aligned}
$$

And we use the formulas from above again to restate as:

$$
\begin{aligned}
\rho_{X,Y}^2 = 1 - \frac{Var(Y \mid X = x)}{Var(Y)} \\
\end{aligned}
$$

which gives us the 'proportion of variance explained'. So if there isn't much variance left in $Y$ after conditioning on $X$ (i.e., the numerator is small relative to the denominator), if we subtract it from 1, we get a high $r^2$. And vice versa. 

2. Simulating a regression. 

```{r}
library(stfspack)
sim.lm
sim_0_1 <- sim.lm(n = 50, a = 0, b = 1)
head(sim_0_1)
plot(sim_0_1[,1], sim_0_1[,2])
```

Still using all the default values for parameters:

```{r}
sim_0_1 <- sim.lm(n = 50, a = 0, b = 1, 
                  sigma.disturb = 1, mu.x = 8, sigma.x = 2, 
                  rdisturb = rnorm, rx = rnorm, het.coef = 0)
plot(sim_0_1[,1], sim_0_1[,2])
```

Now I'll change one at a time:

```{r}
sim_0_1 <- sim.lm(n = 50, a = 0, b = 1, 
                  sigma.disturb = 2, mu.x = 8, sigma.x = 2, 
                  rdisturb = rnorm, rx = rnorm, het.coef = 0)
plot(sim_0_1[,1], sim_0_1[,2])
```

```{r}
sim_0_1 <- sim.lm(n = 50, a = 0, b = 1, 
                  sigma.disturb = 1, mu.x = 16, sigma.x = 2, 
                  rdisturb = rnorm, rx = rnorm, het.coef = 0)
plot(sim_0_1[,1], sim_0_1[,2])
```

```{r}
sim_0_1 <- sim.lm(n = 50, a = 0, b = 1, 
                  sigma.disturb = 1, mu.x = 8, sigma.x = 4, 
                  rdisturb = rnorm, rx = rnorm, het.coef = 0)
plot(sim_0_1[,1], sim_0_1[,2])
```

```{r}
sim_0_1 <- sim.lm(n = 50, a = 0, b = 1, 
                  sigma.disturb = 1, mu.x = 8, sigma.x = 2, 
                  rdisturb = rlaplace, rx = rnorm, het.coef = 0)
plot(sim_0_1[,1], sim_0_1[,2])
```
